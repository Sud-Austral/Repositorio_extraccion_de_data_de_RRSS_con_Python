{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraccion de datos en Facebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fuente: https://www.iteramos.com/pregunta/50562/insertar-imagen-en-descuento-notebook-ipython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En primer lugar, importe \"urllib3\", \"facebook\", \"requests\".\n",
    "Defina un token variable y establezca su valor en lo que obtuvo \n",
    "como \"Token de acceso de usuario\".\n",
    "\n",
    "https://towardsdatascience.com/how-to-use-facebook-graph-api-and-extract-data-using-python-1839e19d6999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get request parameters-- id,created_time,name,message,insights.metric(post_impressions,post_impressions_unique,post_consumptions,post_consumptions_unique),comments.summary(true).limit(200){created_time,message,reactions.type(SAD).summary(1).as(sad),reactions.type(WOW).summary(1).as(wow),reactions.type(LOVE).summary(1).as(love),reactions.type(HAHA).summary(1).as(haha),reactions.type(ANGRY).summary(1).as(angry),id,like_count,comment_count,comments.limit(200){message,created_time,like_count,comments.limit(200),reactions.type(SAD).summary(1).as(sad),reactions.type(WOW).summary(1).as(wow),reactions.type(LOVE).summary(1).as(love),reactions.type(HAHA).summary(1).as(haha),reactions.type(ANGRY).summary(1).as(angry)}},shares,type,link,actions,place,targeting,feed_targeting,scheduled_publish_time,backdated_time,description,likes.summary(true),reactions.type(SAD).summary(1).as(sad),reactions.type(WOW).summary(1).as(wow),reactions.type(LOVE).summary(1).as(love),reactions.type(HAHA).summary(1).as(haha),reactions.type(ANGRY).summary(1).as(angry)\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 22] Invalid argument: '0_https://www.facebook.com/cocacolachile_data_20200305.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-a4647ba450a1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    354\u001b[0m         \u001b[1;31m#print(\"path--\",path)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    355\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 356\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_feed_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget_page_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moffset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfield_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdate_since_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    357\u001b[0m             \u001b[0mcheck\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'data'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    358\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcheck\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-a4647ba450a1>\u001b[0m in \u001b[0;36mget_feed_data\u001b[1;34m(self, target_page, offset, fields, json_path, date_string)\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m         \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjson_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"w\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m#In Python 3, we need to specify encoding when open a file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m         \u001b[1;31m#f = open(json_path, \"w\") # This code is for python 2.7\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: [Errno 22] Invalid argument: '0_https://www.facebook.com/cocacolachile_data_20200305.json'"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Mon Feb 19 15:26:26 2018\n",
    "Usage : python Facebook_scraper_insight.py techinsider YYYY-MM-DD\n",
    "Update the below parameters in the program before execution\n",
    "    token_input = 'Paste your access token string here from the Graph API'\n",
    "    target_page_input = 'Enter the facebook page name'\n",
    "    date_since_input = 'Enter the date from which data is required'\n",
    "Script runs on Python 2/3 with minor changes which are commented in the program\n",
    "Note: Need an Admin level access to the page on the Facebook to execute this script and collect the page insights\n",
    "@author: sahil\n",
    "\"\"\"\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "'''\n",
    "reloading sys for utf8 encoding is for Python 2.7\n",
    "This line should be removed for Python 3\n",
    "In Python 3, we need to specify encoding when open a file\n",
    "f = open(\"file.csv\", encoding='utf-8')\n",
    "'''\n",
    "#reload(sys)\n",
    "#sys.setdefaultencoding('utf8')\n",
    "#f = open(\"file.csv\", encoding='utf-8')\n",
    "\n",
    "class FacebookScraper:\n",
    "    '''\n",
    "    FacebookScraper class to scrape facebook info\n",
    "    '''\n",
    "\n",
    "    def __init__(self, token):\n",
    "        self.token = token\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_to_epochtime(date_string):\n",
    "        '''Enter date_string in 2000-01-01 format and convert to epochtime for GET request'''\n",
    "        try:\n",
    "            epoch = int(time.mktime(time.strptime(date_string, '%Y-%m-%d')))\n",
    "            return epoch\n",
    "        except ValueError:\n",
    "            print('Invalid string format. Make sure to use %Y-%m-%d')\n",
    "            quit()\n",
    "\n",
    "    def get_feed_data(self, target_page, offset, fields, json_path, date_string):\n",
    "        \"\"\"This function will get the feed data\"\"\"\n",
    "        \n",
    "        url = \"https://graph.facebook.com/v2.10/{}/feed\".format(target_page)\n",
    "        param = dict()\n",
    "        param[\"access_token\"] = self.token\n",
    "        param[\"limit\"] = \"100\"\n",
    "        param[\"offset\"] = offset\n",
    "        param[\"fields\"] = fields\n",
    "        param[\"since\"] = self.extract_to_epochtime(date_string)\n",
    "        #print(\"PARAMETER---\",param)\n",
    "        #print(\"URL---\",url)\n",
    "        r = requests.get(url, param)\n",
    "        data = json.loads(r.text)\n",
    "        f = open(json_path, \"w\", encoding='utf-8')  #In Python 3, we need to specify encoding when open a file\n",
    "        #f = open(json_path, \"w\") # This code is for python 2.7\n",
    "        f.write(json.dumps(data, indent=4))\n",
    "        print(\"json file has been generated\")\n",
    "\n",
    "        f.close()\n",
    "\n",
    "        return data\n",
    "   \n",
    "    def create_table_header(self, list_rows, file_path, page_name, table_name):\n",
    "        '''This method will create a table according to header and table name'''\n",
    "\n",
    "        if table_name == \"feed\" :\n",
    "            header = [\"page_name\", \"id\", \"type\", \"created_time\", \"message\", \"lifetime_post_impressions\", \"lifetime_post_impressions_unique\", \"lifetime_post_consumptions\", \"lifetime_post_consumptions_unique\", \"name\", \"description\", \"actions_name\", \"share_count\", \"comment_count\", \"like_count\", \"sad_count\", \"wow_count\", \"love_count\", \"haha_count\", \"angry_count\"]\n",
    "        elif table_name == \"comment_replies\":\n",
    "            header = [\"page_name\", \"parent_comment_id\", \"parent_comment_message\", \"reply_comment_message\", \"comment_id\", \"created_time\", \"comment_like_cnt\"]\n",
    "        elif table_name == \"comments\":\n",
    "            header = [\"page_name\", \"post_id\", \"created_time\", \"message\", \"reply_cmt_cnt\", \"comments_like_cnt\", \"comment_haha_cnt\", \"comment_sad_cnt\", \"comment_wow_cnt\", \"comment_love_cnt\", \"comment_angry_cnt\", \"message_id\"]\n",
    "            print(\"Specified table name is not valid.\")\n",
    "        else:\n",
    "            quit()\n",
    "\n",
    "        file = open(file_path, 'w',encoding='utf-8')\n",
    "        #file = open(file_path, 'w')\n",
    "        file.write(','.join(header) + '\\n')\n",
    "        for i in list_rows:\n",
    "            file.write('\"' + page_name + '\",')\n",
    "            for j in range(len(i)):\n",
    "                row_string = ''\n",
    "                if j < len(i) -1 :\n",
    "                    row_string += '\"' + str(i[j]).replace('\"', '').replace('\\n', '') + '\"' + ','\n",
    "                else:\n",
    "                    row_string += '\"' + str(i[j]).replace('\"', '').replace('\\n', '') + '\"' + '\\n'\n",
    "                file.write(row_string)\n",
    "        file.close()\n",
    "        print(\"Generated {} table csv File for {}\".format(table_name, page_name))\n",
    "\n",
    "    def extract_feed_data(self, response_json_list):\n",
    "        '''This method takes response json data and convert to csv'''\n",
    "        print(\"extract_feed_data-----\")\n",
    "        list_all = []\n",
    "        for response_json in response_json_list:\n",
    "            data = response_json[\"data\"]\n",
    "            #print(\"FEED data--\",data)\n",
    "            for i in range(len(data)):\n",
    "                list_row = []\n",
    "                row = data[i]\n",
    "                id = row[\"id\"]\n",
    "                try:\n",
    "                    type = row[\"type\"]\n",
    "                except KeyError:\n",
    "                    type = \"\"\n",
    "                try:\n",
    "                    created_time = row[\"created_time\"]\n",
    "                except KeyError:\n",
    "                    created_time = \"\"\n",
    "                try:\n",
    "                    message = row[\"message\"]\n",
    "                except KeyError:\n",
    "                    message = \"\"                   \n",
    "                try:\n",
    "                    lifetime_post_impressions = row[\"insights\"][\"data\"][0][\"values\"][0][\"value\"]\n",
    "                except KeyError:\n",
    "                    lifetime_post_impressions = \"\"      \n",
    "                try:\n",
    "                    lifetime_post_impressions_unique = row[\"insights\"][\"data\"][1][\"values\"][0][\"value\"]\n",
    "                except KeyError:\n",
    "                    lifetime_post_impressions_unique = \"\"\n",
    "                try:\n",
    "                    lifetime_post_consumptions = row[\"insights\"][\"data\"][2][\"values\"][0][\"value\"]\n",
    "                except KeyError:\n",
    "                    lifetime_post_consumptions = \"\"    \n",
    "                try:\n",
    "                    lifetime_post_consumptions_unique = row[\"insights\"][\"data\"][3][\"values\"][0][\"value\"]\n",
    "                except KeyError:\n",
    "                    lifetime_post_consumptions_unique = \"\"  \n",
    "                try:\n",
    "                    name = row[\"name\"]\n",
    "                except KeyError:\n",
    "                    name = \"\"\n",
    "                try:\n",
    "                    description = row[\"description\"]\n",
    "                except KeyError:\n",
    "                    description = \"\"\n",
    "                try:\n",
    "                    actions_link = row[\"actions\"][0][\"link\"]\n",
    "                except KeyError:\n",
    "                    actions_link = \"\"\n",
    "                try:\n",
    "                    share_count = row[\"shares\"][\"count\"]\n",
    "                except KeyError:\n",
    "                    share_count = \"\"\n",
    "                try:\n",
    "                    comment_count = row[\"comments\"][\"summary\"][\"total_count\"]\n",
    "                except KeyError:\n",
    "                    comment_count = \"\"\n",
    "                try:\n",
    "                    like_count = row[\"likes\"][\"summary\"][\"total_count\"]\n",
    "                except KeyError:\n",
    "                    like_count = \"\"\n",
    "                try:\n",
    "                    sad_count = row[\"sad\"][\"summary\"][\"total_count\"]\n",
    "                except KeyError:\n",
    "                    sad_count = \"\"                    \n",
    "                try:\n",
    "                    wow_count = row[\"wow\"][\"summary\"][\"total_count\"]\n",
    "                except KeyError:\n",
    "                    wow_count = \"\"\n",
    "                try:\n",
    "                    love_count = row[\"love\"][\"summary\"][\"total_count\"]\n",
    "                except KeyError:\n",
    "                    love_count = \"\"\n",
    "                try:\n",
    "                    haha_count = row[\"haha\"][\"summary\"][\"total_count\"]\n",
    "                except KeyError:\n",
    "                    haha_count = \"\"\n",
    "                try:\n",
    "                    angry_count = row[\"angry\"][\"summary\"][\"total_count\"]\n",
    "                except KeyError:\n",
    "                    angry_count = \"\"               \n",
    "               \n",
    "                list_row.extend((id, type, created_time, message,lifetime_post_impressions,lifetime_post_impressions_unique,lifetime_post_consumptions,lifetime_post_consumptions_unique, name,description, actions_link, share_count, comment_count, like_count,sad_count,wow_count,love_count,haha_count,angry_count))\n",
    "                list_all.append(list_row)\n",
    "       \n",
    "        return list_all\n",
    "\n",
    "    def extract_comments_data(self, response_json_list):\n",
    "        '''Function to extract post comment data'''\n",
    "        list_all = []\n",
    "        for response_json in response_json_list:\n",
    "            data = response_json[\"data\"]\n",
    "            # like_list = []\n",
    "            for i in range(len(data)):\n",
    "                #print(\"datalen--\",len(data))\n",
    "                row = data[i]\n",
    "                post_id = row[\"id\"]\n",
    "                try:\n",
    "                   comment_count = row[\"comments\"][\"summary\"][\"total_count\"]\n",
    "                   #print(\"comment_count---\",comment_count)\n",
    "                except KeyError:\n",
    "                    comment_count = 0\n",
    "                if comment_count > 0:\n",
    "                    comments = row[\"comments\"][\"data\"]\n",
    "                    for comment in comments:\n",
    "                        row_list = []\n",
    "                        created_time = comment[\"created_time\"]\n",
    "                        message = comment[\"message\"].encode('latin1', 'ignore')\n",
    "                        reply_cmt_cnt=comment[\"comment_count\"]                        \n",
    "                        comment_like_cnt = comment[\"like_count\"]\n",
    "                        comment_haha_cnt = comment[\"haha\"][\"summary\"][\"total_count\"]\n",
    "                        comment_sad_cnt = comment[\"sad\"][\"summary\"][\"total_count\"]\n",
    "                        comment_wow_cnt = comment[\"wow\"][\"summary\"][\"total_count\"]\n",
    "                        comment_love_cnt = comment[\"love\"][\"summary\"][\"total_count\"]\n",
    "                        comment_angry_cnt = comment[\"angry\"][\"summary\"][\"total_count\"]                        \n",
    "                        message_id = comment[\"id\"]\n",
    "                        row_list.extend((post_id, created_time, message,\\\n",
    "                        reply_cmt_cnt,comment_like_cnt,comment_haha_cnt,comment_sad_cnt,comment_wow_cnt,comment_love_cnt,comment_angry_cnt,message_id))\n",
    "                        #print(row_list)\n",
    "                        list_all.append(row_list)\n",
    "               \n",
    "                # Check if the next link exists\n",
    "                try:\n",
    "                    next_link = row[\"comments\"][\"paging\"][\"next\"]\n",
    "                    print(\"Next link for comments data\")\n",
    "                except KeyError:\n",
    "                    next_link = None\n",
    "                    continue\n",
    "               \n",
    "                if next_link is not None:\n",
    "                    r = requests.get(next_link.replace(\"limit=200\", \"limit=200\"))\n",
    "                    comments_data = json.loads(r.text)\n",
    "                    while True:\n",
    "                        for i in range(len(comments_data[\"data\"])):\n",
    "                            row_list = []\n",
    "                            comment = comments_data[\"data\"][i]\n",
    "                            created_time = comment[\"created_time\"]\n",
    "                            message = comment[\"message\"].encode('latin1', 'ignore')\n",
    "                            reply_cmt_cnt=comment[\"comment_count\"]                            \n",
    "                            comment_like_cnt = comment[\"like_count\"]\n",
    "                            comment_haha_cnt = comment[\"haha\"][\"summary\"][\"total_count\"]                            \n",
    "                            comment_sad_cnt = comment[\"sad\"][\"summary\"][\"total_count\"]\n",
    "                            comment_wow_cnt = comment[\"wow\"][\"summary\"][\"total_count\"]\n",
    "                            comment_love_cnt = comment[\"love\"][\"summary\"][\"total_count\"]\n",
    "                            comment_angry_cnt = comment[\"angry\"][\"summary\"][\"total_count\"]                                                    \n",
    "                            message_id = comment[\"id\"]\n",
    "                            row_list.extend((post_id, created_time, message,\\\n",
    "                            reply_cmt_cnt,comment_like_cnt,comment_haha_cnt,comment_sad_cnt,comment_wow_cnt,comment_love_cnt,comment_angry_cnt,message_id))\n",
    "                            list_all.append(row_list)\n",
    "                        try:\n",
    "                            next = comments_data[\"paging\"][\"next\"]\n",
    "                            r = requests.get(next.replace(\"limit=200\", \"limit=200\"))\n",
    "                            comments_data = json.loads(r.text)\n",
    "                        except KeyError:\n",
    "                            break\n",
    "        return list_all\n",
    "   \n",
    "    def extract_comment_replies_data(self, response_json_list):\n",
    "        '''This will get the replies to the comments posted on FB'''\n",
    "        print(\"extract_comment_replies_data-----\")\n",
    "        list_all = []\n",
    "        for response_json in response_json_list:\n",
    "            data = response_json[\"data\"]\n",
    "            #print(\"data len---\",len(data))\n",
    "        for i in range(len(data)):\n",
    "            row = data[i]\n",
    "            comments=row[\"comments\"][\"data\"]\n",
    "            for j in range(len(comments)):\n",
    "                cmt_data=comments[j]\n",
    "                try:\n",
    "                    comment_count=cmt_data[\"comment_count\"]\n",
    "                except KeyError:\n",
    "                    comment_count = 0                \n",
    "                if comment_count > 0:\n",
    "                    #print(\"comment_count\",comment_count)\n",
    "                    try:\n",
    "                        reply_cmt = cmt_data[\"comments\"][\"data\"]\n",
    "                    except:\n",
    "                        print(\"Breaking loop out of reply cmt\")\n",
    "                        break\n",
    "                    for k in range(len(reply_cmt)):\n",
    "                        #print(\"Reply comment message--\",reply_cmt[k][\"message\"])     \n",
    "                        row_list = []\n",
    "                        parent_comment_id = cmt_data[\"id\"]\n",
    "                        parent_comment_message = cmt_data[\"message\"]\n",
    "                        reply_comment_message = reply_cmt[k][\"message\"]\n",
    "                        comment_id = reply_cmt[k][\"id\"]\n",
    "                        created_time = reply_cmt[k][\"created_time\"]\n",
    "                        comment_like_cnt = reply_cmt[k][\"like_count\"]\n",
    "\n",
    "                        row_list.extend((parent_comment_id, parent_comment_message, reply_comment_message,comment_id,created_time,comment_like_cnt))                     \n",
    "                        list_all.append(row_list)\n",
    "               \n",
    "                # Check if the next link exists\n",
    "                try:\n",
    "                    next_link = cmt_data[\"comments\"][\"paging\"][\"next\"]\n",
    "                except KeyError:\n",
    "                    next_link = None\n",
    "                    continue\n",
    "                else:\n",
    "                    print(\"No reply comments in the post\")\n",
    "                    break\n",
    "                \n",
    "                if next_link is not None:\n",
    "                    r = requests.get(next_link.replace(\"limit=100\", \"limit=100\"))\n",
    "                    comments_data = json.loads(r.text)\n",
    "                    while True:\n",
    "                        for i in range(len(comments_data[\"data\"])):\n",
    "                            row_list = []\n",
    "                            #comment = comments_data[\"data\"][i]\n",
    "                            #print(\"Next loop comment data\",comment)\n",
    "                            print(\"Next loop reply comment data \")\n",
    "                            parent_comment_id = cmt_data[\"id\"]\n",
    "                            parent_comment_message = cmt_data[\"message\"]\n",
    "                            reply_comment_message = reply_cmt[k][\"message\"]\n",
    "                            comment_id = reply_cmt[k][\"id\"]\n",
    "                            created_time = reply_cmt[k][\"created_time\"]\n",
    "                            comment_like_cnt = reply_cmt[k][\"like_count\"]\n",
    "                            row_list.extend((parent_comment_id, parent_comment_message, reply_comment_message,comment_id,created_time,comment_like_cnt))                     \n",
    "                            list_all.append(row_list)\n",
    "                        try:\n",
    "                            next = comments_data[\"paging\"][\"next\"]\n",
    "                            r = requests.get(next.replace(\"limit=100\", \"limit=100\"))\n",
    "                            comments_data = json.loads(r.text)\n",
    "                        except KeyError:\n",
    "                            #print(\"Comments for the post {} completed\".format(post_id))\n",
    "                            break \n",
    "        return list_all\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    now = datetime.datetime.now()\n",
    "    current_day=now.strftime(\"%Y%m%d\")\n",
    "#    token_input = sys.argv[1]\n",
    "    #target_page_input = sys.argv[1]\n",
    "    #date_since_input = sys.argv[2]\n",
    "    token_input = 'EAAHvm8OZBTkgBAPopnkrNV9WK80LiuRkMxCrZCOpTZB28gBPvvt64i9l37FKU9oK5aJw62bfR3WX972IkGIpxmogZCGIhvVu5duwIIRpTiATvux2VBhdFIS8CfNKBaefO6p2QZBYZCMWDqiBeK7nuJKLUs6KJ9tf6nD6PF8BOGGgZDZD'\n",
    "    target_page_input = 'https://www.facebook.com/cocacolachile'\n",
    "    date_since_input = '2010-01-01'\n",
    "    json_path_input = target_page_input+\"_data_\"+current_day+\".json\"\n",
    "    csv_feed_path_input = target_page_input+\"_feed_\"+current_day+\".csv\"\n",
    "    csv_reply_comment_path_input = target_page_input+\"_reply_comments_\"+current_day+\".csv\"\n",
    "    csv_comments_path_input = target_page_input+\"_post_comments_\"+current_day+\".csv\"\n",
    "\n",
    "    # Get request parameters \n",
    "    field_input='id,created_time,name,message,insights.metric(post_impressions,post_impressions_unique,post_consumptions,post_consumptions_unique),comments.summary(true).limit(200){created_time,message,reactions.type(SAD).summary(1).as(sad),reactions.type(WOW).summary(1).as(wow),reactions.type(LOVE).summary(1).as(love),reactions.type(HAHA).summary(1).as(haha),reactions.type(ANGRY).summary(1).as(angry),id,like_count,comment_count,comments.limit(200){message,created_time,like_count,comments.limit(200),reactions.type(SAD).summary(1).as(sad),reactions.type(WOW).summary(1).as(wow),reactions.type(LOVE).summary(1).as(love),reactions.type(HAHA).summary(1).as(haha),reactions.type(ANGRY).summary(1).as(angry)}},shares,type,link,actions,place,targeting,feed_targeting,scheduled_publish_time,backdated_time,description,likes.summary(true),reactions.type(SAD).summary(1).as(sad),reactions.type(WOW).summary(1).as(wow),reactions.type(LOVE).summary(1).as(love),reactions.type(HAHA).summary(1).as(haha),reactions.type(ANGRY).summary(1).as(angry)'\n",
    "\n",
    "    fb = FacebookScraper(token_input)\n",
    "    \n",
    "    print(\"Get request parameters--\",field_input)\n",
    "\n",
    "    offset = 0\n",
    "    json_list = []\n",
    "    while True:\n",
    "        path = str(offset) + \"_\" + json_path_input\n",
    "        #print(\"path--\",path)\n",
    "        try:\n",
    "            data = fb.get_feed_data(target_page_input, str(offset), field_input, path, date_since_input)\n",
    "            check = data['data']\n",
    "            if (len(check) >= 100):\n",
    "                json_list.append(data)\n",
    "                offset += 100\n",
    "                print(\"Offset--\",offset)\n",
    "            else:\n",
    "                json_list.append(data)\n",
    "                print(\"End of loop for obtaining more than 100 feed records.\")\n",
    "                break\n",
    "        except KeyError:\n",
    "            print(\"Error with get request.\")\n",
    "            quit()\n",
    "    \n",
    "    print(\"Create feed table\")    \n",
    "    feed_table_list = fb.extract_feed_data(json_list)\n",
    "    print(feed_table_list[0])\n",
    "    fb.create_table_header(feed_table_list, csv_feed_path_input, target_page_input, \"feed\")\n",
    "    print(\"Feed generated\")    \n",
    "    \n",
    "    print(\"Create Comment table\")\n",
    "    comments_table_list = fb.extract_comments_data(json_list)\n",
    "    print(comments_table_list[0])\n",
    "    fb.create_table_header(comments_table_list, csv_comments_path_input, target_page_input, \"comments\")\n",
    "    print(\"comments_table_list generated\")\n",
    "    \n",
    "    print(\"Create Comment replies table\")\n",
    "    comment_replies_list = fb.extract_comment_replies_data(json_list)\n",
    "    print(comment_replies_list[0])\n",
    "    fb.create_table_header(comment_replies_list, csv_reply_comment_path_input, target_page_input, \"comment_replies\")\n",
    "    print(\"comments_table_list generated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
