{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming de tweets con Azure Databricks y su despliegue en Power BI\n",
    "\n",
    "#### Fuentes:\n",
    "\n",
    "Structured Streaming with Azure Databricks into Power BI & Cosmos DB\n",
    "\n",
    "https://github.com/giulianorapoz/DatabricksStreamingPowerBI\n",
    "\n",
    "Tutorial: Anomaly detection on streaming data using Azure Databricks\n",
    "\n",
    "https://docs.microsoft.com/en-us/azure/cognitive-services/anomaly-detector/tutorials/anomaly-detection-streaming-databricks\n",
    "\n",
    "\n",
    "Tutorial: Sentiment analysis on streaming data using Azure Databricks\n",
    "\n",
    "https://docs.microsoft.com/en-us/azure/azure-databricks/databricks-sentiment-analysis-cognitive-services\n",
    "\n",
    "\n",
    "Power BI\n",
    "\n",
    "https://docs.microsoft.com/en-us/azure/databricks/integrations/bi/power-bi\n",
    "\n",
    "Power BI Connects to Azure Databricks\n",
    "\n",
    "https://towardsdatascience.com/power-bi-connects-to-azure-databricks-44bea6731be7\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Christian Castro \n",
      "last updated: 2020-04-27 \n",
      "\n",
      "numpy 1.18.1\n",
      "pandas 1.0.1\n",
      "matplotlib 3.1.3\n",
      "Propiedad de DataIntelligence\n"
     ]
    }
   ],
   "source": [
    "# %load_ext watermark\n",
    "%watermark -a \"Christian Castro\" -u -d -p numpy,pandas,matplotlib\n",
    "%watermark -a \"Propiedad de DataIntelligence\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Introducción\n",
    "\n",
    "En este notebook revisaremos el concepto de Streaming Estructurado con **Azure Databricks** y en el cómo se puede conectar directamente con Power BI, lo que permite la visualización y análisis avanzados.\n",
    "\n",
    "Construiremos una ruta de consumo de datos directamente con Azure Databricks lo que nos permitirá transmitir datos a un clúster de **Apache Spark** en tiempo casi real. Mostraremos algunas de las capacidades de análisis a las que se puede llamar directamente desde Databricks utilizando la API de Text Analytics, luego conectaremos Databricks directamente a Power BI para treas de visualización y análisis. \n",
    "\n",
    "Se deja como tarea la lectura y escritura directamente desde Databricks  en CosmosDB como almacenamiento persistente y uso posterior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Configuración de nuestros recursos. \n",
    "\n",
    "Necesitaremos lo siguiente:\n",
    "* Un espacio de nombres en Azure.\n",
    "* Un espacio de trabajo de Databricks y un cluster Apache Spark para ejecutar nuestros nuestros notebooks.\n",
    "* Un Event Hub, para que Databricks envíe los datos.\n",
    "* Una cuenta de servicios cognitivos, para acceder a la API de Text Analytics.\n",
    "* Una cuenta de Twitter, para obtener un streaming de datos.\n",
    "* Power BI Desktop para visualizar y analizar los datos. \n",
    "* (Opcional) Una base de datos CosmosDB, para almacenar datos de forma persistente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Crear un **espacio de nombres**\n",
    "\n",
    "* Dá click en las tres líneas horizontales de la esquina superior izquierda de Azure, dá click en **Crear un recurso** y en el cuadro de búsqueda ingresa **Event Hub**.\n",
    "\n",
    "* Dá click en el ícono **Event Hubs** y luego en **Crear**. Ésto no creará un event hub inmediatamente, sino que primero se requerirá la creación de un **espacio de nombres**. Es por ello ue llegarán a la ventana **Crear un espacio de nombres**.\n",
    "\n",
    "* En **Crear un espacio de nombres** sólo ingresa datos en la pestaña de **Datos básicos** (lo que no se indica que modifiques déjalo tal cual, así en toda la guía):\n",
    "\n",
    "    * Suscripción: Suscripción de Azure 1\n",
    "        *Grupo de recursos: Crea uno nuevo: (nuevogrupo2)\n",
    "\n",
    "    * Nombre de espacio de nombres: tweetstopowerbi8\n",
    "    \n",
    "    * Ubicación: Busca una ubicación diferente (existen cuotas que entrega Azure por región que puede provocarte errores más adelante): (US) Centro-Sur de EE.UU.\n",
    "    \n",
    "    * Plan de tarifa: Estándar\n",
    "\n",
    "Espere hasta que salga el mensaje: Validación correcta.\n",
    "\n",
    "Dá click en **Crear**.\n",
    "\n",
    "Surgirá un mensaje: **La implementación está en curso**\n",
    "Espere hasta que aparezca el mensaje: **Se completó la implementación**\n",
    "\n",
    "Dá click a **Ir al recurso**\n",
    "\n",
    "Entramos al Espacio de nombres de Event Hubs llamado: tweetstopowerbi8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Crear un Event Hub\n",
    "\n",
    "En el espacio de nombres tweetstopowerbi8, da click a **+ Event Hub**. Dale un nombre: tweetstopowerbi8 y luego click a **Crear**.\n",
    "\n",
    "Ahora el **Event Hub** está listo para funcionar, y tenemos todas las cadenas de conexión necesarias para que Databricks envíe datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fig 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el espacio de nombres tweetstopowerbi8, en la lista desplegable de la izquierda, en el listado de Configuración da click a **Directivas de acceso compartido** y en la ventana de la derecha a **RootManageSharedAccessKey**\n",
    "\n",
    "Se cargará automáticamente la **Directiva SAS**\n",
    "\n",
    "Copia en un txt las claves y ls cadenas de conexión:\n",
    "\n",
    "Clave principal\n",
    "fpKYGIGLA3QSCNihV5I8e3Sv/mCfmb3ZUgZQocKCXUc=\n",
    "\n",
    "Clave secundaria\n",
    "Aak8snk7d89yQULitOuoWtpct6OeYmhpaieRdpIe4mM=\n",
    "\n",
    "Cadena de conexión: clave principal\n",
    "Endpoint=sb://tweetstopowerbi8.servicebus.windows.net/;SharedAccessKeyName=RootManageSharedAccessKey;SharedAccessKey=fpKYGIGLA3QSCNihV5I8e3Sv/mCfmb3ZUgZQocKCXUc=\n",
    "    \n",
    "Cadena de conexión: clave secundaria   \n",
    "Endpoint=sb://tweetstopowerbi8.servicebus.windows.net/;SharedAccessKeyName=RootManageSharedAccessKey;SharedAccessKey=Aak8snk7d89yQULitOuoWtpct6OeYmhpaieRdpIe4mM="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora el **Event Hub** está listo para funcionar, y tenemos todas las cadenas de conexión necesarias para que Databricks envíe datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Construir un Workspace Databricks\n",
    "\n",
    "Busque **Azure Databricks** en **+ Crear un recurso** al hacer click a las tres lineas horizontales de la esquina superior izquierda. Da click en **Crear**. \n",
    "\n",
    "* Suscripción: Suscripción de Azure 1\n",
    "    *Grupo de recursos: Crea uno nuevo: nuevogrupo2\n",
    "\n",
    "* Nombre de espacio de nombres: tweetstopowerbi8\n",
    "    \n",
    "* Ubicación: Busca una ubicación diferente (existen cuotas que entrega Azure por región que puede provocarte errores más adelante): (US) Centro-Sur de EE.UU.\n",
    "    \n",
    "* Plan de tarifa: **Premium** Ésto es muy importante, pues si no elegimos premiun no vamos a obtener la cadena de conexión jdbc.\n",
    "\n",
    "Dá click en revisar y crear.\n",
    "\n",
    "Espera a que se te indique: **Validación correcta** y da click a **Crear**.\n",
    "\n",
    "Se inicializará la implementación.\n",
    "\n",
    "Cuando la implementación esté completa, damos click a **Ir al recurso**\n",
    "\n",
    "Le damos click  Iniciar área de trabajo, lo que nos redirecionará al portal de Azure Databrick. Necesitaremos ingresar las credenciales de Azure nuevamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.1 Construyendo Clusters.\n",
    "\n",
    "Ya estamos en Azure Databricks!\n",
    "\n",
    "Le damos clik a **New Cluster** debajo de la franja **Common Tasks**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acepte todos los demás valores predeterminados que no sean los siguientes:\n",
    "\n",
    "Ingrese un nombre para el clúster: tweetstopowerbi8\n",
    "\n",
    "Databricks Runtime Version: Runtime 6.4 (Scala 2.11, Spark 2.4.5)\n",
    "\n",
    "Asegúrate de que la casilla de verificación: **Terminar después de 120 minutos de inactividad** esté seleccionada. Proporcione una duración (en minutos) para terminar el clúster, si el clúster no se está utilizando.\n",
    "\n",
    "Seleccione **Crear clúster**.\n",
    "\n",
    "La creación del clúster lleva varios minutos. Una vez que el clúster se esté ejecutando, puede adjuntar notebooks al clúster y ejecutar trabajos de Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2 Adjuntar bibliotecas al clúster de Spark\n",
    "\n",
    "Utilizarás las API de Twitter para enviar tweets a Event Hubs y el conector Apache Spark Event Hubs para leer y escribir datos en Azure Event Hubs. \n",
    "\n",
    "Para usar estas API como parte de tu clúster, agrégalas dándole click a **Install New** en **Libraries** en el cluster recién creado.\n",
    "\n",
    "Se abrirá la ventana **Install Library**, seleccionamos Maven en instalamos como coordenadas:\n",
    "\n",
    "com.microsoft.azure:azure-eventhubs-spark_2.11:2.3.10\n",
    "\n",
    "org.twitter4j:twitter4j-core:4.0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Kit de herramientas de servicios cognitivos\n",
    "\n",
    "Para calcular el sentimiento de los tweets, se requiere un acceso a los **Servicios Cognitivos** de Microsoft. Esto permitirá que Databricks llame a la API de Text Analytics en tiempo casi real directamente desde el notebook y calcule el sentimiento de un tweet dado. \n",
    "\n",
    "Crea un recurso como ya lo hemos hecho varias veces y busca:\n",
    "Text Analytics. Dá click en **Crear**.\n",
    "\n",
    "1 Nombre: tweetstopowerbi8\n",
    "\n",
    "2 Suscripción: Suscripción de Azure 1\n",
    "\n",
    "3 Ubicación: (US) Centro-Sur de EE.UU.\n",
    "\n",
    "4 Plan de tarifa: S\n",
    "\n",
    "5 Grupo de recursos: Crea uno nuevo: nuevogrupo2\n",
    "\n",
    "Da click en **Crear**\n",
    "\n",
    "Una vez implementado, haz click en ir al recurso, y bajo ADMINISTRACIÓN  DE RECURSOS, dá click en **Claves y punto de conexión** \n",
    "\n",
    "Toma nota de la URL del extremo y las claves. Estos valores serán necesarios para que Databricks llame con éxito a la API de Text Analytics.\n",
    "\n",
    "NOMBRE:\n",
    "tweetstopowerbi8\n",
    "Extremo:\n",
    "https://tweetstopowerbi8.cognitiveservices.azure.com/\n",
    "Clave1:\n",
    "3f6e681ce10c430fadc5fa12b5899774\n",
    "Clave2:\n",
    "b9eb64027de54831ae0a754d2bff93a2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 Crear notebooks en Databricks\n",
    "\n",
    "Crearemos cuatro notebooks en el workspace de Databricks con los siguientes nombres:\n",
    "\n",
    "1 SendTweetsToEventHub (To send tweets to the event hub)\n",
    "2 TweetSentiment (To calculate sentiment from stream of tweets from event hub)\n",
    "3 ScheduledTableCreate (To create and continuously update the dataset)\n",
    "4 DatasetValidation (To validate the dataset directly within Databricks)\n",
    "\n",
    "En el cluster de Azure Databricks tweetstopowerbi8, en la franja negra de la izquierda anda seleccionando consecutivamente: Workspace, Shared, Shared, Create, Notebook.\n",
    "\n",
    "Crea los cuatro notebooks en el lenguaje **Scala**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Código del Notebook SendTweetsToEventHub\n",
    "\n",
    "Hay un detalle que hay que tmar en cuanta: Tweeter aplica una restricción a la cantidad que puede ser consumida de tweets cada 15 minutos.\n",
    "https://dev.twitter.com/docs/rate-limiting/1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scala.collection.JavaConverters._\n",
    "    import com.microsoft.azure.eventhubs._\n",
    "    import java.util.concurrent._\n",
    "    import scala.collection.immutable._\n",
    "    import scala.concurrent.Future\n",
    "    import scala.concurrent.ExecutionContext.Implicits.global\n",
    "\n",
    "    val namespaceName = \"tweetstopowerbi8.servicebus.windows.net/\"\n",
    "    val eventHubName = \"tweetstopowerbi8\"\n",
    "    val sasKeyName = \"RootManageSharedAccessKey\"\n",
    "    val sasKey = \"ZhvoV1uJtdw1d8tTKmTM5PNB55gHCjSOn24yjVXcwQg=\"\n",
    "    val connStr = new ConnectionStringBuilder()\n",
    "                .setNamespaceName(namespaceName)\n",
    "                .setEventHubName(eventHubName)\n",
    "                .setSasKeyName(sasKeyName)\n",
    "                .setSasKey(sasKey)\n",
    "\n",
    "    val pool = Executors.newScheduledThreadPool(1)\n",
    "    val eventHubClient = EventHubClient.create(connStr.toString(), pool)\n",
    "\n",
    "    def sleep(time: Long): Unit = Thread.sleep(time)\n",
    "\n",
    "    def sendEvent(message: String, delay: Long) = {\n",
    "      sleep(delay)\n",
    "      val messageData = EventData.create(message.getBytes(\"UTF-8\"))\n",
    "      eventHubClient.get().send(messageData)\n",
    "      System.out.println(\"Sent event: \" + message + \"\\n\")\n",
    "    }\n",
    "\n",
    "    // Add your own values to the list\n",
    "    val testSource = List(\"Azure is the greatest!\", \"Azure isn't working :(\", \"Azure is okay.\")\n",
    "\n",
    "    // Specify 'test' if you prefer to not use Twitter API and loop through a list of values you define in `testSource`\n",
    "    // Otherwise specify 'twitter'\n",
    "\n",
    "    // val dataSource = \"test\"\n",
    "    val dataSource = \"twitter\"\n",
    "\n",
    "    if (dataSource == \"twitter\") {\n",
    "\n",
    "      import twitter4j._\n",
    "      import twitter4j.TwitterFactory\n",
    "      import twitter4j.Twitter\n",
    "      import twitter4j.conf.ConfigurationBuilder\n",
    "\n",
    "      // Twitter configuration!\n",
    "      // Replace values below with you\n",
    "\n",
    "      val twitterConsumerKey = \"koO4XqTuWFr5ADGcE8kjIkVoU\"\n",
    "      val twitterConsumerSecret = \"3F4sk9qU8zbKBROuLPUUj1uvE2YuhseXPe0ahMQoivg4icN5bL\"\n",
    "      val twitterOauthAccessToken = \"1230251564616515586-2KqPsCG2mIJp3irRjENgHpCfQUxTUg\"\n",
    "      val twitterOauthTokenSecret = \"6PJfMtYGY7w6csiIX9m1S5jFEKNZ3hE9PVkHKeN1S14iM\"\n",
    "\n",
    "      val cb = new ConfigurationBuilder()\n",
    "        cb.setDebugEnabled(true)\n",
    "        .setOAuthConsumerKey(twitterConsumerKey)\n",
    "        .setOAuthConsumerSecret(twitterConsumerSecret)\n",
    "        .setOAuthAccessToken(twitterOauthAccessToken)\n",
    "        .setOAuthAccessTokenSecret(twitterOauthTokenSecret)\n",
    "\n",
    "      val twitterFactory = new TwitterFactory(cb.build())\n",
    "      val twitter = twitterFactory.getInstance()\n",
    "\n",
    "      // Getting tweets with keyword \"Azure\" and sending them to the Event Hub in realtime!\n",
    "      val query = new Query(\"source:twitter4j colmedchile\")\n",
    "      query.setCount(100)\n",
    "      query.lang(\"en\")\n",
    "      var finished = false\n",
    "      while (!finished) {\n",
    "        val result = twitter.search(query)\n",
    "        val statuses = result.getTweets()\n",
    "        var lowestStatusId = Long.MaxValue\n",
    "        for (status <- statuses.asScala) {\n",
    "          if(!status.isRetweet()){\n",
    "            sendEvent(status.getText(), 5000)\n",
    "          }\n",
    "          lowestStatusId = Math.min(status.getId(), lowestStatusId)\n",
    "        }\n",
    "        query.setMaxId(lowestStatusId - 1)\n",
    "      }\n",
    "\n",
    "    } else if (dataSource == \"test\") {\n",
    "      // Loop through the list of test input data\n",
    "      while (true) {\n",
    "        testSource.foreach {\n",
    "          sendEvent(_,5000)\n",
    "        }\n",
    "      }\n",
    "\n",
    "    } else {\n",
    "      System.out.println(\"Unsupported Data Source. Set 'dataSource' to \\\"twitter\\\" or \\\"test\\\"\")\n",
    "    }\n",
    "\n",
    "    // Closing connection to the Event Hub\n",
    "    eventHubClient.get().close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente paso es tomar esta secuencia de tweets y aplicarle un análisis de opinión. Las siguientes líneas de código leídas desde EventHub, llaman a la API de Text Analytics y pasan el cuerpo del tweet para que se calcule el análisis. Obtén el análisis de los Tweets en el notebook TweetSentiment.\n",
    "\n",
    "Agrega las siguientes líneas de código para llamar a la API de Text Analytics para calcular el análisis de opinión de la transmisión de Twitter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Creamos el notebook TweetSentiment, que analiza los tweets desde Event Hub.\n",
    "\n",
    "El siguiente programa está escrito en Scala:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "import org.apache.spark.eventhubs._\n",
    "\n",
    "// Build connection string with the above information\n",
    "val connectionString = ConnectionStringBuilder(\"Endpoint=sb://tweetstopowerbi7.servicebus.windows.net/;SharedAccessKeyName=RootManageSharedAccessKey;SharedAccessKey=ZhvoV1uJtdw1d8tTKmTM5PNB55gHCjSOn24yjVXcwQg=\")\n",
    "  .setEventHubName(\"tweetstopowerbi7\")\n",
    "  .build\n",
    "\n",
    "val customEventhubParameters =\n",
    "  EventHubsConf(connectionString)\n",
    "  .setMaxEventsPerTrigger(5)\n",
    ".setStartingPosition(EventPosition.fromEndOfStream) //added this with martin from the databricks offical doc\n",
    "\n",
    "val incomingStream = spark.readStream.format(\"eventhubs\").options(customEventhubParameters.toMap).load()\n",
    "incomingStream.printSchema\n",
    "// Event Hub message format is JSON and contains \"body\" field\n",
    "// Body is binary, so we cast it to string to see the actual content of the message\n",
    "val messages =\n",
    "  incomingStream\n",
    "  .withColumn(\"Offset\", $\"offset\".cast(LongType))\n",
    "  .withColumn(\"Time (readable)\", $\"enqueuedTime\".cast(TimestampType))\n",
    "  .withColumn(\"Timestamp\", $\"enqueuedTime\".cast(LongType))\n",
    "  .withColumn(\"Body\", $\"body\".cast(StringType))\n",
    "  .select(\"Timestamp\", \"Body\")\n",
    ".as[(String, String)] //added this with martin from the databrick official doc\n",
    "\n",
    "\n",
    "// NEW CODE CELL ----------\n",
    "\n",
    "import java.io._\n",
    "import java.net._\n",
    "import java.util._\n",
    "\n",
    "class Document(var id: String, var text: String, var language: String = \"\", var sentiment: Double = 0.0) extends Serializable\n",
    "\n",
    "class Documents(var documents: List[Document] = new ArrayList[Document]()) extends Serializable {\n",
    "\n",
    "    def add(id: String, text: String, language: String = \"\") {\n",
    "        documents.add (new Document(id, text, language))\n",
    "    }\n",
    "    def add(doc: Document) {\n",
    "        documents.add (doc)\n",
    "    }\n",
    "}\n",
    "\n",
    "// NEW CODE CELL ----------\n",
    "\n",
    "class CC[T] extends Serializable { def unapply(a:Any):Option[T] = Some(a.asInstanceOf[T]) }\n",
    "object M extends CC[scala.collection.immutable.Map[String, Any]]\n",
    "object L extends CC[scala.collection.immutable.List[Any]]\n",
    "object S extends CC[String]\n",
    "object D extends CC[Double]\n",
    "\n",
    "// NEW CODE CELL ----------\n",
    "\n",
    "import javax.net.ssl.HttpsURLConnection\n",
    "import com.google.gson.Gson\n",
    "import com.google.gson.GsonBuilder\n",
    "import com.google.gson.JsonObject\n",
    "import com.google.gson.JsonParser\n",
    "import scala.util.parsing.json._\n",
    "\n",
    "object SentimentDetector extends Serializable {\n",
    "\n",
    "  // Cognitive Services API connection settings\n",
    "  val accessKey = \"ef5cfde06ebb4b3fadb16305e61bc807\"\n",
    "  val host = \"https://tweetstopowerbi7.cognitiveservices.azure.com/\"\n",
    "  val languagesPath = \"/text/analytics/v2.0/languages\"\n",
    "  val sentimentPath = \"/text/analytics/v2.0/sentiment\"\n",
    "  val languagesUrl = new URL(host+languagesPath)\n",
    "  val sentimenUrl = new URL(host+sentimentPath)\n",
    "\n",
    "  def getConnection(path: URL): HttpsURLConnection = {\n",
    "    val connection = path.openConnection().asInstanceOf[HttpsURLConnection]\n",
    "    connection.setRequestMethod(\"POST\")\n",
    "    connection.setRequestProperty(\"Content-Type\", \"text/json\")\n",
    "    connection.setRequestProperty(\"Ocp-Apim-Subscription-Key\", accessKey)\n",
    "    connection.setDoOutput(true)\n",
    "    return connection\n",
    "  }\n",
    "\n",
    "  def prettify (json_text: String): String = {\n",
    "    val parser = new JsonParser()\n",
    "    val json = parser.parse(json_text).getAsJsonObject()\n",
    "    val gson = new GsonBuilder().setPrettyPrinting().create()\n",
    "    return gson.toJson(json)\n",
    "  }\n",
    "\n",
    "  // Handles the call to Cognitive Services API.\n",
    "  // Expects Documents as parameters and the address of the API to call.\n",
    "  // Returns an instance of Documents in response.\n",
    "  def processUsingApi(inputDocs: Documents, path: URL): String = {\n",
    "    val docText = new Gson().toJson(inputDocs)\n",
    "    val encoded_text = docText.getBytes(\"UTF-8\")\n",
    "    val connection = getConnection(path)\n",
    "    val wr = new DataOutputStream(connection.getOutputStream())\n",
    "    wr.write(encoded_text, 0, encoded_text.length)\n",
    "    wr.flush()\n",
    "    wr.close()\n",
    "\n",
    "    val response = new StringBuilder()\n",
    "    val in = new BufferedReader(new InputStreamReader(connection.getInputStream()))\n",
    "    var line = in.readLine()\n",
    "    while (line != null) {\n",
    "        response.append(line)\n",
    "        line = in.readLine()\n",
    "    }\n",
    "    in.close()\n",
    "    return response.toString()\n",
    "  }\n",
    "\n",
    "  // Calls the language API for specified documents.\n",
    "  // Returns a documents with language field set.\n",
    "  def getLanguage (inputDocs: Documents): Documents = {\n",
    "    try {\n",
    "      val response = processUsingApi(inputDocs, languagesUrl)\n",
    "      // In case we need to log the json response somewhere\n",
    "      val niceResponse = prettify(response)\n",
    "      val docs = new Documents()\n",
    "      val result = for {\n",
    "            // Deserializing the JSON response from the API into Scala types\n",
    "            Some(M(map)) <- scala.collection.immutable.List(JSON.parseFull(niceResponse))\n",
    "            L(documents) = map(\"documents\")\n",
    "            M(document) <- documents\n",
    "            S(id) = document(\"id\")\n",
    "            L(detectedLanguages) = document(\"detectedLanguages\")\n",
    "            M(detectedLanguage) <- detectedLanguages\n",
    "            S(language) = detectedLanguage(\"iso6391Name\")\n",
    "      } yield {\n",
    "            docs.add(new Document(id = id, text = id, language = language))\n",
    "      }\n",
    "      return docs\n",
    "    } catch {\n",
    "          case e: Exception => return new Documents()\n",
    "    }\n",
    "  }\n",
    "\n",
    "  // Calls the sentiment API for specified documents. Needs a language field to be set for each of them.\n",
    "  // Returns documents with sentiment field set, taking a value in the range from 0 to 1.\n",
    "  def getSentiment (inputDocs: Documents): Documents = {\n",
    "    try {\n",
    "      val response = processUsingApi(inputDocs, sentimenUrl)\n",
    "      val niceResponse = prettify(response)\n",
    "      val docs = new Documents()\n",
    "      val result = for {\n",
    "            // Deserializing the JSON response from the API into Scala types\n",
    "            Some(M(map)) <- scala.collection.immutable.List(JSON.parseFull(niceResponse))\n",
    "            L(documents) = map(\"documents\")\n",
    "            M(document) <- documents\n",
    "            S(id) = document(\"id\")\n",
    "            D(sentiment) = document(\"score\")\n",
    "      } yield {\n",
    "            docs.add(new Document(id = id, text = id, sentiment = sentiment))\n",
    "      }\n",
    "      return docs\n",
    "    } catch {\n",
    "        case e: Exception => return new Documents()\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "// User Defined Function for processing content of messages to return their sentiment.\n",
    "val toSentiment = udf((textContent: String) => {\n",
    "  val inputDocs = new Documents()\n",
    "  inputDocs.add (textContent, textContent)\n",
    "  val docsWithLanguage = SentimentDetector.getLanguage(inputDocs)\n",
    "  val docsWithSentiment = SentimentDetector.getSentiment(docsWithLanguage)\n",
    "  if (docsWithLanguage.documents.isEmpty) {\n",
    "    // Placeholder value to display for no score returned by the sentiment API\n",
    "    (-1).toDouble\n",
    "  } else {\n",
    "    docsWithSentiment.documents.get(0).sentiment.toDouble\n",
    "  }\n",
    "})\n",
    "\n",
    "// NEW CODE CELL ----------\n",
    "\n",
    "// Prepare a dataframe with Content and Sentiment columns\n",
    "val streamingDataFrame = incomingStream.selectExpr(\"cast (body as string) AS Content\").withColumn(\"body\", toSentiment($\"Content\"))\n",
    "\n",
    "// Display the streaming data with the sentiment\n",
    "\n",
    "streamingDataFrame.writeStream.outputMode(\"append\").format(\"console\").option(\"truncate\", false).start()\n",
    "\n",
    "// NEW CODE CELL ----------\n",
    "\n",
    "//WRITE THE STREAM TO PARQUET FORMAT/////\n",
    "import org.apache.spark.sql.streaming.Trigger.ProcessingTime\n",
    "\n",
    "val result =\n",
    "  streamingDataFrame\n",
    "    .writeStream\n",
    "    .format(\"parquet\")\n",
    "    .option(\"path\", \"/mnt/DatabricksSentimentPowerBI\")\n",
    "    .option(\"checkpointLocation\", \"/mnt/sample/check2\")\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Crear tabla de datos para que Power BI se conecte\n",
    "\n",
    "Necesitamos escribir los datos como formato **parquet** en el almacenamiento de blobs que pasan en la ruta de nuestro almacenamiento de blobs montado.\n",
    "\n",
    "Añadimos las siguientes líneas al final del programa TweetSentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//WRITE THE STREAM TO PARQUET FORMAT/////  \n",
    "import org.apache.spark.sql.streaming.Trigger.ProcessingTime \n",
    "val result = streamingDataFrame\n",
    ".writeStream\n",
    ".format(\"parquet\")\n",
    ".option(\"path\", \"/mnt/DatabricksSentimentPowerBI\")\n",
    ".option(\"checkpointLocation\", \"/mnt/sample/check2\")\n",
    ".start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para verificar que los datos se escriban en el almacenamiento de blobs montado directamente desde el notebook Databricks, crea un nuevo notebook **DatasetValidation** y ejecuta los siguientes comandos para mostrar el contenido de los archivos de parquet directamente dentro de Databricks. Si los datos se escriben correctamente, una salida al consultar la tabla en Databricks debería ser similar a la siguiente:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fig nnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Crear el notebook ScheduledTableCreate\n",
    "\n",
    "Ahora tenemos transmisión de datos de Twitter con el sentimiento adjunto que fluye en un almacenamiento de blobs montado. El siguiente paso es conectar Databricks (y este conjunto de datos) directamente a PowerBIPower BI para su posterior análisis y disección de datos. \n",
    "\n",
    "Para hacer esto, necesitamos escribir los archivos de parquet en un conjunto de datos que PowerBIPower BI podrá leer con éxito a intervalos regulares (es decir, actualizar continuamente el conjunto de datos a intervalos específicos para el flujo de datos por lotes). Para hacer esto, cree el cuaderno final ScheduledDatasetCreation y ejecute el siguiente conjunto de comandos scala como un programa para ejecutarse cada minuto. (Esto actualizará la tabla creada cada 1 minuto con el flujo de datos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7 Conectar Power BI al clúster de Databricks\n",
    "\n",
    "Para permitir que PowerBIPower BI se conecte primero a Databricks, se requiere que la información de conexión JDBC de los clústeres se proporcione como una dirección de servidor para la conexión PowerBIPower BI. \n",
    "\n",
    "Para obtener esto, navegue al clúster dentro de Databricks y seleccione el clúster que se va a conectar. En la página del clúster, seleccione la pestaña JDBC / ODBC (Nota: **si no creó un espacio de trabajo Premium Databricks, esta opción no estará disponible**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para construir la dirección del servidor, tome la URL JDBC que se muestra en el clúster y haga lo siguiente: • Reemplace jdbc: hive2 con https. • Elimine todo en la ruta entre el número de puerto y sql que retiene los componentes para que tenga una url similar a la siguiente: • https://westeurope.azuredatabricks.net:443/sql/protocolv1/o/1406775902027556/0424- 131603-inky272 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "jdbc:spark://southindia.azuredatabricks.net:443/default;transportMode=http;ssl=1;httpPath=sql/protocolv1/o/7401067854218687/0425-221454-peg462;AuthMech=3;UID=token;PWD=<personal-access-token>\n",
    "    \n",
    "#### Dirección el servidor:\n",
    "    \n",
    "https://southindia.azuredatabricks.net:443/sql/protocolv1/o/7401067854218687/0425-221454-peg462"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para generar el acceso personal a tokentoken, seleccione Configuración de usuario en el panel de control del clúster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crear un clúster de Apache Spark dentro de Databricks\n",
    "\n",
    "Para ejecutar notebooks para consumir el streming de data, primero se requiere un clúster. Para crear un clúster Apache Spark dentro de Databricks, cargue un Workspace desde el recurso Databricks se creó. Desde el portal de Databricks, seleccione Cluster.\n",
    "\n",
    "dá click en: 'iniciar area de trabajo'\n",
    "\n",
    "En un nuevo clúster, proporcione los siguientes valores para crear el clúster. ¡NOTA! - Para las capacidades de lectura / escritura en CosmosDB, se requiere una versión Apache Spark de 2.2.0. Al momento de escribir 2.3.0 aún no es compatible.\n",
    "\n",
    "Adjuntar bibliotecas a Spark Cluster\n",
    "\n",
    "Para permitir que la API de Twitter envíe tweets a Databricks y Databricks para leer y escribir datos en Event Hubs y CosmosDB, se requieren tres paquetes: \n",
    "\n",
    "• Conector Spark Event Hubs: com.microsoft.azure:azure-eventhubs-spark_2.11:2.3. 1 \n",
    "\n",
    "• API de Twitter - org.twitter4j: twitter4j-core: 4.0.6 \n",
    "\n",
    "• CosmosDB Spark Connector: http://repo1.maven.org/maven2/com/microsoft/azure/azure-cosmosdb-spark_2.2.0_2.11/ 1.1.1 / azure-cosmosdb-spark_2.2.0_2.11-1.1.1-uber.jar\n",
    "\n",
    "Haga clic derecho en el espacio de trabajo de Databricks y seleccione Crear> Biblioteca. En la página Nueva biblioteca, seleccione Maven Coordinate e ingrese los nombres de las bibliotecas anteriores. se mantiene se encuentra aquí: https://github.com/Azure/azure-cosmosdb-spark\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kit de herramientas de servicios cognitivos\n",
    "\n",
    "Para calcular el sentimiento de los tweets, se requiere acceso a los Servicios Cognitivos de Microsoft. Esto permitirá que Databricks llame a la API de Text Analytics en tiempo casi real directamente desde el cuaderno y calcule el sentimiento de un tweet dado. Busque la API de Text Analytics en Azure Portal. Proporcione un nombre, ubicación y nivel de precios. (F0 será suficiente para los propósitos de esta demostración)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez creado, haga clic en **Claves y puntos de conexión** y tome nota de la URL del punto final y la Clave principal que se utilizará. Estos valores serán necesarios para que Databricks llame con éxito a la API de Text Analytics.\n",
    "\n",
    "endpoint:\n",
    "https://tweetstopowerbi5.cognitiveservices.azure.com/\n",
    "\n",
    "\n",
    "clave 1:\n",
    "\n",
    "dae1210d95694ace8758d4cc0bea86d2\n",
    "\n",
    "clave 2:\n",
    "\n",
    "35a59dc1f1274710aa101ac432fb90d6\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crear notebooks Databricks\n",
    "\n",
    "Para ejecutar el código, necesitaremos crear 4 cuadernos en el espacio de trabajo de Databricks creado de la siguiente manera:\n",
    "\n",
    "* EventHubTweets (para enviar tweets al centro de eventos)\n",
    "* TweetSentiment (Para calcular el sentimiento de la secuencia de tweets del centro de eventos)\n",
    "* ScheduledDatasetCreation (Para crear y actualizar continuamente el conjunto de datos)\n",
    "* DatasetValidation (para validar el conjunto de datos directamente dentro de Databricks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crear tabla de datos para que Power BI se conecte\n",
    "\n",
    "Primero, necesitamos escribir datos como formato **parquet** en el almacenamiento de blobs que pasan en la ruta de nuestro almacenamiento de blobs montado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "//WRITE THE STREAM TO PARQUET FORMAT/////  \n",
    "import org.apache.spark.sql.streaming.Trigger.ProcessingTime \n",
    "val result = streamingDataFrame\n",
    ".writeStream\n",
    ".format(\"parquet\")\n",
    ".option(\"path\", \"/mnt/DatabricksSentimentPowerBI\")\n",
    ".option(\"checkpointLocation\", \"/mnt/sample/check2\")\n",
    ".start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conecte Power BI al clúster de Databricks\n",
    "\n",
    "Para permitir que PowerBIPower BI se conecte primero a Databricks, se requiere que la información de conexión JDBC de los clústeres se proporcione como una dirección de servidor para la conexión PowerBIPower BI. Para obtener esto, navegue al clúster dentro de Databricks y seleccione el clúster que se va a conectar. En la página del clúster, seleccione la pestaña JDBC / ODBC (Nota: si no creó un espacio de trabajo Premium Databricks, esta opción no estará disponible).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para construir la dirección del servidor, tome la URL JDBC que se muestra en el clúster y haga lo siguiente: • Reemplace jdbc: hive2 con https. • Elimine todo en la ruta entre el número de puerto y sql que retiene los componentes para que tenga una url similar a la siguiente: \n",
    "\n",
    "https://westeurope.azuredatabricks.net:443/sql/protocolv1/o/1406775902027556/0424- 131603-inky272 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para verificar que los datos se escriben en el almacenamiento de blobs montado directamente desde el cuaderno Databricks, cree un nuevo DatasetValidation del cuaderno y ejecute los siguientes comandos para mostrar el contenido de los archivos de parquet directamente dentro de Databricks. Si los datos se escriben correctamente, una salida al consultar la tabla en Databricks debería ser similar a la siguiente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora tenemos la transmisión de datos de Twitter con el sentimiento adjunto fluyendo hacia un almacenamiento de blobs montado. El siguiente paso es conectar Databricks (y este conjunto de datos) directamente a PowerBIPower BI para su posterior análisis y disección de datos. Para hacer esto, necesitamos escribir los archivos de parquet en un conjunto de datos que PowerBIPower BI podrá leer con éxito a intervalos regulares (es decir, actualizar continuamente el conjunto de datos a intervalos específicos para el flujo de datos por lotes). Para hacer esto, cree el cuaderno final ScheduledDatasetCreation y ejecute el siguiente conjunto de comandos scala como un programa para ejecutarse cada minuto. (Esto actualizará la tabla creada cada 1 minuto con el flujo de datos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El paso final es conectar Databricks a PowerBIPower BI para permitir el flujo de datos por lotes y realizar análisis. Para hacer esto, abra el escritorio de PowerBIPower BI abierto y haga clic en Obtener datos. Seleccione Spark (beta) para comenzar a configurar la conexión del clúster Databricks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://southindia.azuredatabricks.net:443/sql/protocolv1/o/7401067854218687/0425-221454-peg462"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para generar el acceso personal a tokentoken, seleccione Configuración de usuario en el panel de control del clúster.\n",
    "\n",
    "Token:\n",
    "\n",
    "dapi50bc7f1e8a1b5a3b24c1a58d9696dea7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
