{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming de tweets con Azure Databricks y su despliegue en Power BI\n",
    "\n",
    "#### Fuentes:\n",
    "\n",
    "Structured Streaming with Azure Databricks into Power BI & Cosmos DB\n",
    "\n",
    "https://github.com/giulianorapoz/DatabricksStreamingPowerBI\n",
    "\n",
    "Tutorial: Anomaly detection on streaming data using Azure Databricks\n",
    "\n",
    "https://docs.microsoft.com/en-us/azure/cognitive-services/anomaly-detector/tutorials/anomaly-detection-streaming-databricks\n",
    "\n",
    "\n",
    "Tutorial: Sentiment analysis on streaming data using Azure Databricks\n",
    "\n",
    "https://docs.microsoft.com/en-us/azure/azure-databricks/databricks-sentiment-analysis-cognitive-services\n",
    "\n",
    "\n",
    "Power BI\n",
    "\n",
    "https://docs.microsoft.com/en-us/azure/databricks/integrations/bi/power-bi\n",
    "\n",
    "Power BI Connects to Azure Databricks\n",
    "\n",
    "https://towardsdatascience.com/power-bi-connects-to-azure-databricks-44bea6731be7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The watermark extension is already loaded. To reload it, use:\n",
      "  %reload_ext watermark\n",
      "Christian Castro \n",
      "last updated: 2020-04-27 \n",
      "\n",
      "numpy 1.18.1\n",
      "pandas 1.0.1\n",
      "matplotlib 3.1.3\n",
      "Propiedad de DataIntelligence\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark -a \"Christian Castro\" -u -d -p numpy,pandas,matplotlib\n",
    "%watermark -a \"Propiedad de DataIntelligence\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 Introducción\n",
    "\n",
    "En este notebook nos basaremos en el concepto de Streaming estructurado con Databricks y en el cómo se puede conectar directamente para utilizarlo junto con Power BI y Cosmos DB, lo que permite la visualización y análisis avanzados para una mayor disección de los datos consumidos por streaming estructurados. Construiremos una ruta de consumo de datos directamente con Azure Databricks que nos permitirá transmitir datos a un clúster de Apache Spark en tiempo casi real. Mostraremos algunas de las capacidades de análisis a las que se puede llamar directamente desde Databricks utilizando la API de Text Analytics, luego conectaremos Databricks directamente a Power BI para análisis e informes de disección de datos adicionales. Como paso final, leeremos y escribiremos desde Databricks directamente en CosmosDB como almacenamiento persistente y uso posterior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El primer paso es configurar todos nuestros recursos individuales. Necesitaremos lo siguiente:\n",
    "\n",
    "*    Un espacio de trabajo de Databricks y Apache spark cluster t. (Para ejecutar nuestros nuestros cuadernos).\n",
    "*    Un centro de eventos, f. (Para que Databricks envíe los datos).\n",
    "*    Una cuenta de servicios cognitivos t. (Para acceder a la API de Text Analytics).\n",
    "*    Una aplicación de Twitter para los datos. (Para proporcionar la transmisión actual de datos).\n",
    "*    Una base de datos CosmosDB como. (Para almacenar datos de forma persistente. Los datos)\n",
    "*    Power BI Desktop para visualización de datos t. (Para visualizar y analizar los datos). \n",
    "     \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Crear un **espacio de nombres**\n",
    "\n",
    "Crea un Event Hub buscando Event Hubs en **Crear un recurso** en Azure. Dale en Crear.\n",
    "\n",
    "En **Crear espacio de nombres**, elije un espacio de nombres y selecciona una suscripción de Azure, un grupo de recursos y una ubicación para crear el recurso.\n",
    "\n",
    "tweetstopowerbi6\n",
    "nuevogrupo1\n",
    "(Europe) Este de Noruega\n",
    "10 unidades de procesamiento\n",
    "Crear\n",
    "\n",
    "Espere hasta que salga el mensaje: Implementación correcta.\n",
    "\n",
    "En **Crear espacio de nombres**, elije un espacio de nombres y selecciona una suscripción de Azure, un grupo de recursos y una ubicación para crear el recurso.\n",
    "\n",
    "tweetstopowerbi6\n",
    "nuevogrupo1\n",
    "(Europe) Este de Noruega\n",
    "10 unidades de procesamiento\n",
    "Crear\n",
    "\n",
    "Espere hasta que salga el mensaje: Implementación correcta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Crear un Event Hub\n",
    "\n",
    "A continuación, cree un Event Hub a través del espacio de nombres que acaba de crear. Haga clic dentro del mismo panel en el que está en **Event Hubs** debajo del título **Entidades** y luego **+ Event Hubs**. Dale un nombre y luego presiona Crear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 Dirijámonos al espacio de nombres recién construído, buscando en todos los recursos de Azure y démosle click a **Directivas de acceso compartido**, y luego a **RootManageSharedAccessKey**\n",
    "\n",
    "Esperemos unos segundo y copiemos lo siguiente para permitir  más adelante que Databricks envíe datos de permisos al Centro de eventos.:\n",
    "\n",
    "Clave principal\n",
    "ZhvoV1uJtdw1d8tTKmTM5PNB55gHCjSOn24yjVXcwQg=\n",
    "\n",
    "Clave secundaria\n",
    "QU5qhEkyX49aa5coS+u85IpjxzXzRvzzN9eBCoROybE=\n",
    "\n",
    "Cadena de conexión: clave principal\n",
    "Endpoint=sb://tweetstopowerbi7.servicebus.windows.net/;SharedAccessKeyName=RootManageSharedAccessKey;SharedAccessKey=ZhvoV1uJtdw1d8tTKmTM5PNB55gHCjSOn24yjVXcwQg=\n",
    "    \n",
    "Cadena de conexión: clave secundaria   \n",
    "Endpoint=sb://tweetstopowerbi7.servicebus.windows.net/;SharedAccessKeyName=RootManageSharedAccessKey;SharedAccessKey=QU5qhEkyX49aa5coS+u85IpjxzXzRvzzN9eBCoROybE=\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Ahora el **Event Hub** está listo para funcionar, y tenemos todas las cadenas de conexión necesarias para que Databricks envíe datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Event hub: tweetstopowerbi7\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 Construir un Workspace Databricks\n",
    "\n",
    "Ahora construyamos un Databricks Workspace. Busque **Azure Databricks** en **+ Crear un recurso** en Azure y da click en Crear. \n",
    "\n",
    "Proporcione su grupo de recursos: nuevogrupo1\n",
    "\n",
    " un **nombre de área de trabajo= tweetstopowerbi6**,\n",
    "\n",
    "ubicación: (Asia Pacific) Japón Occidental\n",
    "\n",
    "y Premium como el nivel de precios más alto - (¡Nota importante !: para cConnection a través de DirectQuery a PowerBIPower BI no funcionará sin esto, ¡necesitará esto!)\n",
    "\n",
    "Dá click en **Revisar y crear**\n",
    "\n",
    "luego click en **Crear**\n",
    "\n",
    "Cuando la implementación esté completa, damos click a **Ir al recurso**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nos guiamos ahora por:\n",
    "\n",
    "Create a Spark cluster in Databricks\n",
    "\n",
    "https://docs.microsoft.com/en-us/azure/cognitive-services/anomaly-detector/tutorials/anomaly-detection-streaming-databricks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6 le damos click  Iniciar área de trabajo, lo que nos redirecionará al portal de Azure Databrick, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7 Le damos clik a **New Cluster**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acepte todos los demás valores predeterminados que no sean los siguientes:\n",
    "\n",
    "Ingrese un nombre para el clúster: tweetstopowerbi6\n",
    "\n",
    "Para este artículo, cree un clúster con Runtime 6.2 (Scala 2.11, Spark 2.4.4)\n",
    "\n",
    "NO seleccione 5.3.\n",
    "\n",
    "Asegúrese de que la casilla de verificación: Terminar después de __ minutos de inactividad esté seleccionada. Proporcione una duración (en minutos) para terminar el clúster, si el clúster no se está utilizando.\n",
    "\n",
    "Seleccione **Crear clúster**.\n",
    "\n",
    "La creación del clúster lleva varios minutos. Una vez que el clúster se está ejecutando, puede adjuntar notebooks al clúster y ejecutar trabajos de Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8 Adjuntar bibliotecas al clúster de Spark\n",
    "\n",
    "En este tutorial, utilizará las API de Twitter para enviar tweets a Event Hubs. También usa el conector Apache Spark Event Hubs para leer y escribir datos en Azure Event Hubs. Para usar estas API como parte de su clúster, agréguelas como bibliotecas a Azure Databricks y luego asócielas con su clúster Spark. Las siguientes instrucciones muestran cómo agregar las bibliotecas a la carpeta Compartida en su espacio de trabajo.\n",
    "\n",
    "En el espacio de trabajo de Azure Databricks, seleccione Espacio de trabajo y luego haga clic con el botón derecho en Compartido. En el menú contextual, seleccione Crear> Biblioteca."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9 En la página Nueva biblioteca, para Fuente, seleccione Maven. Para Coordenadas, ingrese la coordenada para el paquete que desea agregar. Aquí están las coordenadas de Maven para las bibliotecas utilizadas en este tutorial:\n",
    "\n",
    "Conector de Spark Event Hubs: \n",
    "\n",
    "com.microsoft.azure:azure-eventhubs-spark_2.11:2.3.10\n",
    "\n",
    "API de Twitter - org.twitter4j: twitter4j-core: 4.0.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "añadimos los .jar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si no hay ningún clúster en la página de la biblioteca, seleccione Clústeres y ejecute el clúster que ha creado. Espere hasta que el estado muestre 'En ejecución' y luego regrese a la página de la biblioteca. En la página de la biblioteca, seleccione el clúster donde desea usar la biblioteca y luego seleccione Instalar. Una vez que la biblioteca se asocia correctamente con el clúster, el estado cambia inmediatamente a Instalado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 Kit de herramientas de servicios cognitivos\n",
    "\n",
    "Para calcular el sentimiento de los tweets, se requiere acceso a los Servicios Cognitivos de Microsoft. Esto permitirá que Databricks llame a la API de Text Analytics en tiempo casi real directamente desde el cuaderno y calcule el sentimiento de un tweet dado. Busque la API de Text Analytics en Azure Portal. \n",
    "\n",
    "Proporcione un nombre: tweetstopowerbi6\n",
    "\n",
    "ubicación: (Asia Pacific) Japón Oriental \n",
    "\n",
    "nivel de precios. (S (1000 llamadas por minuto))\n",
    "\n",
    "Grupo de recursos: nuevogrupo1\n",
    "\n",
    "Click en crear.\n",
    "\n",
    "Una vez implementado, haga clic en ir al recurso, y bajo ADMINISTRACIÓN  DE RECURSOS, dá click en **Claves y punto de conexión** Claves y tome nota de la URL del punto final y la Clave principal que se utilizará. Estos valores serán necesarios para que Databricks llame con éxito a la API de Text Analytics.\n",
    "\n",
    "NOMBRE:\n",
    "tweetstopowerbi7\n",
    "Extremo:\n",
    "https://tweetstopowerbi7.cognitiveservices.azure.com/\n",
    "Clave1:\n",
    "ef5cfde06ebb4b3fadb16305e61bc807\n",
    "Clave2:\n",
    "f6220e32b3b644bb8e12e25f9ecf018c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 Crear cuadernos en Databricks\n",
    "\n",
    "Crearemos dos notebooks en el workspace de Databricks con los siguientes nombres:\n",
    "\n",
    "* SendTweetsToEventHub: un cuaderno de productor que utiliza para obtener tweets de Twitter y transmitirlos a Event Hubs.\n",
    "* AnalyzeTweetsFromEventHub: una libreta de consumo que utiliza para leer los tweets de Event Hubs y ejecutar el análisis de sentimientos.\n",
    "\n",
    "En el espacio de trabajo de Azure Databricks, seleccione Espacio de trabajo en el panel izquierdo. En el menú desplegable Espacio de trabajo, seleccione Crear y luego seleccione Cuaderno.\n",
    "\n",
    "\n",
    "\n",
    "En el cuadro de diálogo Crear cuaderno, ingrese SendTweetsToEventHub como nombre, seleccione Scala como idioma y seleccione el clúster Spark que creó anteriormente.\n",
    "\n",
    "Crear cuaderno en Databricks\n",
    "\n",
    "Selecciona Crear.\n",
    "\n",
    "Repita los pasos para crear el cuaderno AnalyzeTweetsFromEventHub."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Notebook SendTweetsToEventHub\n",
    "\n",
    "El siguiente programa está escrito en Scala:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scala.collection.JavaConverters._\n",
    "    import com.microsoft.azure.eventhubs._\n",
    "    import java.util.concurrent._\n",
    "    import scala.collection.immutable._\n",
    "    import scala.concurrent.Future\n",
    "    import scala.concurrent.ExecutionContext.Implicits.global\n",
    "\n",
    "    val namespaceName = \"tweetstopowerbi7.servicebus.windows.net/\"\n",
    "    val eventHubName = \"tweetstopowerbi7\"\n",
    "    val sasKeyName = \"RootManageSharedAccessKey\"\n",
    "    val sasKey = \"ZhvoV1uJtdw1d8tTKmTM5PNB55gHCjSOn24yjVXcwQg=\"\n",
    "    val connStr = new ConnectionStringBuilder()\n",
    "                .setNamespaceName(namespaceName)\n",
    "                .setEventHubName(eventHubName)\n",
    "                .setSasKeyName(sasKeyName)\n",
    "                .setSasKey(sasKey)\n",
    "\n",
    "    val pool = Executors.newScheduledThreadPool(1)\n",
    "    val eventHubClient = EventHubClient.create(connStr.toString(), pool)\n",
    "\n",
    "    def sleep(time: Long): Unit = Thread.sleep(time)\n",
    "\n",
    "    def sendEvent(message: String, delay: Long) = {\n",
    "      sleep(delay)\n",
    "      val messageData = EventData.create(message.getBytes(\"UTF-8\"))\n",
    "      eventHubClient.get().send(messageData)\n",
    "      System.out.println(\"Sent event: \" + message + \"\\n\")\n",
    "    }\n",
    "\n",
    "    // Add your own values to the list\n",
    "    val testSource = List(\"Azure is the greatest!\", \"Azure isn't working :(\", \"Azure is okay.\")\n",
    "\n",
    "    // Specify 'test' if you prefer to not use Twitter API and loop through a list of values you define in `testSource`\n",
    "    // Otherwise specify 'twitter'\n",
    "\n",
    "    // val dataSource = \"test\"\n",
    "    val dataSource = \"twitter\"\n",
    "\n",
    "    if (dataSource == \"twitter\") {\n",
    "\n",
    "      import twitter4j._\n",
    "      import twitter4j.TwitterFactory\n",
    "      import twitter4j.Twitter\n",
    "      import twitter4j.conf.ConfigurationBuilder\n",
    "\n",
    "      // Twitter configuration!\n",
    "      // Replace values below with you\n",
    "\n",
    "      val twitterConsumerKey = \"koO4XqTuWFr5ADGcE8kjIkVoU\"\n",
    "      val twitterConsumerSecret = \"3F4sk9qU8zbKBROuLPUUj1uvE2YuhseXPe0ahMQoivg4icN5bL\"\n",
    "      val twitterOauthAccessToken = \"1230251564616515586-2KqPsCG2mIJp3irRjENgHpCfQUxTUg\"\n",
    "      val twitterOauthTokenSecret = \"6PJfMtYGY7w6csiIX9m1S5jFEKNZ3hE9PVkHKeN1S14iM\"\n",
    "\n",
    "      val cb = new ConfigurationBuilder()\n",
    "        cb.setDebugEnabled(true)\n",
    "        .setOAuthConsumerKey(twitterConsumerKey)\n",
    "        .setOAuthConsumerSecret(twitterConsumerSecret)\n",
    "        .setOAuthAccessToken(twitterOauthAccessToken)\n",
    "        .setOAuthAccessTokenSecret(twitterOauthTokenSecret)\n",
    "\n",
    "      val twitterFactory = new TwitterFactory(cb.build())\n",
    "      val twitter = twitterFactory.getInstance()\n",
    "\n",
    "      // Getting tweets with keyword \"Azure\" and sending them to the Event Hub in realtime!\n",
    "      val query = new Query(\" #covid\")\n",
    "      query.setCount(100)\n",
    "      query.lang(\"en\")\n",
    "      var finished = false\n",
    "      while (!finished) {\n",
    "        val result = twitter.search(query)\n",
    "        val statuses = result.getTweets()\n",
    "        var lowestStatusId = Long.MaxValue\n",
    "        for (status <- statuses.asScala) {\n",
    "          if(!status.isRetweet()){\n",
    "            sendEvent(status.getText(), 5000)\n",
    "          }\n",
    "          lowestStatusId = Math.min(status.getId(), lowestStatusId)\n",
    "        }\n",
    "        query.setMaxId(lowestStatusId - 1)\n",
    "      }\n",
    "\n",
    "    } else if (dataSource == \"test\") {\n",
    "      // Loop through the list of test input data\n",
    "      while (true) {\n",
    "        testSource.foreach {\n",
    "          sendEvent(_,5000)\n",
    "        }\n",
    "      }\n",
    "\n",
    "    } else {\n",
    "      System.out.println(\"Unsupported Data Source. Set 'dataSource' to \\\"twitter\\\" or \\\"test\\\"\")\n",
    "    }\n",
    "\n",
    "    // Closing connection to the Event Hub\n",
    "    eventHubClient.get().close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente paso es tomar esta secuencia de tweets y aplicarle un análisis de opinión. Las siguientes líneas de código leídas desde EventHub, llaman a la API de Text Analytics y pasan el cuerpo del tweet para que se calcule el análisis. Obtén el análisis de los Tweets en el notebook TweetSentiment.\n",
    "\n",
    "Agrega las siguientes líneas de código para llamar a la API de Text Analytics para calcular el análisis de opinión de la transmisión de Twitter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Creamos el notebook TweetSentiment, que analiza los tweets desde Event Hub.\n",
    "\n",
    "El siguiente programa está escrito en Scala:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "import org.apache.spark.eventhubs._\n",
    "\n",
    "// Build connection string with the above information\n",
    "val connectionString = ConnectionStringBuilder(\"Endpoint=sb://tweetstopowerbi7.servicebus.windows.net/;SharedAccessKeyName=RootManageSharedAccessKey;SharedAccessKey=ZhvoV1uJtdw1d8tTKmTM5PNB55gHCjSOn24yjVXcwQg=\")\n",
    "  .setEventHubName(\"tweetstopowerbi7\")\n",
    "  .build\n",
    "\n",
    "val customEventhubParameters =\n",
    "  EventHubsConf(connectionString)\n",
    "  .setMaxEventsPerTrigger(5)\n",
    ".setStartingPosition(EventPosition.fromEndOfStream) //added this with martin from the databricks offical doc\n",
    "\n",
    "val incomingStream = spark.readStream.format(\"eventhubs\").options(customEventhubParameters.toMap).load()\n",
    "incomingStream.printSchema\n",
    "// Event Hub message format is JSON and contains \"body\" field\n",
    "// Body is binary, so we cast it to string to see the actual content of the message\n",
    "val messages =\n",
    "  incomingStream\n",
    "  .withColumn(\"Offset\", $\"offset\".cast(LongType))\n",
    "  .withColumn(\"Time (readable)\", $\"enqueuedTime\".cast(TimestampType))\n",
    "  .withColumn(\"Timestamp\", $\"enqueuedTime\".cast(LongType))\n",
    "  .withColumn(\"Body\", $\"body\".cast(StringType))\n",
    "  .select(\"Timestamp\", \"Body\")\n",
    ".as[(String, String)] //added this with martin from the databrick official doc\n",
    "\n",
    "\n",
    "// NEW CODE CELL ----------\n",
    "\n",
    "import java.io._\n",
    "import java.net._\n",
    "import java.util._\n",
    "\n",
    "class Document(var id: String, var text: String, var language: String = \"\", var sentiment: Double = 0.0) extends Serializable\n",
    "\n",
    "class Documents(var documents: List[Document] = new ArrayList[Document]()) extends Serializable {\n",
    "\n",
    "    def add(id: String, text: String, language: String = \"\") {\n",
    "        documents.add (new Document(id, text, language))\n",
    "    }\n",
    "    def add(doc: Document) {\n",
    "        documents.add (doc)\n",
    "    }\n",
    "}\n",
    "\n",
    "// NEW CODE CELL ----------\n",
    "\n",
    "class CC[T] extends Serializable { def unapply(a:Any):Option[T] = Some(a.asInstanceOf[T]) }\n",
    "object M extends CC[scala.collection.immutable.Map[String, Any]]\n",
    "object L extends CC[scala.collection.immutable.List[Any]]\n",
    "object S extends CC[String]\n",
    "object D extends CC[Double]\n",
    "\n",
    "// NEW CODE CELL ----------\n",
    "\n",
    "import javax.net.ssl.HttpsURLConnection\n",
    "import com.google.gson.Gson\n",
    "import com.google.gson.GsonBuilder\n",
    "import com.google.gson.JsonObject\n",
    "import com.google.gson.JsonParser\n",
    "import scala.util.parsing.json._\n",
    "\n",
    "object SentimentDetector extends Serializable {\n",
    "\n",
    "  // Cognitive Services API connection settings\n",
    "  val accessKey = \"ef5cfde06ebb4b3fadb16305e61bc807\"\n",
    "  val host = \"https://tweetstopowerbi7.cognitiveservices.azure.com/\"\n",
    "  val languagesPath = \"/text/analytics/v2.0/languages\"\n",
    "  val sentimentPath = \"/text/analytics/v2.0/sentiment\"\n",
    "  val languagesUrl = new URL(host+languagesPath)\n",
    "  val sentimenUrl = new URL(host+sentimentPath)\n",
    "\n",
    "  def getConnection(path: URL): HttpsURLConnection = {\n",
    "    val connection = path.openConnection().asInstanceOf[HttpsURLConnection]\n",
    "    connection.setRequestMethod(\"POST\")\n",
    "    connection.setRequestProperty(\"Content-Type\", \"text/json\")\n",
    "    connection.setRequestProperty(\"Ocp-Apim-Subscription-Key\", accessKey)\n",
    "    connection.setDoOutput(true)\n",
    "    return connection\n",
    "  }\n",
    "\n",
    "  def prettify (json_text: String): String = {\n",
    "    val parser = new JsonParser()\n",
    "    val json = parser.parse(json_text).getAsJsonObject()\n",
    "    val gson = new GsonBuilder().setPrettyPrinting().create()\n",
    "    return gson.toJson(json)\n",
    "  }\n",
    "\n",
    "  // Handles the call to Cognitive Services API.\n",
    "  // Expects Documents as parameters and the address of the API to call.\n",
    "  // Returns an instance of Documents in response.\n",
    "  def processUsingApi(inputDocs: Documents, path: URL): String = {\n",
    "    val docText = new Gson().toJson(inputDocs)\n",
    "    val encoded_text = docText.getBytes(\"UTF-8\")\n",
    "    val connection = getConnection(path)\n",
    "    val wr = new DataOutputStream(connection.getOutputStream())\n",
    "    wr.write(encoded_text, 0, encoded_text.length)\n",
    "    wr.flush()\n",
    "    wr.close()\n",
    "\n",
    "    val response = new StringBuilder()\n",
    "    val in = new BufferedReader(new InputStreamReader(connection.getInputStream()))\n",
    "    var line = in.readLine()\n",
    "    while (line != null) {\n",
    "        response.append(line)\n",
    "        line = in.readLine()\n",
    "    }\n",
    "    in.close()\n",
    "    return response.toString()\n",
    "  }\n",
    "\n",
    "  // Calls the language API for specified documents.\n",
    "  // Returns a documents with language field set.\n",
    "  def getLanguage (inputDocs: Documents): Documents = {\n",
    "    try {\n",
    "      val response = processUsingApi(inputDocs, languagesUrl)\n",
    "      // In case we need to log the json response somewhere\n",
    "      val niceResponse = prettify(response)\n",
    "      val docs = new Documents()\n",
    "      val result = for {\n",
    "            // Deserializing the JSON response from the API into Scala types\n",
    "            Some(M(map)) <- scala.collection.immutable.List(JSON.parseFull(niceResponse))\n",
    "            L(documents) = map(\"documents\")\n",
    "            M(document) <- documents\n",
    "            S(id) = document(\"id\")\n",
    "            L(detectedLanguages) = document(\"detectedLanguages\")\n",
    "            M(detectedLanguage) <- detectedLanguages\n",
    "            S(language) = detectedLanguage(\"iso6391Name\")\n",
    "      } yield {\n",
    "            docs.add(new Document(id = id, text = id, language = language))\n",
    "      }\n",
    "      return docs\n",
    "    } catch {\n",
    "          case e: Exception => return new Documents()\n",
    "    }\n",
    "  }\n",
    "\n",
    "  // Calls the sentiment API for specified documents. Needs a language field to be set for each of them.\n",
    "  // Returns documents with sentiment field set, taking a value in the range from 0 to 1.\n",
    "  def getSentiment (inputDocs: Documents): Documents = {\n",
    "    try {\n",
    "      val response = processUsingApi(inputDocs, sentimenUrl)\n",
    "      val niceResponse = prettify(response)\n",
    "      val docs = new Documents()\n",
    "      val result = for {\n",
    "            // Deserializing the JSON response from the API into Scala types\n",
    "            Some(M(map)) <- scala.collection.immutable.List(JSON.parseFull(niceResponse))\n",
    "            L(documents) = map(\"documents\")\n",
    "            M(document) <- documents\n",
    "            S(id) = document(\"id\")\n",
    "            D(sentiment) = document(\"score\")\n",
    "      } yield {\n",
    "            docs.add(new Document(id = id, text = id, sentiment = sentiment))\n",
    "      }\n",
    "      return docs\n",
    "    } catch {\n",
    "        case e: Exception => return new Documents()\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "// User Defined Function for processing content of messages to return their sentiment.\n",
    "val toSentiment = udf((textContent: String) => {\n",
    "  val inputDocs = new Documents()\n",
    "  inputDocs.add (textContent, textContent)\n",
    "  val docsWithLanguage = SentimentDetector.getLanguage(inputDocs)\n",
    "  val docsWithSentiment = SentimentDetector.getSentiment(docsWithLanguage)\n",
    "  if (docsWithLanguage.documents.isEmpty) {\n",
    "    // Placeholder value to display for no score returned by the sentiment API\n",
    "    (-1).toDouble\n",
    "  } else {\n",
    "    docsWithSentiment.documents.get(0).sentiment.toDouble\n",
    "  }\n",
    "})\n",
    "\n",
    "// NEW CODE CELL ----------\n",
    "\n",
    "// Prepare a dataframe with Content and Sentiment columns\n",
    "val streamingDataFrame = incomingStream.selectExpr(\"cast (body as string) AS Content\").withColumn(\"body\", toSentiment($\"Content\"))\n",
    "\n",
    "// Display the streaming data with the sentiment\n",
    "\n",
    "streamingDataFrame.writeStream.outputMode(\"append\").format(\"console\").option(\"truncate\", false).start()\n",
    "\n",
    "// NEW CODE CELL ----------\n",
    "\n",
    "//WRITE THE STREAM TO PARQUET FORMAT/////\n",
    "import org.apache.spark.sql.streaming.Trigger.ProcessingTime\n",
    "\n",
    "val result =\n",
    "  streamingDataFrame\n",
    "    .writeStream\n",
    "    .format(\"parquet\")\n",
    "    .option(\"path\", \"/mnt/DatabricksSentimentPowerBI\")\n",
    "    .option(\"checkpointLocation\", \"/mnt/sample/check2\")\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Crear tabla de datos para que Power BI se conecte\n",
    "\n",
    "Necesitamos escribir los datos como formato **parquet** en el almacenamiento de blobs que pasan en la ruta de nuestro almacenamiento de blobs montado.\n",
    "\n",
    "Añadimos las siguientes líneas al final del programa TweetSentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//WRITE THE STREAM TO PARQUET FORMAT/////  \n",
    "import org.apache.spark.sql.streaming.Trigger.ProcessingTime \n",
    "val result = streamingDataFrame\n",
    ".writeStream\n",
    ".format(\"parquet\")\n",
    ".option(\"path\", \"/mnt/DatabricksSentimentPowerBI\")\n",
    ".option(\"checkpointLocation\", \"/mnt/sample/check2\")\n",
    ".start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para verificar que los datos se escriban en el almacenamiento de blobs montado directamente desde el notebook Databricks, crea un nuevo notebook **DatasetValidation** y ejecuta los siguientes comandos para mostrar el contenido de los archivos de parquet directamente dentro de Databricks. Si los datos se escriben correctamente, una salida al consultar la tabla en Databricks debería ser similar a la siguiente:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fig nnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Crear el notebook ScheduledTableCreate\n",
    "\n",
    "Ahora tenemos transmisión de datos de Twitter con el sentimiento adjunto que fluye en un almacenamiento de blobs montado. El siguiente paso es conectar Databricks (y este conjunto de datos) directamente a PowerBIPower BI para su posterior análisis y disección de datos. \n",
    "\n",
    "Para hacer esto, necesitamos escribir los archivos de parquet en un conjunto de datos que PowerBIPower BI podrá leer con éxito a intervalos regulares (es decir, actualizar continuamente el conjunto de datos a intervalos específicos para el flujo de datos por lotes). Para hacer esto, cree el cuaderno final ScheduledDatasetCreation y ejecute el siguiente conjunto de comandos scala como un programa para ejecutarse cada minuto. (Esto actualizará la tabla creada cada 1 minuto con el flujo de datos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7 Conectar Power BI al clúster de Databricks\n",
    "\n",
    "Para permitir que PowerBIPower BI se conecte primero a Databricks, se requiere que la información de conexión JDBC de los clústeres se proporcione como una dirección de servidor para la conexión PowerBIPower BI. \n",
    "\n",
    "Para obtener esto, navegue al clúster dentro de Databricks y seleccione el clúster que se va a conectar. En la página del clúster, seleccione la pestaña JDBC / ODBC (Nota: **si no creó un espacio de trabajo Premium Databricks, esta opción no estará disponible**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para construir la dirección del servidor, tome la URL JDBC que se muestra en el clúster y haga lo siguiente: • Reemplace jdbc: hive2 con https. • Elimine todo en la ruta entre el número de puerto y sql que retiene los componentes para que tenga una url similar a la siguiente: • https://westeurope.azuredatabricks.net:443/sql/protocolv1/o/1406775902027556/0424- 131603-inky272 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "jdbc:spark://southindia.azuredatabricks.net:443/default;transportMode=http;ssl=1;httpPath=sql/protocolv1/o/7401067854218687/0425-221454-peg462;AuthMech=3;UID=token;PWD=<personal-access-token>\n",
    "    \n",
    "#### Dirección el servidor:\n",
    "    \n",
    "https://southindia.azuredatabricks.net:443/sql/protocolv1/o/7401067854218687/0425-221454-peg462"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para generar el acceso personal a tokentoken, seleccione Configuración de usuario en el panel de control del clúster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crear un clúster de Apache Spark dentro de Databricks\n",
    "\n",
    "Para ejecutar notebooks para consumir el streming de data, primero se requiere un clúster. Para crear un clúster Apache Spark dentro de Databricks, cargue un Workspace desde el recurso Databricks se creó. Desde el portal de Databricks, seleccione Cluster.\n",
    "\n",
    "dá click en: 'iniciar area de trabajo'\n",
    "\n",
    "En un nuevo clúster, proporcione los siguientes valores para crear el clúster. ¡NOTA! - Para las capacidades de lectura / escritura en CosmosDB, se requiere una versión Apache Spark de 2.2.0. Al momento de escribir 2.3.0 aún no es compatible.\n",
    "\n",
    "Adjuntar bibliotecas a Spark Cluster\n",
    "\n",
    "Para permitir que la API de Twitter envíe tweets a Databricks y Databricks para leer y escribir datos en Event Hubs y CosmosDB, se requieren tres paquetes: \n",
    "\n",
    "• Conector Spark Event Hubs: com.microsoft.azure:azure-eventhubs-spark_2.11:2.3. 1 \n",
    "\n",
    "• API de Twitter - org.twitter4j: twitter4j-core: 4.0.6 \n",
    "\n",
    "• CosmosDB Spark Connector: http://repo1.maven.org/maven2/com/microsoft/azure/azure-cosmosdb-spark_2.2.0_2.11/ 1.1.1 / azure-cosmosdb-spark_2.2.0_2.11-1.1.1-uber.jar\n",
    "\n",
    "Haga clic derecho en el espacio de trabajo de Databricks y seleccione Crear> Biblioteca. En la página Nueva biblioteca, seleccione Maven Coordinate e ingrese los nombres de las bibliotecas anteriores. se mantiene se encuentra aquí: https://github.com/Azure/azure-cosmosdb-spark\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kit de herramientas de servicios cognitivos\n",
    "\n",
    "Para calcular el sentimiento de los tweets, se requiere acceso a los Servicios Cognitivos de Microsoft. Esto permitirá que Databricks llame a la API de Text Analytics en tiempo casi real directamente desde el cuaderno y calcule el sentimiento de un tweet dado. Busque la API de Text Analytics en Azure Portal. Proporcione un nombre, ubicación y nivel de precios. (F0 será suficiente para los propósitos de esta demostración)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez creado, haga clic en **Claves y puntos de conexión** y tome nota de la URL del punto final y la Clave principal que se utilizará. Estos valores serán necesarios para que Databricks llame con éxito a la API de Text Analytics.\n",
    "\n",
    "endpoint:\n",
    "https://tweetstopowerbi5.cognitiveservices.azure.com/\n",
    "\n",
    "\n",
    "clave 1:\n",
    "\n",
    "dae1210d95694ace8758d4cc0bea86d2\n",
    "\n",
    "clave 2:\n",
    "\n",
    "35a59dc1f1274710aa101ac432fb90d6\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crear notebooks Databricks\n",
    "\n",
    "Para ejecutar el código, necesitaremos crear 4 cuadernos en el espacio de trabajo de Databricks creado de la siguiente manera:\n",
    "\n",
    "* EventHubTweets (para enviar tweets al centro de eventos)\n",
    "* TweetSentiment (Para calcular el sentimiento de la secuencia de tweets del centro de eventos)\n",
    "* ScheduledDatasetCreation (Para crear y actualizar continuamente el conjunto de datos)\n",
    "* DatasetValidation (para validar el conjunto de datos directamente dentro de Databricks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crear tabla de datos para que Power BI se conecte\n",
    "\n",
    "Primero, necesitamos escribir datos como formato **parquet** en el almacenamiento de blobs que pasan en la ruta de nuestro almacenamiento de blobs montado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "//WRITE THE STREAM TO PARQUET FORMAT/////  \n",
    "import org.apache.spark.sql.streaming.Trigger.ProcessingTime \n",
    "val result = streamingDataFrame\n",
    ".writeStream\n",
    ".format(\"parquet\")\n",
    ".option(\"path\", \"/mnt/DatabricksSentimentPowerBI\")\n",
    ".option(\"checkpointLocation\", \"/mnt/sample/check2\")\n",
    ".start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conecte Power BI al clúster de Databricks\n",
    "\n",
    "Para permitir que PowerBIPower BI se conecte primero a Databricks, se requiere que la información de conexión JDBC de los clústeres se proporcione como una dirección de servidor para la conexión PowerBIPower BI. Para obtener esto, navegue al clúster dentro de Databricks y seleccione el clúster que se va a conectar. En la página del clúster, seleccione la pestaña JDBC / ODBC (Nota: si no creó un espacio de trabajo Premium Databricks, esta opción no estará disponible).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para construir la dirección del servidor, tome la URL JDBC que se muestra en el clúster y haga lo siguiente: • Reemplace jdbc: hive2 con https. • Elimine todo en la ruta entre el número de puerto y sql que retiene los componentes para que tenga una url similar a la siguiente: \n",
    "\n",
    "https://westeurope.azuredatabricks.net:443/sql/protocolv1/o/1406775902027556/0424- 131603-inky272 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para verificar que los datos se escriben en el almacenamiento de blobs montado directamente desde el cuaderno Databricks, cree un nuevo DatasetValidation del cuaderno y ejecute los siguientes comandos para mostrar el contenido de los archivos de parquet directamente dentro de Databricks. Si los datos se escriben correctamente, una salida al consultar la tabla en Databricks debería ser similar a la siguiente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora tenemos la transmisión de datos de Twitter con el sentimiento adjunto fluyendo hacia un almacenamiento de blobs montado. El siguiente paso es conectar Databricks (y este conjunto de datos) directamente a PowerBIPower BI para su posterior análisis y disección de datos. Para hacer esto, necesitamos escribir los archivos de parquet en un conjunto de datos que PowerBIPower BI podrá leer con éxito a intervalos regulares (es decir, actualizar continuamente el conjunto de datos a intervalos específicos para el flujo de datos por lotes). Para hacer esto, cree el cuaderno final ScheduledDatasetCreation y ejecute el siguiente conjunto de comandos scala como un programa para ejecutarse cada minuto. (Esto actualizará la tabla creada cada 1 minuto con el flujo de datos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El paso final es conectar Databricks a PowerBIPower BI para permitir el flujo de datos por lotes y realizar análisis. Para hacer esto, abra el escritorio de PowerBIPower BI abierto y haga clic en Obtener datos. Seleccione Spark (beta) para comenzar a configurar la conexión del clúster Databricks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://southindia.azuredatabricks.net:443/sql/protocolv1/o/7401067854218687/0425-221454-peg462"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para generar el acceso personal a tokentoken, seleccione Configuración de usuario en el panel de control del clúster.\n",
    "\n",
    "Token:\n",
    "\n",
    "dapi50bc7f1e8a1b5a3b24c1a58d9696dea7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
