{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Fri Oct 13 13:07:27 2017\n",
    "@author: walfaelschung\n",
    "\"\"\"\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "@author: walfaelschung\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "#%% required packages:\n",
    "import urllib.request\n",
    "import json\n",
    "import csv\n",
    "import re\n",
    "import time\n",
    "import sys\n",
    "import networkx as nx\n",
    "from networkx.algorithms import bipartite\n",
    "# pandas and os is only needed for the like network: TODO: get rid of it:\n",
    "import pandas\n",
    "import os\n",
    "\n",
    "#%%  like scripts that need review\n",
    "\n",
    "def getlikes(token,seed):\n",
    "    data = urllib.request.urlopen(\"https://graph.facebook.com/v2.9/\"+seed+\"/likes?limit=100&access_token=\"+token).read()\n",
    "    data = json.loads(data)\n",
    "    list = data\n",
    "    while \"paging\" in data and \"next\" in data[\"paging\"]:\n",
    "                        url = str(data[\"paging\"][\"next\"])\n",
    "                        data = urllib.request.urlopen(url).read()\n",
    "                        data = json.loads(data)\n",
    "                        for entry in data[\"data\"]:\n",
    "                            list[\"data\"].append(entry)\n",
    "    likenetwork = parselikes(list,seed, token)\n",
    "    print (\"..one more page found...\")\n",
    "    return likenetwork\n",
    "\n",
    "def depthone(token,seed):\n",
    "    a = getlikes(token,seed)\n",
    "    #return a\n",
    "    for page in a:\n",
    "        getlikes(token,str(page[\"targetid\"]))\n",
    "# open the first file manually\n",
    "    dtype_dic= {'sourceid': str, 'sourcename' : str, \"targetid\" : str, \"targetname\" : str}\n",
    "    k = pandas.read_csv(\"likenetwork_\"+seed+\".csv\", header=0, sep=\";\", dtype = dtype_dic)\n",
    "# and append every following file\n",
    "    for page in a:\n",
    "        l = pandas.read_csv(\"likenetwork_\"+page[\"targetid\"]+\".csv\", header=0, sep=\";\", dtype = dtype_dic)\n",
    "        k = k.append (l, ignore_index=True)\n",
    "\n",
    "    k.to_csv(\"likenetwork_depth_one_\"+seed+\".csv\", index=False, sep= \";\",encoding='utf-8')\n",
    "    os.remove(\"likenetwork_\"+seed+\".csv\")\n",
    "    for page in a:\n",
    "        os.remove(\"likenetwork_\"+page[\"targetid\"]+\".csv\")\n",
    "\n",
    "def getlikenetwork (idlist, token):\n",
    "    for seed in idlist:\n",
    "        print(\"Fetching page likes for page with ID \"+seed)\n",
    "        depthone (token,seed)\n",
    "    dtype_dic= {'sourceid': str, 'sourcename' : str, \"targetid\" : str, \"targetname\" : str}\n",
    "    p = pandas.read_csv(\"likenetwork_depth_one_\"+seed+\".csv\", header=0, sep=\";\", dtype = dtype_dic)\n",
    "    for page in idlist:\n",
    "        o = pandas.read_csv(\"likenetwork_depth_one_\"+page+\".csv\", header=0, sep=\";\", dtype = dtype_dic)\n",
    "        p = p.append (o, ignore_index=True)\n",
    "    p.to_csv(\"likenetwork_merged.csv\", index=False, sep= \";\", encoding='utf-8')\n",
    "\n",
    "    # now read in the csv and create the like network\n",
    "    dicti = []\n",
    "    pageids = []\n",
    "    targetids = []\n",
    "    with open (\"likenetwork_merged.csv\", encoding=\"utf-8\") as file:\n",
    "        readCSV = csv.reader(file, delimiter=';')\n",
    "        next(readCSV)\n",
    "        for row in readCSV:\n",
    "            pageid=row[0]\n",
    "            targetid=row[2]\n",
    "            pageids.append(pageid)\n",
    "            targetids.append(targetid)\n",
    "\n",
    "    for i in range(len(pageids)):\n",
    "        if pageids[i] in idlist and targetids[i] in idlist:\n",
    "            zeile = (pageids[i], targetids[i])\n",
    "            dicti.append(zeile)\n",
    "\n",
    "    # use networkx to make a file out of it and write to disk\n",
    "    G=nx.Graph()\n",
    "    G.add_edges_from(dicti)\n",
    "    pagenames = []\n",
    "    for i in idlist:\n",
    "        a = getpageinfo(token,i)\n",
    "        pagenames.append (a[\"name\"])\n",
    "    mapping = dict(zip(idlist, pagenames))\n",
    "    G=nx.relabel_nodes(G,mapping)\n",
    "    nx.write_graphml(G, \"like_network_pages.graphml\")\n",
    "    nx.write_weighted_edgelist(G, 'like_network_edgelist.csv', delimiter=\";\", encoding=\"utf-8\")\n",
    "    print(\"Done with the page likes - You'll find an edgelist in csv-format and a graphml-file in your working directory.\")\n",
    "    dicti = []\n",
    "    pageids = []\n",
    "    targetids = []\n",
    "    with open (\"likenetwork_merged.csv\", encoding=\"utf-8\") as file:\n",
    "        readCSV = csv.reader(file, delimiter=';')\n",
    "        next(readCSV)\n",
    "        for row in readCSV:\n",
    "            pageid=row[0]\n",
    "            targetid=row[2]\n",
    "            pageids.append(pageid)\n",
    "            targetids.append(targetid)\n",
    "    for i in range(len(pageids)):\n",
    "        if pageids[i] in idlist or targetids[i] in idlist:\n",
    "            if pageids[i] in idlist and targetids[i] in idlist:\n",
    "                pass\n",
    "            else:\n",
    "                zeile = (pageids[i], targetids[i])\n",
    "                dicti.append(zeile)\n",
    "    G=nx.Graph()\n",
    "    G.add_edges_from(dicti)\n",
    "    F = bipartite.weighted_projected_graph(G, idlist, ratio=False)\n",
    "    pagenames = []\n",
    "    for i in idlist:\n",
    "        a = getpageinfo(token,i)\n",
    "        pagenames.append (a[\"name\"])\n",
    "    mapping = dict(zip(idlist, pagenames))\n",
    "    F=nx.relabel_nodes(F,mapping)\n",
    "    nx.write_graphml(F, \"allies_network_pages.graphml\")\n",
    "    nx.write_weighted_edgelist(F, 'allies_network_edgelist.csv', delimiter=\";\", encoding=\"utf-8\")\n",
    "    print(\"Done with the allies (page like overlap) - You'll find an edgelist in csv-format and a graphml-file in your working directory.\")    \n",
    "    print(\"You can add metadata (category, description, etc.) to every node of your page like network.\")\n",
    "    print(\"This does take time, so please choose if you want to [c]ollect metadata for each page or [s]kip:\"),\n",
    "    prompt = '>'\n",
    "    skp = set(['skip','s'])\n",
    "    cllct = set(['collect','c'])\n",
    "    cors = input(prompt).lower()\n",
    "    if cors in skp:\n",
    "        pass\n",
    "    if cors in cllct:\n",
    "        print (\"Okay, let us collect aditional data:\")\n",
    "       \n",
    "        for page in idlist:\n",
    "            infolist = []\n",
    "\n",
    "            o = pandas.read_csv(\"likenetwork_depth_one_\"+page+\".csv\", header=0, sep=\";\")\n",
    "            newlist = list(o.targetid)\n",
    "            newlist.append(page)\n",
    "            newlist = list(set(newlist))\n",
    "            for newid in newlist:\n",
    "                information = getpageinfo(token,str(newid))\n",
    "                print(\"Collecting additional information on page \"+str(newid))\n",
    "                line = {\"id\":information[\"id\"],\"name\":information[\"name\"],\"category\":information[\"category\"],\"description\":\"about\",\"fans\":information[\"fan_count\"],\"talked_about\":information[\"talking_about_count\"],\"rating\":information[\"rating_count\"]}\n",
    "                if \"about\" in information:\n",
    "                    line[\"description\"] = information[\"about\"]\n",
    "                else: pass    \n",
    "                infolist.append(line)\n",
    "            with open(\"metadate_for_likenetwork_of_\"+page+\".csv\",\"a\", newline='', encoding=\"utf-8\") as file:\n",
    "                writer = csv.writer(file, delimiter=\";\")\n",
    "                writer.writerow([\"id\",\"name\",\"category\",\"description\",\"fans\",\"talked_about\",\"rating\"])\n",
    "                for entry in infolist:\n",
    "                    writer.writerow([entry[\"id\"], entry[\"name\"], entry[\"category\"], entry[\"description\"], entry[\"fans\"], entry[\"talked_about\"], entry[\"rating\"]])\n",
    "\n",
    "\n",
    "#%%\n",
    "\n",
    "def parselikes(list, seed, token):\n",
    "    namelist = getpagename(token,seed)\n",
    "    likeslist = []\n",
    "    for entry in list[\"data\"]:\n",
    "        zeile = {\"sourceid\":namelist[\"id\"], \"sourcename\":namelist[\"name\"],\"targetid\":entry[\"id\"],\"targetname\":entry[\"name\"]}\n",
    "        likeslist.append(zeile)\n",
    "    with open(\"likenetwork_\"+seed+\".csv\",\"w\", newline='', encoding=\"utf-8\") as file:\n",
    "        columns = [\"sourceid\", \"sourcename\", \"targetid\",\"targetname\"]\n",
    "        writer = csv.writer(file, delimiter=\";\")\n",
    "        writer.writerow(columns)\n",
    "        for entry in likeslist:\n",
    "            writer.writerow([entry[\"sourceid\"], entry[\"sourcename\"], entry[\"targetid\"], entry[\"targetname\"]])\n",
    "    return likeslist\n",
    "#%%\n",
    "def tryRequestData(url, errorcount):\n",
    "    if errorcount == 5:\n",
    "         print (\"Still can't establish connection. Log in and out of Facebook and try again later.\")\n",
    "         sys.exit()\n",
    "\n",
    "    try:\n",
    "        data= requestdata(url)\n",
    "    except urllib.error.URLError as error:\n",
    "        e = json.loads(error.read())\n",
    "        if e[\"error\"][\"code\"] == 190:\n",
    "            print(\"There seems to be a problem with your access token. Please enter valid token or enter q to quit:\")\n",
    "            token = input()\n",
    "            if token == \"q\":\n",
    "                print(token)\n",
    "                sys.exit()\n",
    "            url = re.sub(r\"token=.+\", \"token=\"+token, url)\n",
    "            errorcount = errorcount + 1\n",
    "            data= tryRequestData(url, errorcount)\n",
    "        else:\n",
    "            print(\"Unknown Facebook error. Trying again...\")\n",
    "            time.sleep(10)\n",
    "            errorcount = errorcount + 1\n",
    "            data = tryRequestData(url, errorcount)\n",
    "    except:\n",
    "\n",
    "        print (\"Can't establish connection. Trying again.\")\n",
    "        time.sleep(10)\n",
    "        errorcount = errorcount + 1\n",
    "        data= tryRequestData(url, errorcount)\n",
    "    return data\n",
    "\n",
    "def getpageinfo(token,seed):\n",
    "    try:\n",
    "        data = urllib.request.urlopen(\"https://graph.facebook.com/v2.9/\"+seed+\"/?fields=id,name,about,category,cover,fan_count,rating_count,talking_about_count&access_token=\"+token).read()\n",
    "        data = json.loads(data)\n",
    "        return data\n",
    "    except:\n",
    "        pass\n",
    "def requestdata(url):\n",
    "    data = urllib.request.urlopen(url).read()\n",
    "    return data\n",
    "def getpagename(token,seed):\n",
    "    data = urllib.request.urlopen(\"https://graph.facebook.com/v2.9/\"+seed+\"/?fields=id,name&access_token=\"+token).read()\n",
    "    data = json.loads(data)\n",
    "    return data\n",
    "\n",
    "def grab (idlist, token):\n",
    "    for seed in idlist:\n",
    "        getdata (token,seed)\n",
    "\n",
    "\n",
    "def getdata(token, seed, n):\n",
    "    #global data\n",
    "    url = \"https://graph.facebook.com/v2.9/\"+seed+\"/feed?fields=from,link,permalink_url,message,type,created_time,reactions.limit(1000),comments.limit(100){created_time,from,message}&limit=1&access_token=\"+token\n",
    "    datalist = []\n",
    "    rounds = 1\n",
    "    postcount = 1\n",
    "    reactioncount = 0\n",
    "    commentcount = 0\n",
    "    errorcount = 0\n",
    "    # get data from facebook graph api\n",
    "    print(\"Loading post 1\")\n",
    "    data = tryRequestData(url, errorcount)\n",
    "\n",
    "    data = json.loads(data)\n",
    "    # here's our first post with 100 comments and 1000 reactions. Before we move to the next post, we want to retrieve all comments and reactions\n",
    "    # get the comments:\n",
    "    for post in data[\"data\"]:\n",
    "        if \"comments\" in post:\n",
    "            if \"paging\" in post[\"comments\"] and \"next\" in post[\"comments\"][\"paging\"]:\n",
    "                urlc = str(post[\"comments\"][\"paging\"][\"next\"])\n",
    "                print(\"More than 100 comments found...Let's get them all!\")\n",
    "                datac = tryRequestData(urlc, errorcount)\n",
    "                #datac = urllib.request.urlopen(urlc).read()\n",
    "                datac = json.loads(datac)\n",
    "                for entry in datac[\"data\"]:\n",
    "                    data[\"data\"][0][\"comments\"][\"data\"].append(entry)\n",
    "                while \"paging\" in datac and \"next\" in datac[\"paging\"]:\n",
    "                    print (\"...and another 100...\")\n",
    "                    urld = str(datac[\"paging\"][\"next\"])\n",
    "                    datac = tryRequestData(urld, errorcount)\n",
    "                    datac = json.loads(datac)\n",
    "                    for entry in datac[\"data\"]:\n",
    "                        data[\"data\"][0][\"comments\"][\"data\"].append(entry)\n",
    "            print (\"I've got \"+str(len(data[\"data\"][0][\"comments\"][\"data\"]))+\" comments, let's move on..\")\n",
    "            commentcount = commentcount + len(data[\"data\"][0][\"comments\"][\"data\"])\n",
    "    # get the reactions\n",
    "    for post in data[\"data\"]:\n",
    "        if \"reactions\" in post:\n",
    "            if \"paging\" in post[\"reactions\"] and \"next\" in post[\"reactions\"][\"paging\"]:\n",
    "                urlr = str(post[\"reactions\"][\"paging\"][\"next\"])\n",
    "                print(\"More than 1000 reactions found...Let's get them all!\")\n",
    "                datar = tryRequestData(urlr, errorcount)\n",
    "                #datar = urllib.request.urlopen(urlr).read()\n",
    "                datar = json.loads(datar)\n",
    "                for entry in datar[\"data\"]:\n",
    "                    data[\"data\"][0][\"reactions\"][\"data\"].append(entry)\n",
    "                while \"paging\" in datar and \"next\" in datar[\"paging\"]:\n",
    "                    print (\"...and another 1000...\")\n",
    "                    urlt = str(datar[\"paging\"][\"next\"])\n",
    "                    datar = tryRequestData(urlt, errorcount)\n",
    "                    #datar = urllib.request.urlopen(urlt).read()\n",
    "                    datar = json.loads(datar)\n",
    "                    for entry in datar[\"data\"]:\n",
    "                        data[\"data\"][0][\"reactions\"][\"data\"].append(entry)\n",
    "            print (\"I've got \"+str(len(data[\"data\"][0][\"reactions\"][\"data\"]))+\" reactions, let's move on..\")\n",
    "            reactioncount = reactioncount + len(data[\"data\"][0][\"reactions\"][\"data\"])\n",
    "    datalist.append(data)\n",
    "# after the first post, we start writing the data to csv. This way, if something crashes during retrieval, the progress ist saved to disk.\n",
    "    namelist = getpagename(token,str(seed))\n",
    "    parsedata_first (datalist, seed,namelist)\n",
    "# as long as there are posts left, the loop shall continue:\n",
    "\n",
    "    while \"paging\" in data and \"next\" in data[\"paging\"]:\n",
    "        url = str(data[\"paging\"][\"next\"])\n",
    "        datalist_new = []\n",
    "        rounds += 1\n",
    "        postcount += 1\n",
    "        print(\"Loading post \"+ str(rounds))\n",
    "        data = tryRequestData(url, errorcount)\n",
    "        #data = urllib.request.urlopen(url).read()\n",
    "        data = json.loads(data)\n",
    "        # get the comments:\n",
    "        for post in data[\"data\"]:\n",
    "            if \"comments\" in post:\n",
    "                if \"paging\" in post[\"comments\"] and \"next\" in post[\"comments\"][\"paging\"]:\n",
    "                    urlc = str(post[\"comments\"][\"paging\"][\"next\"])\n",
    "                    print(\"More than 100 comments found...Let's get them all!\")\n",
    "                    datac = tryRequestData(urlc, errorcount)\n",
    "                    #datac = urllib.request.urlopen(urlc).read()\n",
    "                    datac = json.loads(datac)\n",
    "                    for entry in datac[\"data\"]:\n",
    "                        data[\"data\"][0][\"comments\"][\"data\"].append(entry)\n",
    "                    while \"paging\" in datac and \"next\" in datac[\"paging\"]:\n",
    "                        print (\"...and another 100...\")\n",
    "                        urld = str(datac[\"paging\"][\"next\"])\n",
    "                        datac = tryRequestData(urld, errorcount)\n",
    "                        #datac = urllib.request.urlopen(urld).read()\n",
    "                        datac = json.loads(datac)\n",
    "                        for entry in datac[\"data\"]:\n",
    "                            data[\"data\"][0][\"comments\"][\"data\"].append(entry)\n",
    "                print (\"I've got \"+str(len(data[\"data\"][0][\"comments\"][\"data\"]))+\" comments, let's move on..\")\n",
    "                commentcount = commentcount + len(data[\"data\"][0][\"comments\"][\"data\"])\n",
    "        # get the reactions\n",
    "        for post in data[\"data\"]:\n",
    "            if \"reactions\" in post:\n",
    "                if \"paging\" in post[\"reactions\"] and \"next\" in post[\"reactions\"][\"paging\"]:\n",
    "                    urlr = str(post[\"reactions\"][\"paging\"][\"next\"])\n",
    "                    print(\"More than 1000 reactions found...Let's get them all!\")\n",
    "                    datar = tryRequestData(urlr, errorcount)\n",
    "                    #datar = urllib.request.urlopen(urlr).read()\n",
    "                    datar = json.loads(datar)\n",
    "                    for entry in datar[\"data\"]:\n",
    "                        data[\"data\"][0][\"reactions\"][\"data\"].append(entry)\n",
    "                    while \"paging\" in datar and \"next\" in datar[\"paging\"]:\n",
    "                        print (\"...and another 1000...\")\n",
    "                        urlt = str(datar[\"paging\"][\"next\"])\n",
    "                        datar = tryRequestData(urlt, errorcount)\n",
    "                        #datar = urllib.request.urlopen(urlt).read()\n",
    "                        datar = json.loads(datar)\n",
    "                        for entry in datar[\"data\"]:\n",
    "                            data[\"data\"][0][\"reactions\"][\"data\"].append(entry)\n",
    "                print (\"I've got \"+str(len(data[\"data\"][0][\"reactions\"][\"data\"]))+\" reactions, let's move on..\")\n",
    "                reactioncount = reactioncount + len(data[\"data\"][0][\"reactions\"][\"data\"])\n",
    "        datalist_new.append(data)\n",
    "        datalist.append(data)\n",
    "        parsedata (datalist_new, seed,namelist)\n",
    "        if rounds == n:\n",
    "            break\n",
    "    print(\"Retrieved \"+str(postcount)+\" posts\")\n",
    "    print(\"Retrieved \"+str(commentcount)+\" comments\")\n",
    "    print(\"Retrieved \"+str(reactioncount)+\" reactions\")\n",
    "    print(\"Let me write a csv-file to your working directory...\")\n",
    "    print(\"Done.\")\n",
    "    #parsedata (datalist, seed)\n",
    "    return datalist\n",
    "\n",
    "def parsedata_first(datalist, seed,namelist):\n",
    "    list = []\n",
    "    for entry in datalist:\n",
    "        for post in entry[\"data\"]:\n",
    "            zeile = {\"id\":\"id\",\"name\":\"name\",\"time\":post[\"created_time\"],\"type\":post[\"type\"], \"permalink\":\"\", \"link\":\"\",\"message\":\"\"}\n",
    "            try:\n",
    "                zeile[\"message\"] = post[\"message\"]\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                zeile[\"permalink\"] = post[\"permalink_url\"]\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            try:\n",
    "                zeile[\"link\"] = post[\"link\"]\n",
    "            except:\n",
    "                pass\n",
    "            list.append(zeile)\n",
    "\n",
    "# dealing with comments\n",
    "            if \"comments\" in post:\n",
    "                for comment in post[\"comments\"][\"data\"]:\n",
    "                    zeile = {\"id\":comment[\"id\"],\"name\":comment[\"id\"],\"time\":comment[\"created_time\"],\"type\":\"comment\", \"permalink\":\"\", \"link\":\"\",\"message\":comment[\"message\"]}\n",
    "                    list.append(zeile)\n",
    "\n",
    "# dealing with reactions\n",
    "            if \"reactions\" in post:\n",
    "                for reaction in post[\"reactions\"][\"data\"]:\n",
    "                    zeile = {\"id\":reaction[\"id\"],\"name\":reaction[\"name\"],\"time\":post[\"created_time\"],\"type\":reaction[\"type\"], \"permalink\":\"\", \"link\":\"\", \"message\":\"\"}\n",
    "                    list.append(zeile)\n",
    "\n",
    "    with open(\"posts_from_\"+seed+\".csv\",\"w\", newline='', encoding=\"utf-8\") as file:\n",
    "        columns = [\"page_id\", \"page_name\", \"user_id\",\"timestamp\",\"type\",\"link\",\"permalink\",\"message\"]\n",
    "        writer = csv.writer(file, delimiter=\";\")\n",
    "        writer.writerow(columns)\n",
    "        for entry in list:\n",
    "            writer.writerow([namelist[\"id\"], namelist[\"name\"], entry[\"id\"], entry[\"time\"], entry[\"type\"], entry[\"link\"], entry[\"permalink\"], entry[\"message\"]])\n",
    "    return list\n",
    "\n",
    "#for all following posts, we use append mode\n",
    "def parsedata(datalist, seed,namelist):\n",
    "    list = []\n",
    "    for entry in datalist:\n",
    "        for post in entry[\"data\"]:\n",
    "            zeile = {\"id\":\"id\",\"name\":\"name\",\"time\":post[\"created_time\"],\"type\":post[\"type\"], \"permalink\":\"\", \"link\":\"\",\"message\":\"\"}\n",
    "            try:\n",
    "                zeile[\"message\"] = post[\"message\"]\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            try:\n",
    "                zeile[\"link\"] = post[\"link\"]\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                zeile[\"permalink\"] = post[\"permalink_url\"]\n",
    "            except:\n",
    "                pass\n",
    "            list.append(zeile)\n",
    "\n",
    "# dealing with comments\n",
    "            if \"comments\" in post:\n",
    "                for comment in post[\"comments\"][\"data\"]:\n",
    "                    zeile = {\"id\":comment[\"id\"],\"name\":comment[\"id\"],\"time\":comment[\"created_time\"],\"type\":\"comment\", \"permalink\":\"\", \"link\":\"\",\"message\":comment[\"message\"]}\n",
    "                    list.append(zeile)\n",
    "\n",
    "# dealing with reactions\n",
    "            if \"reactions\" in post:\n",
    "                for reaction in post[\"reactions\"][\"data\"]:\n",
    "                    zeile = {\"id\":reaction[\"id\"],\"name\":reaction[\"name\"],\"time\":post[\"created_time\"],\"type\":reaction[\"type\"], \"permalink\":\"\", \"link\":\"\", \"message\":\"\"}\n",
    "                    list.append(zeile)\n",
    "\n",
    "    with open(\"posts_from_\"+seed+\".csv\",\"a\", newline='', encoding=\"utf-8\") as file:\n",
    "        writer = csv.writer(file, delimiter=\";\")\n",
    "        for entry in list:\n",
    "            writer.writerow([namelist[\"id\"], namelist[\"name\"], entry[\"id\"], entry[\"time\"], entry[\"type\"], entry[\"link\"], entry[\"permalink\"], entry[\"message\"]])\n",
    "    return list\n",
    "\n",
    "def getdatar(token, seed, since, until):\n",
    "    #global data\n",
    "    url = \"https://graph.facebook.com/v2.9/\"+seed+\"/feed?fields=from,link,permalink_url,message,type,created_time,reactions.limit(1000),comments.limit(100){created_time,from,message}&limit=1&since=\"+since+\"&until=\"+until+\"&access_token=\"+token\n",
    "    datalist = []\n",
    "    rounds = 1\n",
    "    postcount = 1\n",
    "    reactioncount = 0\n",
    "    commentcount = 0\n",
    "    errorcount = 0\n",
    "    # get data from facebook graph api\n",
    "    print(\"Loading post 1\")\n",
    "    data = tryRequestData(url, errorcount)\n",
    "        #data = urllib.request.urlopen(\"https://graph.facebook.com/v2.9/\"+seed+\"/feed?fields=from,link,permalink_url,message,type,created_time,reactions.limit(1000),comments.limit(100){created_time,from,message}&limit=1&access_token=\"+token).read()\n",
    "\n",
    "    data = json.loads(data)\n",
    "    # here's our first post with 100 comments and 5000 reactions. Before we move to the next post, we want to retrieve all comments and reactions\n",
    "    # get the comments:\n",
    "    for post in data[\"data\"]:\n",
    "        if \"comments\" in post:\n",
    "            if \"paging\" in post[\"comments\"] and \"next\" in post[\"comments\"][\"paging\"]:\n",
    "                urlc = str(post[\"comments\"][\"paging\"][\"next\"])\n",
    "                print(\"More than 100 comments found...Let's get them all!\")\n",
    "                datac = tryRequestData(urlc, errorcount)\n",
    "                #datac = urllib.request.urlopen(urlc).read()\n",
    "                datac = json.loads(datac)\n",
    "                for entry in datac[\"data\"]:\n",
    "                    data[\"data\"][0][\"comments\"][\"data\"].append(entry)\n",
    "                while \"paging\" in datac and \"next\" in datac[\"paging\"]:\n",
    "                    print (\"...and another 100...\")\n",
    "                    urld = str(datac[\"paging\"][\"next\"])\n",
    "                    datac = tryRequestData(urld, errorcount)\n",
    "                    #datac = urllib.request.urlopen(urld).read()\n",
    "                    datac = json.loads(datac)\n",
    "                    for entry in datac[\"data\"]:\n",
    "                        data[\"data\"][0][\"comments\"][\"data\"].append(entry)\n",
    "            print (\"I've got \"+str(len(data[\"data\"][0][\"comments\"][\"data\"]))+\" comments, let's move on..\")\n",
    "            commentcount = commentcount + len(data[\"data\"][0][\"comments\"][\"data\"])\n",
    "    # get the reactions\n",
    "    for post in data[\"data\"]:\n",
    "        if \"reactions\" in post:\n",
    "            if \"paging\" in post[\"reactions\"] and \"next\" in post[\"reactions\"][\"paging\"]:\n",
    "                urlr = str(post[\"reactions\"][\"paging\"][\"next\"])\n",
    "                print(\"More than 1000 reactions found...Let's get them all!\")\n",
    "                datar = tryRequestData(urlr, errorcount)\n",
    "                #datar = urllib.request.urlopen(urlr).read()\n",
    "                datar = json.loads(datar)\n",
    "                for entry in datar[\"data\"]:\n",
    "                    data[\"data\"][0][\"reactions\"][\"data\"].append(entry)\n",
    "                while \"paging\" in datar and \"next\" in datar[\"paging\"]:\n",
    "                    print (\"...and another 1000...\")\n",
    "                    urlt = str(datar[\"paging\"][\"next\"])\n",
    "                    datar = tryRequestData(urlt, errorcount)\n",
    "                    #datar = urllib.request.urlopen(urlt).read()\n",
    "                    datar = json.loads(datar)\n",
    "                    for entry in datar[\"data\"]:\n",
    "                        data[\"data\"][0][\"reactions\"][\"data\"].append(entry)\n",
    "            print (\"I've got \"+str(len(data[\"data\"][0][\"reactions\"][\"data\"]))+\" reactions, let's move on..\")\n",
    "            reactioncount = reactioncount + len(data[\"data\"][0][\"reactions\"][\"data\"])\n",
    "    datalist.append(data)\n",
    "# after the first post, we start writing the data to csv. This way, if something crashes during retrieval, the progress ist saved to disk.\n",
    "    namelist = getpagename(token,str(seed))\n",
    "    parsedata_first (datalist, seed,namelist)\n",
    "# as long as there are posts left, the loop shall continue:\n",
    "\n",
    "    while \"paging\" in data and \"next\" in data[\"paging\"]:\n",
    "        url = str(data[\"paging\"][\"next\"])\n",
    "        datalist_new = []\n",
    "        rounds += 1\n",
    "        postcount += 1\n",
    "        print(\"Loading post \"+ str(rounds))\n",
    "        data = tryRequestData(url, errorcount)\n",
    "        #data = urllib.request.urlopen(url).read()\n",
    "        data = json.loads(data)\n",
    "        # get the comments:\n",
    "        for post in data[\"data\"]:\n",
    "            if \"comments\" in post:\n",
    "                if \"paging\" in post[\"comments\"] and \"next\" in post[\"comments\"][\"paging\"]:\n",
    "                    urlc = str(post[\"comments\"][\"paging\"][\"next\"])\n",
    "                    print(\"More than 100 comments found...Let's get them all!\")\n",
    "                    datac = tryRequestData(urlc, errorcount)\n",
    "                    #datac = urllib.request.urlopen(urlc).read()\n",
    "                    datac = json.loads(datac)\n",
    "                    for entry in datac[\"data\"]:\n",
    "                        data[\"data\"][0][\"comments\"][\"data\"].append(entry)\n",
    "                    while \"paging\" in datac and \"next\" in datac[\"paging\"]:\n",
    "                        print (\"...and another 100...\")\n",
    "                        urld = str(datac[\"paging\"][\"next\"])\n",
    "                        datac = tryRequestData(urld, errorcount)\n",
    "                        #datac = urllib.request.urlopen(urld).read()\n",
    "                        datac = json.loads(datac)\n",
    "                        for entry in datac[\"data\"]:\n",
    "                            data[\"data\"][0][\"comments\"][\"data\"].append(entry)\n",
    "                print (\"I've got \"+str(len(data[\"data\"][0][\"comments\"][\"data\"]))+\" comments, let's move on..\")\n",
    "                commentcount = commentcount + len(data[\"data\"][0][\"comments\"][\"data\"])\n",
    "        # get the reactions\n",
    "        for post in data[\"data\"]:\n",
    "            if \"reactions\" in post:\n",
    "                if \"paging\" in post[\"reactions\"] and \"next\" in post[\"reactions\"][\"paging\"]:\n",
    "                    urlr = str(post[\"reactions\"][\"paging\"][\"next\"])\n",
    "                    print(\"More than 1000 reactions found...Let's get them all!\")\n",
    "                    datar = tryRequestData(urlr, errorcount)\n",
    "                    #datar = urllib.request.urlopen(urlr).read()\n",
    "                    datar = json.loads(datar)\n",
    "                    for entry in datar[\"data\"]:\n",
    "                        data[\"data\"][0][\"reactions\"][\"data\"].append(entry)\n",
    "                    while \"paging\" in datar and \"next\" in datar[\"paging\"]:\n",
    "                        print (\"...and another 1000...\")\n",
    "                        urlt = str(datar[\"paging\"][\"next\"])\n",
    "                        datar = tryRequestData(urlt, errorcount)\n",
    "                        #datar = urllib.request.urlopen(urlt).read()\n",
    "                        datar = json.loads(datar)\n",
    "                        for entry in datar[\"data\"]:\n",
    "                            data[\"data\"][0][\"reactions\"][\"data\"].append(entry)\n",
    "                print (\"I've got \"+str(len(data[\"data\"][0][\"reactions\"][\"data\"]))+\" reactions, let's move on..\")\n",
    "                reactioncount = reactioncount + len(data[\"data\"][0][\"reactions\"][\"data\"])\n",
    "        datalist_new.append(data)\n",
    "        datalist.append(data)\n",
    "        parsedata (datalist_new, seed,namelist)\n",
    "#        if rounds == n:\n",
    "#            break\n",
    "    print(\"Retrieved \"+str(postcount)+\" posts\")\n",
    "    print(\"Retrieved \"+str(commentcount)+\" comments\")\n",
    "    print(\"Retrieved \"+str(reactioncount)+\" reactions\")\n",
    "    print(\"Let me write a csv-file to your working directory...\")\n",
    "    print(\"Done.\")\n",
    "    #parsedata (datalist, seed)\n",
    "    return datalist\n",
    "\n",
    "#%% auxiliary scripts not for data-collection but for the user-input\n",
    "def listinput ():\n",
    "    prompt = '> '\n",
    "    idlist = []\n",
    "    print (\"Please input the first Facebook page ID you want to collect information about\")\n",
    "    a = input(prompt)\n",
    "    idlist.append(a)\n",
    "    print (\"And the next one please...\")\n",
    "    a = input(prompt)\n",
    "    idlist.append(a)\n",
    "    nextinput (idlist)\n",
    "    return idlist\n",
    "\n",
    "def nextinput (idlist):\n",
    "    prompt = '> '\n",
    "    while True:\n",
    "        print (\"Add another ID or press [s]tart to collect data\")\n",
    "        strt = set(['start','s'])\n",
    "        a = input(prompt).lower()\n",
    "        if a in strt:\n",
    "            return idlist\n",
    "        else:\n",
    "            idlist.append(a)\n",
    "            continue\n",
    "#%% change id deals with the problem of bipartite networks if user-id and page-id are identical due to admins posting on a site\n",
    "            # admin id is replaced here.\n",
    "\n",
    "def change_id(idlist, userids):\n",
    "    output = []\n",
    "    for i in userids:\n",
    "        if i in idlist:\n",
    "            i= i.replace(str(i),str(i+\"admin\"))\n",
    "            output.append(i)\n",
    "        else:\n",
    "            output.append(i)\n",
    "    return output\n",
    "#%% the scripts for the network creation once the data is collected:\n",
    "def networks(idlist, token):\n",
    "    \n",
    "\n",
    "    print (\"Let me calculate a network of content overlap between the pages you provided...\")\n",
    "    # next we set a regular expression to extract links. MIT license from Diego Perini (https://gist.github.com/dperini/729294)\n",
    "    #URL_REGEX = r'^(?:(?:https?|ftp)://)(?:\\S+(?::\\S*)?@)?(?:(?:[1-9]\\d?|1\\d\\d|2[01]\\d|22[0-3])(?:\\.(?:1?\\d{1,2}|2[0-4]\\d|25[0-5])){2}(?:\\.(?:[1-9]\\d?|1\\d\\d|2[0-4]\\d|25[0-4]))|(?:(?:[a-z\\u00a1-\\uffff0-9]+-?)*[a-z\\u00a1-\\uffff0-9]+)(?:\\.(?:[a-z\\u00a1-\\uffff0-9]+-?)*[a-z\\u00a1-\\uffff0-9]+)*(?:\\.(?:[a-z\\u00a1-\\uffff]{2,})))(?::\\d{2,5})?(?:/[^\\s]*)?$'\n",
    "    URL_REGEX = re.compile(\n",
    "        u\"^\"\n",
    "        # protocol identifier\n",
    "        u\"(?:(?:https?|ftp)://)\"\n",
    "        # user:pass authentication\n",
    "        u\"(?:\\S+(?::\\S*)?@)?\"\n",
    "        u\"(?:\"\n",
    "        # IP address exclusion\n",
    "        # private & local networks\n",
    "        u\"(?!(?:10|127)(?:\\.\\d{1,3}){3})\"\n",
    "        u\"(?!(?:169\\.254|192\\.168)(?:\\.\\d{1,3}){2})\"\n",
    "        u\"(?!172\\.(?:1[6-9]|2\\d|3[0-1])(?:\\.\\d{1,3}){2})\"\n",
    "        # IP address dotted notation octets\n",
    "        # excludes loopback network 0.0.0.0\n",
    "        # excludes reserved space >= 224.0.0.0\n",
    "        # excludes network & broadcast addresses\n",
    "        # (first & last IP address of each class)\n",
    "        u\"(?:[1-9]\\d?|1\\d\\d|2[01]\\d|22[0-3])\"\n",
    "        u\"(?:\\.(?:1?\\d{1,2}|2[0-4]\\d|25[0-5])){2}\"\n",
    "        u\"(?:\\.(?:[1-9]\\d?|1\\d\\d|2[0-4]\\d|25[0-4]))\"\n",
    "        u\"|\"\n",
    "        # host name\n",
    "        u\"(?:(?:[a-z\\u00a1-\\uffff0-9]-?)*[a-z\\u00a1-\\uffff0-9]+)\"\n",
    "        # domain name\n",
    "        u\"(?:\\.(?:[a-z\\u00a1-\\uffff0-9]-?)*[a-z\\u00a1-\\uffff0-9]+)*\"\n",
    "        # TLD identifier\n",
    "        u\"(?:\\.(?:[a-z\\u00a1-\\uffff]{2,}))\"\n",
    "        u\")\"\n",
    "        # port number\n",
    "        u\"(?::\\d{2,5})?\"\n",
    "        # resource path\n",
    "        u\"(?:/\\S*)?\"\n",
    "        u\"$\"\n",
    "        , re.UNICODE)\n",
    "    content_edgelist = []\n",
    "    idict = {}\n",
    "    actdict = {}\n",
    "    cdict = {}\n",
    "    pdict = {}\n",
    "    rdict = {}\n",
    "    ids = idlist\n",
    "\n",
    "    for i in ids:\n",
    "        linkids = []\n",
    "        messageids = []\n",
    "        pageids = []\n",
    "    \n",
    "        with open (\"posts_from_\"+i+\".csv\", \"r+\", encoding=\"utf-8\") as file:\n",
    "            readCSV = csv.reader((x.replace('\\0', '') for x in file), delimiter=';')\n",
    "            next(readCSV)\n",
    "            rows = 0\n",
    "            typesof = []\n",
    "            for row in readCSV:\n",
    "                pageid=row[0]\n",
    "                typeof=row[4]\n",
    "                linkid=row[5]\n",
    "                messageid=row[7]\n",
    "                pageids.append(pageid)\n",
    "                linkids.append(linkid)\n",
    "                messageids.append(messageid)\n",
    "                typesof.append(typeof)\n",
    "                rows = rows+1\n",
    "            actdict.update({i: rows})\n",
    "            cdict.update({i: typesof.count(\"comment\")})\n",
    "            pdict.update({i: typesof.count(\"status\") + typesof.count(\"photo\") + typesof.count(\"video\") + typesof.count(\"link\") + typesof.count(\"event\")})\n",
    "            rdict.update({i: typesof.count(\"LIKE\") + typesof.count(\"SAD\") + typesof.count(\"HAHA\") + typesof.count(\"LOVE\") + typesof.count(\"ANGRY\")+ typesof.count(\"WOW\")})\n",
    "            urls = []\n",
    "            for x in messageids:\n",
    "                if isinstance(x, str) == True:\n",
    "                    for url in re.findall(URL_REGEX, x):\n",
    "                        urls.append(url)\n",
    "            for x in linkids:\n",
    "                if isinstance(x, str) == True:\n",
    "                    for url in re.findall(URL_REGEX, x):\n",
    "                        urls.append(url)\n",
    "            urlset = set(urls)\n",
    "            uniqueurls = list(urlset)\n",
    "            idict.update({i: len(urlset)})\n",
    "            for i in range(len(uniqueurls)):\n",
    "                zeile = (uniqueurls[i], pageids[0])\n",
    "                content_edgelist.append(zeile)\n",
    "     # trim idlist and dicts for empty entries:\n",
    "    actdict = {k: v for k, v in actdict.items() if v != 0}\n",
    "    ids = list(actdict.keys())\n",
    "    #    udict = {k: v for k, v in udict.items() if k in ids}\n",
    "    cdict = {k: v for k, v in cdict.items() if k in ids}\n",
    "    pdict = {k: v for k, v in pdict.items() if k in ids}\n",
    "    rdict = {k: v for k, v in rdict.items() if k in ids}\n",
    "    \n",
    "    with open(\"edgelist_content.csv\", \"w\", newline='', encoding=\"utf-8\") as file:\n",
    "        writer = csv.writer(file, delimiter=\";\")\n",
    "        writer.writerow([\"Source\",\"Target\"])\n",
    "        for i in range(len(content_edgelist)):\n",
    "            writer.writerow(content_edgelist[i])\n",
    "    \n",
    "    #\n",
    "    G=nx.MultiGraph()\n",
    "    G.add_edges_from(content_edgelist)\n",
    "    for u,v,data in G.edges(data=True):\n",
    "        w = data['weight'] = 1.0\n",
    "        G.edges(data=True)\n",
    "    H = nx.Graph()\n",
    "    for u,v,data in G.edges(data=True):\n",
    "        w = data['weight']\n",
    "        if H.has_edge(u,v):\n",
    "            H[u][v]['weight'] += w\n",
    "        else:\n",
    "            H.add_edge(u, v, weight=w)\n",
    "    #    H.edges(data=True)\n",
    "    #    H.is_directed()\n",
    "    #    nx.is_bipartite(H)\n",
    "    F = bipartite.weighted_projected_graph(H, ids, ratio=False)\n",
    "    nx.set_node_attributes(F,actdict, 'total_activities')\n",
    "    nx.set_node_attributes(F,cdict, 'comments')\n",
    "    nx.set_node_attributes(F, pdict,'posts')\n",
    "    nx.set_node_attributes(F, rdict,'reactions')\n",
    "    elist = list(F.edges())\n",
    "    for i in elist:\n",
    "        F[i[0]][i[1]]['sfrac'] = F[i[0]][i[1]]['weight'] / idict.get(i[0])\n",
    "        F[i[0]][i[1]]['tfrac'] = F[i[0]][i[1]]['weight'] / idict.get(i[1])\n",
    "        if F[i[0]][i[1]]['sfrac'] < F[i[0]][i[1]]['tfrac']:\n",
    "            F[i[0]][i[1]]['maxfrac'] = F[i[0]][i[1]]['sfrac']\n",
    "        else:\n",
    "            F[i[0]][i[1]]['maxfrac'] = F[i[0]][i[1]]['tfrac']\n",
    "    pagenames = []\n",
    "    for i in ids:\n",
    "        a = getpageinfo(token,i)\n",
    "        pagenames.append (a[\"name\"])\n",
    "    mapping = dict(zip(ids, pagenames))\n",
    "    F=nx.relabel_nodes(F,mapping)\n",
    "    nx.write_graphml(F, \"content_projection_pages.graphml\")\n",
    "    nx.write_weighted_edgelist(F, 'content_projection_edgelist.csv', delimiter=\";\", encoding=\"utf-8\")\n",
    "    L = bipartite.weighted_projected_graph(H, uniqueurls, ratio=False)\n",
    "    nx.write_graphml(L, \"content_projection_content.graphml\")\n",
    "    nx.write_weighted_edgelist(F, 'content_projection_content_edgelist.csv', delimiter=\";\", encoding=\"utf-8\")\n",
    "    print(\"Done with the content overlap - You'll find a weighted edgelist in csv-format and a graphml-file in your working directory.\")\n",
    "    \n",
    "    print(\"Do you want to collect the network of page-likes as well?\")\n",
    "    print(\"Note: This takes a while is not possible in retrospect. It will always collect the status quo.\")\n",
    "    print(\"So if you have already done it, for your set of pages, we recommend to skip.\")\n",
    "    print(\"Please provide a choice: [c]ollect page-like data or [s]kip:\"),\n",
    "    prompt = '>'\n",
    "    skp = set(['skip','s'])\n",
    "    cllct = set(['collect','c'])\n",
    "    cors = input(prompt).lower()\n",
    "    if cors in skp:\n",
    "        pass\n",
    "    if cors in cllct:\n",
    "        print (\"Okay, let us collect the network of page-likes\")\n",
    "        getlikenetwork (ids, token)\n",
    "\n",
    "\n",
    "#%%\n",
    "\n",
    "\n",
    "#%%\n",
    "def main():\n",
    "    token, idlist = prep()\n",
    "    user(token, idlist)\n",
    "\n",
    "\n",
    "def prep():\n",
    "    print (\"Preparing functions.\")\n",
    "    print (\"Done.\")\n",
    "    prompt = '> '\n",
    "    print (\"Welcome to the Sammlr application.\")\n",
    "    print (\"This tool will allow you to collect posts, comments, and reactions from a public Facebook page or create networks from multiple public Facebook pages.\")\n",
    "    print (\"You are required to enter an acces token to the Facebook Graph API.\")\n",
    "    print (\"Please provide your token before we start:\")\n",
    "    token = input(prompt)\n",
    "    print (\"Thanks.\")\n",
    "    print (\"Now tell me, do you want to collect raw data from one page, or do you want the network of multiple pages?\")\n",
    "    print (\"Please choose [s]ingle page or [n]etwork for more than one page:\")\n",
    "    sngl = set(['single','s'])\n",
    "    ntwrk = set(['network','n'])\n",
    "    sorn = input(prompt).lower()\n",
    "    if sorn in sngl:\n",
    "        print (\"Please input the Facebook page ID you want to collect information about\")\n",
    "        seed = input(prompt)\n",
    "        print (\"Thanks.\")\n",
    "        pinfo = getpageinfo(token, seed)\n",
    "        print(\"You want to collect data from the page \"+pinfo[\"name\"])\n",
    "        idlist = []\n",
    "        idlist.append(seed)\n",
    "        return token, idlist\n",
    "    if sorn in ntwrk:\n",
    "        idlist = listinput()\n",
    "        return token, idlist\n",
    "def user(token, idlist):\n",
    "    prompt = '> '\n",
    "    print(\"By default, Sammlr will collect information on the last 100 posts\")\n",
    "    print(\"Alternatively, you can specify a date range or the number of posts to collect.\")\n",
    "    dflt = set(['default','d'])\n",
    "    rng = set(['range','r'])\n",
    "    nmbr = set(['number','n'])\n",
    "    n = 100\n",
    "    print(\"Please choose [d]efault, [r]ange, [n]umber, or any other key to exit:\")\n",
    "    choice = input(prompt).lower()\n",
    "    if choice in dflt:\n",
    "        print(\"You have chosen the default setting to collect the last 100 posts.\")\n",
    "        for i in idlist:\n",
    "            seed = i\n",
    "            try:\n",
    "                getdata(token,seed,n)\n",
    "            except:\n",
    "                pass\n",
    "    if choice in nmbr:\n",
    "        print(\"You have chosen to specify the number of latests posts you want to collect.\")\n",
    "        print(\"Please provide a whole number:\")\n",
    "        n = int(input(prompt))\n",
    "        for i in idlist:\n",
    "            seed = i\n",
    "            try:\n",
    "                getdata(token,seed,n)\n",
    "            except:\n",
    "                pass\n",
    "    if choice in rng:\n",
    "        print(\"You have chosen to specify a date range for data collection.\")\n",
    "        print(\"Please provide the starting day in the format yyyy-mm-dd:\")\n",
    "        since = input(prompt).lower()\n",
    "        print(\"Please provide the finishing day in the format yyyy-mm-dd:\")\n",
    "        until = input(prompt).lower()\n",
    "        print(\"Allright, you want to collect data from \"+since+\" until \"+until)\n",
    "        print(\"If that is correct, please enter [y]es, otherwise press [r]estart or any key to exit.\")\n",
    "        yes = set(['yes','y'])\n",
    "        restart = set(['restart','r'])\n",
    "        choicetwo = input(prompt).lower()\n",
    "        if choicetwo in yes:\n",
    "            print(\"Allright, let's go.\")\n",
    "            for i in idlist:\n",
    "                seed = i\n",
    "                try:\n",
    "                    getdatar(token,seed,since,until)\n",
    "                except:\n",
    "                    pass\n",
    "        if choicetwo in restart:\n",
    "            user(idlist,seed)\n",
    "        else:\n",
    "            pass\n",
    "        #restart\n",
    "    if len(idlist) > 1:\n",
    "        networks(idlist,token)\n",
    "\n",
    "    else:\n",
    "        print(\"Bye, thanks for using Sammlr.\")\n",
    "  #%%\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
