{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming estructurado con Azure Databricks en Power BI y Cosmos DB para el despliegue y análisis de tweets \n",
    "\n",
    "####Fuentes:\n",
    "\n",
    "Structured Streaming with Azure Databricks into Power BI & Cosmos DB\n",
    "\n",
    "https://github.com/giulianorapoz/DatabricksStreamingPowerBI\n",
    "\n",
    "Tutorial: Anomaly detection on streaming data using Azure Databricks\n",
    "\n",
    "https://docs.microsoft.com/en-us/azure/cognitive-services/anomaly-detector/tutorials/anomaly-detection-streaming-databricks\n",
    "\n",
    "\n",
    "Tutorial: Sentiment analysis on streaming data using Azure Databricks\n",
    "\n",
    "https://docs.microsoft.com/en-us/azure/azure-databricks/databricks-sentiment-analysis-cognitive-services\n",
    "\n",
    "\n",
    "Power BI\n",
    "\n",
    "https://docs.microsoft.com/en-us/azure/databricks/integrations/bi/power-bi\n",
    "\n",
    "Power BI Connects to Azure Databricks\n",
    "\n",
    "https://towardsdatascience.com/power-bi-connects-to-azure-databricks-44bea6731be7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Christian Castro \n",
      "last updated: 2020-04-26 \n",
      "\n",
      "numpy 1.18.1\n",
      "pandas 1.0.1\n",
      "matplotlib 3.1.3\n",
      "Propiedad de DataIntelligence\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark -a \"Christian Castro\" -u -d -p numpy,pandas,matplotlib\n",
    "%watermark -a \"Propiedad de DataIntelligence\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este notebook nos basaremos en el concepto de Streaming estructurado con Databricks y en el cómo se puede conectar directamente para utilizarlo junto con Power BI y Cosmos DB, lo que permite la visualización y análisis avanzados para una mayor disección de los datos consumidos por streaming estructurados. Construiremos una ruta de consumo de datos directamente con Azure Databricks que nos permitirá transmitir datos a un clúster de Apache Spark en tiempo casi real. Mostraremos algunas de las capacidades de análisis a las que se puede llamar directamente desde Databricks utilizando la API de Text Analytics, luego conectaremos Databricks directamente a Power BI para análisis e informes de disección de datos adicionales. Como paso final, leeremos y escribiremos desde Databricks directamente en CosmosDB como almacenamiento persistente y uso posterior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El primer paso es configurar todos nuestros recursos individuales. Necesitaremos lo siguiente:\n",
    "\n",
    "*    Un espacio de trabajo de Databricks y Apache spark cluster t. (Para ejecutar nuestros nuestros cuadernos).\n",
    "*    Un centro de eventos, f. (Para que Databricks envíe los datos).\n",
    "*    Una cuenta de servicios cognitivos t. (Para acceder a la API de Text Analytics).\n",
    "*    Una aplicación de Twitter para los datos. (Para proporcionar la transmisión actual de datos).\n",
    "*    Una base de datos CosmosDB como. (Para almacenar datos de forma persistente. Los datos)\n",
    "*    Power BI Desktop para visualización de datos t. (Para visualizar y analizar los datos). \n",
    "     \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 En primer lugar, crea un Event Hub buscando Event Hubs en **Crear un recurso** en Azure. Dale en Crear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 En **Crear espacio de nombres**, elije un espacio de nombres y selecciona una suscripción de Azure, un grupo de recursos y una ubicación para crear el recurso.\n",
    "\n",
    "tweetstopowerbi6\n",
    "nuevogrupo1\n",
    "(Europe) Este de Noruega\n",
    "10 unidades de procesamiento\n",
    "Crear\n",
    "\n",
    "Espere hasta que salga el mensaje: Implementación correcta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 A continuación, cree un Event Hub a través del espacio de nombres que acaba de crear. Haga clic dentro del mismo panel en el que está en **Event Hubs** debajo del título **Entidades** y luego **+ Event Hubs**. Dale un nombre y luego presiona Crear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 Dirijámonos al espacio de nombres recién construído, buscando en todos los recursos de Azure y démosle click a **Directivas de acceso compartido**, y luego a **RootManageSharedAccessKey**\n",
    "\n",
    "Esperemos unos segundo y copiemos lo siguiente para permitir  más adelante que Databricks envíe datos de permisos al Centro de eventos.:\n",
    "\n",
    "Clave principal\n",
    "ZhvoV1uJtdw1d8tTKmTM5PNB55gHCjSOn24yjVXcwQg=\n",
    "\n",
    "Clave secundaria\n",
    "QU5qhEkyX49aa5coS+u85IpjxzXzRvzzN9eBCoROybE=\n",
    "\n",
    "Cadena de conexión: clave principal\n",
    "Endpoint=sb://tweetstopowerbi7.servicebus.windows.net/;SharedAccessKeyName=RootManageSharedAccessKey;SharedAccessKey=ZhvoV1uJtdw1d8tTKmTM5PNB55gHCjSOn24yjVXcwQg=\n",
    "    \n",
    "Cadena de conexión: clave secundaria   \n",
    "Endpoint=sb://tweetstopowerbi7.servicebus.windows.net/;SharedAccessKeyName=RootManageSharedAccessKey;SharedAccessKey=QU5qhEkyX49aa5coS+u85IpjxzXzRvzzN9eBCoROybE=\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Ahora el **Event Hub** está listo para funcionar, y tenemos todas las cadenas de conexión necesarias para que Databricks envíe datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Event hub: tweetstopowerbi7\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 Ahora construyamos un Databricks Workspace. Busque **Azure Databricks** en **+ Crear un recurso** en Azure y da click en Crear. \n",
    "\n",
    "Proporcione su grupo de recursos: nuevogrupo1\n",
    "\n",
    " un **nombre de área de trabajo= tweetstopowerbi6**,\n",
    "\n",
    "ubicación: (Asia Pacific) Japón Occidental\n",
    "\n",
    "y Premium como el nivel de precios más alto - (¡Nota importante !: para cConnection a través de DirectQuery a PowerBIPower BI no funcionará sin esto, ¡necesitará esto!)\n",
    "\n",
    "Dá click en **Revisar y crear**\n",
    "\n",
    "luego click en **Crear**\n",
    "\n",
    "Cuando la implementación esté completa, damos click a **Ir al recurso**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nos guiamos ahora por:\n",
    "\n",
    "Create a Spark cluster in Databricks\n",
    "\n",
    "https://docs.microsoft.com/en-us/azure/cognitive-services/anomaly-detector/tutorials/anomaly-detection-streaming-databricks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6 le damos click  Iniciar área de trabajo, lo que nos redirecionará al portal de Azure Databrick, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7 Le damos clik a **New Cluster**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acepte todos los demás valores predeterminados que no sean los siguientes:\n",
    "\n",
    "Ingrese un nombre para el clúster: tweetstopowerbi6\n",
    "\n",
    "Para este artículo, cree un clúster con Runtime 6.2 (Scala 2.11, Spark 2.4.4)\n",
    "\n",
    "NO seleccione 5.3.\n",
    "\n",
    "Asegúrese de que la casilla de verificación: Terminar después de __ minutos de inactividad esté seleccionada. Proporcione una duración (en minutos) para terminar el clúster, si el clúster no se está utilizando.\n",
    "\n",
    "Seleccione **Crear clúster**.\n",
    "\n",
    "La creación del clúster lleva varios minutos. Una vez que el clúster se está ejecutando, puede adjuntar notebooks al clúster y ejecutar trabajos de Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8 Adjuntar bibliotecas al clúster de Spark\n",
    "\n",
    "En este tutorial, utilizará las API de Twitter para enviar tweets a Event Hubs. También usa el conector Apache Spark Event Hubs para leer y escribir datos en Azure Event Hubs. Para usar estas API como parte de su clúster, agréguelas como bibliotecas a Azure Databricks y luego asócielas con su clúster Spark. Las siguientes instrucciones muestran cómo agregar las bibliotecas a la carpeta Compartida en su espacio de trabajo.\n",
    "\n",
    "En el espacio de trabajo de Azure Databricks, seleccione Espacio de trabajo y luego haga clic con el botón derecho en Compartido. En el menú contextual, seleccione Crear> Biblioteca."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9 En la página Nueva biblioteca, para Fuente, seleccione Maven. Para Coordenadas, ingrese la coordenada para el paquete que desea agregar. Aquí están las coordenadas de Maven para las bibliotecas utilizadas en este tutorial:\n",
    "\n",
    "Conector de Spark Event Hubs: com.microsoft.azure:azure-eventhubs-spark_2.11:2.3.10\n",
    "\n",
    "API de Twitter - org.twitter4j: twitter4j-core: 4.0.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "añadimos los .jar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si no hay ningún clúster en la página de la biblioteca, seleccione Clústeres y ejecute el clúster que ha creado. Espere hasta que el estado muestre 'En ejecución' y luego regrese a la página de la biblioteca. En la página de la biblioteca, seleccione el clúster donde desea usar la biblioteca y luego seleccione Instalar. Una vez que la biblioteca se asocia correctamente con el clúster, el estado cambia inmediatamente a Instalado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "10 Kit de herramientas de servicios cognitivos\n",
    "\n",
    "Para calcular el sentimiento de los tweets, se requiere acceso a los Servicios Cognitivos de Microsoft. Esto permitirá que Databricks llame a la API de Text Analytics en tiempo casi real directamente desde el cuaderno y calcule el sentimiento de un tweet dado. Busque la API de Text Analytics en Azure Portal. \n",
    "\n",
    "Proporcione un nombre: tweetstopowerbi6\n",
    "\n",
    "ubicación: (Asia Pacific) Japón Oriental \n",
    "\n",
    "nivel de precios. (S (1000 llamadas por minuto))\n",
    "\n",
    "Grupo de recursos: nuevogrupo1\n",
    "\n",
    "Click en crear.\n",
    "\n",
    "Una vez implementado, haga clic en ir al recurso, y bajo ADMINISTRACIÓN  DE RECURSOS, dá click en **Claves y punto de conexión** Claves y tome nota de la URL del punto final y la Clave principal que se utilizará. Estos valores serán necesarios para que Databricks llame con éxito a la API de Text Analytics.\n",
    "\n",
    "NOMBRE:\n",
    "tweetstopowerbi7\n",
    "Extremo:\n",
    "https://tweetstopowerbi7.cognitiveservices.azure.com/\n",
    "Clave1:\n",
    "ef5cfde06ebb4b3fadb16305e61bc807\n",
    "Clave2:\n",
    "f6220e32b3b644bb8e12e25f9ecf018c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11 Crear cuadernos en Databricks\n",
    "\n",
    "Crearemos dos notebooks en el workspace de Databricks con los siguientes nombres:\n",
    "\n",
    "* SendTweetsToEventHub: un cuaderno de productor que utiliza para obtener tweets de Twitter y transmitirlos a Event Hubs.\n",
    "* AnalyzeTweetsFromEventHub: una libreta de consumo que utiliza para leer los tweets de Event Hubs y ejecutar el anaisis de sentimientos.\n",
    "\n",
    "En el espacio de trabajo de Azure Databricks, seleccione Espacio de trabajo en el panel izquierdo. En el menú desplegable Espacio de trabajo, seleccione Crear y luego seleccione Cuaderno.\n",
    "\n",
    "\n",
    "\n",
    "En el cuadro de diálogo Crear cuaderno, ingrese SendTweetsToEventHub como nombre, seleccione Scala como idioma y seleccione el clúster Spark que creó anteriormente.\n",
    "\n",
    "Crear cuaderno en Databricks\n",
    "\n",
    "Selecciona Crear.\n",
    "\n",
    "Repita los pasos para crear el cuaderno AnalyzeTweetsFromEventHub."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente programa está escrito en Scala:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import scala.collection.JavaConverters._\n",
    "    import com.microsoft.azure.eventhubs._\n",
    "    import java.util.concurrent._\n",
    "    import scala.collection.immutable._\n",
    "    import scala.concurrent.Future\n",
    "    import scala.concurrent.ExecutionContext.Implicits.global\n",
    "\n",
    "    val namespaceName = \"tweetstopowerbi7.servicebus.windows.net/\"\n",
    "    val eventHubName = \"tweetstopowerbi7\"\n",
    "    val sasKeyName = \"RootManageSharedAccessKey\"\n",
    "    val sasKey = \"ZhvoV1uJtdw1d8tTKmTM5PNB55gHCjSOn24yjVXcwQg=\"\n",
    "    val connStr = new ConnectionStringBuilder()\n",
    "                .setNamespaceName(namespaceName)\n",
    "                .setEventHubName(eventHubName)\n",
    "                .setSasKeyName(sasKeyName)\n",
    "                .setSasKey(sasKey)\n",
    "\n",
    "    val pool = Executors.newScheduledThreadPool(1)\n",
    "    val eventHubClient = EventHubClient.create(connStr.toString(), pool)\n",
    "\n",
    "    def sleep(time: Long): Unit = Thread.sleep(time)\n",
    "\n",
    "    def sendEvent(message: String, delay: Long) = {\n",
    "      sleep(delay)\n",
    "      val messageData = EventData.create(message.getBytes(\"UTF-8\"))\n",
    "      eventHubClient.get().send(messageData)\n",
    "      System.out.println(\"Sent event: \" + message + \"\\n\")\n",
    "    }\n",
    "\n",
    "    // Add your own values to the list\n",
    "    val testSource = List(\"Azure is the greatest!\", \"Azure isn't working :(\", \"Azure is okay.\")\n",
    "\n",
    "    // Specify 'test' if you prefer to not use Twitter API and loop through a list of values you define in `testSource`\n",
    "    // Otherwise specify 'twitter'\n",
    "\n",
    "    // val dataSource = \"test\"\n",
    "    val dataSource = \"twitter\"\n",
    "\n",
    "    if (dataSource == \"twitter\") {\n",
    "\n",
    "      import twitter4j._\n",
    "      import twitter4j.TwitterFactory\n",
    "      import twitter4j.Twitter\n",
    "      import twitter4j.conf.ConfigurationBuilder\n",
    "\n",
    "      // Twitter configuration!\n",
    "      // Replace values below with you\n",
    "\n",
    "      val twitterConsumerKey = \"koO4XqTuWFr5ADGcE8kjIkVoU\"\n",
    "      val twitterConsumerSecret = \"3F4sk9qU8zbKBROuLPUUj1uvE2YuhseXPe0ahMQoivg4icN5bL\"\n",
    "      val twitterOauthAccessToken = \"1230251564616515586-2KqPsCG2mIJp3irRjENgHpCfQUxTUg\"\n",
    "      val twitterOauthTokenSecret = \"6PJfMtYGY7w6csiIX9m1S5jFEKNZ3hE9PVkHKeN1S14iM\"\n",
    "\n",
    "      val cb = new ConfigurationBuilder()\n",
    "        cb.setDebugEnabled(true)\n",
    "        .setOAuthConsumerKey(twitterConsumerKey)\n",
    "        .setOAuthConsumerSecret(twitterConsumerSecret)\n",
    "        .setOAuthAccessToken(twitterOauthAccessToken)\n",
    "        .setOAuthAccessTokenSecret(twitterOauthTokenSecret)\n",
    "\n",
    "      val twitterFactory = new TwitterFactory(cb.build())\n",
    "      val twitter = twitterFactory.getInstance()\n",
    "\n",
    "      // Getting tweets with keyword \"Azure\" and sending them to the Event Hub in realtime!\n",
    "      val query = new Query(\" #covid\")\n",
    "      query.setCount(100)\n",
    "      query.lang(\"en\")\n",
    "      var finished = false\n",
    "      while (!finished) {\n",
    "        val result = twitter.search(query)\n",
    "        val statuses = result.getTweets()\n",
    "        var lowestStatusId = Long.MaxValue\n",
    "        for (status <- statuses.asScala) {\n",
    "          if(!status.isRetweet()){\n",
    "            sendEvent(status.getText(), 5000)\n",
    "          }\n",
    "          lowestStatusId = Math.min(status.getId(), lowestStatusId)\n",
    "        }\n",
    "        query.setMaxId(lowestStatusId - 1)\n",
    "      }\n",
    "\n",
    "    } else if (dataSource == \"test\") {\n",
    "      // Loop through the list of test input data\n",
    "      while (true) {\n",
    "        testSource.foreach {\n",
    "          sendEvent(_,5000)\n",
    "        }\n",
    "      }\n",
    "\n",
    "    } else {\n",
    "      System.out.println(\"Unsupported Data Source. Set 'dataSource' to \\\"twitter\\\" or \\\"test\\\"\")\n",
    "    }\n",
    "\n",
    "    // Closing connection to the Event Hub\n",
    "    eventHubClient.get().close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente paso es tomar esta secuencia de tweets y aplicarle un análisis de opinión. Las siguientes celdas de código leídas desde EventHub, llaman a la API de Text Analytics y pasan el cuerpo del tweet para que se calcule el sentimiento. Obtenga el sentimiento de los Tweets En el cuaderno TweetSentiment agregue las siguientes celdas de código para llamar a la API de Text Analytics para calcular el sentimiento de la transmisión de Twitter.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crear un clúster de Apache Spark dentro de Databricks\n",
    "\n",
    "Para ejecutar notebooks para consumir el streming de data, primero se requiere un clúster. Para crear un clúster Apache Spark dentro de Databricks, cargue un Workspace desde el recurso Databricks se creó. Desde el portal de Databricks, seleccione Cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dá click en: 'iniciar area de trabajo'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En un nuevo clúster, proporcione los siguientes valores para crear el clúster. ¡NOTA! - Para las capacidades de lectura / escritura en CosmosDB, se requiere una versión Apache Spark de 2.2.0. Al momento de escribir 2.3.0 aún no es compatible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adjuntar bibliotecas a Spark Cluster\n",
    "\n",
    "Para permitir que la API de Twitter envíe tweets a Databricks y Databricks para leer y escribir datos en Event Hubs y CosmosDB, se requieren tres paquetes: \n",
    "\n",
    "• Conector Spark Event Hubs: com.microsoft.azure:azure-eventhubs-spark_2.11:2.3. 1 \n",
    "\n",
    "• API de Twitter - org.twitter4j: twitter4j-core: 4.0.6 \n",
    "\n",
    "• CosmosDB Spark Connector: http://repo1.maven.org/maven2/com/microsoft/azure/azure-cosmosdb-spark_2.2.0_2.11/ 1.1.1 / azure-cosmosdb-spark_2.2.0_2.11-1.1.1-uber.jar\n",
    "\n",
    "Haga clic derecho en el espacio de trabajo de Databricks y seleccione Crear> Biblioteca. En la página Nueva biblioteca, seleccione Maven Coordinate e ingrese los nombres de las bibliotecas anteriores. se mantiene se encuentra aquí: https://github.com/Azure/azure-cosmosdb-spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kit de herramientas de servicios cognitivos\n",
    "\n",
    "Para calcular el sentimiento de los tweets, se requiere acceso a los Servicios Cognitivos de Microsoft. Esto permitirá que Databricks llame a la API de Text Analytics en tiempo casi real directamente desde el cuaderno y calcule el sentimiento de un tweet dado. Busque la API de Text Analytics en Azure Portal. Proporcione un nombre, ubicación y nivel de precios. (F0 será suficiente para los propósitos de esta demostración)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez creado, haga clic en **Claves y puntos de conexión** y tome nota de la URL del punto final y la Clave principal que se utilizará. Estos valores serán necesarios para que Databricks llame con éxito a la API de Text Analytics.\n",
    "\n",
    "endpoint:\n",
    "https://tweetstopowerbi5.cognitiveservices.azure.com/\n",
    "\n",
    "\n",
    "clave 1:\n",
    "\n",
    "dae1210d95694ace8758d4cc0bea86d2\n",
    "\n",
    "clave 2:\n",
    "\n",
    "35a59dc1f1274710aa101ac432fb90d6\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crear notebooks Databricks\n",
    "\n",
    "Para ejecutar el código, necesitaremos crear 4 cuadernos en el espacio de trabajo de Databricks creado de la siguiente manera:\n",
    "\n",
    "* EventHubTweets (para enviar tweets al centro de eventos)\n",
    "* TweetSentiment (Para calcular el sentimiento de la secuencia de tweets del centro de eventos)\n",
    "* ScheduledDatasetCreation (Para crear y actualizar continuamente el conjunto de datos)\n",
    "* DatasetValidation (para validar el conjunto de datos directamente dentro de Databricks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conecte Power BI al clúster de Databricks\n",
    "\n",
    "Para permitir que PowerBIPower BI se conecte primero a Databricks, se requiere que la información de conexión JDBC de los clústeres se proporcione como una dirección de servidor para la conexión PowerBIPower BI. Para obtener esto, navegue al clúster dentro de Databricks y seleccione el clúster que se va a conectar. En la página del clúster, seleccione la pestaña JDBC / ODBC (Nota: si no creó un espacio de trabajo Premium Databricks, esta opción no estará disponible).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para construir la dirección del servidor, tome la URL JDBC que se muestra en el clúster y haga lo siguiente: • Reemplace jdbc: hive2 con https. • Elimine todo en la ruta entre el número de puerto y sql que retiene los componentes para que tenga una url similar a la siguiente: \n",
    "\n",
    "https://westeurope.azuredatabricks.net:443/sql/protocolv1/o/1406775902027556/0424- 131603-inky272 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://southindia.azuredatabricks.net:443/sql/protocolv1/o/7401067854218687/0425-221454-peg462\n",
    "\n",
    "\n",
    "AuthMech=3;UID=token;PWD=dapid676f834f0902df637e0b7972a0eca62"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para generar el acceso personal a tokentoken, seleccione Configuración de usuario en el panel de control del clúster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Token:\n",
    "\n",
    "dapid676f834f0902df637e0b7972a0eca62"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crear tabla de datos para que Power BI se conecte\n",
    "\n",
    "Primero, necesitamos escribir datos como formato **parquet** en el almacenamiento de blobs que pasan en la ruta de nuestro almacenamiento de blobs montado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "//WRITE THE STREAM TO PARQUET FORMAT/////  \n",
    "import org.apache.spark.sql.streaming.Trigger.ProcessingTime \n",
    "val result = streamingDataFrame\n",
    ".writeStream\n",
    ".format(\"parquet\")\n",
    ".option(\"path\", \"/mnt/DatabricksSentimentPowerBI\")\n",
    ".option(\"checkpointLocation\", \"/mnt/sample/check2\")\n",
    ".start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para verificar que los datos se escriben en el almacenamiento de blobs montado directamente desde el cuaderno Databricks, cree un nuevo DatasetValidation del cuaderno y ejecute los siguientes comandos para mostrar el contenido de los archivos de parquet directamente dentro de Databricks. Si los datos se escriben correctamente, una salida al consultar la tabla en Databricks debería ser similar a la siguiente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora tenemos la transmisión de datos de Twitter con el sentimiento adjunto fluyendo hacia un almacenamiento de blobs montado. El siguiente paso es conectar Databricks (y este conjunto de datos) directamente a PowerBIPower BI para su posterior análisis y disección de datos. Para hacer esto, necesitamos escribir los archivos de parquet en un conjunto de datos que PowerBIPower BI podrá leer con éxito a intervalos regulares (es decir, actualizar continuamente el conjunto de datos a intervalos específicos para el flujo de datos por lotes). Para hacer esto, cree el cuaderno final ScheduledDatasetCreation y ejecute el siguiente conjunto de comandos scala como un programa para ejecutarse cada minuto. (Esto actualizará la tabla creada cada 1 minuto con el flujo de datos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure la conexión de PowerBI\n",
    "\n",
    "El paso final es conectar Databricks a PowerBIPower BI para permitir el flujo de datos por lotes y realizar análisis. Para hacer esto, abra el escritorio de PowerBIPower BI abierto y haga clic en Obtener datos. Seleccione Spark (beta) para comenzar a configurar la conexión del clúster Databricks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
