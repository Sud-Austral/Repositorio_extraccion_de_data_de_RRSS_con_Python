{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming de tweets con Azure Databricks y su despliegue en Power BI\n",
    "\n",
    "#### Fuentes:\n",
    "\n",
    "Structured Streaming with Azure Databricks into Power BI & Cosmos DB\n",
    "\n",
    "https://github.com/giulianorapoz/DatabricksStreamingPowerBI\n",
    "\n",
    "Tutorial: Anomaly detection on streaming data using Azure Databricks\n",
    "\n",
    "https://docs.microsoft.com/en-us/azure/cognitive-services/anomaly-detector/tutorials/anomaly-detection-streaming-databricks\n",
    "\n",
    "\n",
    "Tutorial: Sentiment analysis on streaming data using Azure Databricks\n",
    "\n",
    "https://docs.microsoft.com/en-us/azure/azure-databricks/databricks-sentiment-analysis-cognitive-services\n",
    "\n",
    "\n",
    "Power BI\n",
    "\n",
    "https://docs.microsoft.com/en-us/azure/databricks/integrations/bi/power-bi\n",
    "\n",
    "Power BI Connects to Azure Databricks\n",
    "\n",
    "https://towardsdatascience.com/power-bi-connects-to-azure-databricks-44bea6731be7\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The watermark extension is already loaded. To reload it, use:\n",
      "  %reload_ext watermark\n",
      "Christian Castro \n",
      "last updated: 2020-04-27 \n",
      "\n",
      "numpy 1.18.1\n",
      "pandas 1.0.1\n",
      "matplotlib 3.1.3\n",
      "Propiedad de DataIntelligence\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark -a \"Christian Castro\" -u -d -p numpy,pandas,matplotlib\n",
    "%watermark -a \"Propiedad de DataIntelligence\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Introducción\n",
    "\n",
    "En este notebook revisaremos el concepto de Streaming Estructurado con **Azure Databricks** y en el cómo se puede conectar directamente con Power BI, lo que permite la visualización y análisis avanzados.\n",
    "\n",
    "Construiremos una ruta de consumo de datos directamente con Azure Databricks lo que nos permitirá transmitir datos a un clúster de **Apache Spark** en tiempo casi real. Mostraremos algunas de las capacidades de análisis a las que se puede llamar directamente desde Databricks utilizando la API de Text Analytics, luego conectaremos Databricks directamente a Power BI para treas de visualización y análisis. \n",
    "\n",
    "Se deja como tarea la lectura y escritura directamente desde Databricks  en CosmosDB como almacenamiento persistente y uso posterior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](figuras_twitter_powerbi/t_streaming_001.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Configuración de nuestros recursos. \n",
    "\n",
    "Necesitaremos lo siguiente:\n",
    "* Un espacio de nombres en Azure.\n",
    "* Un espacio de trabajo de Databricks y un cluster Apache Spark para ejecutar nuestros nuestros notebooks.\n",
    "* Un Event Hub, para que Databricks envíe los datos.\n",
    "* Una cuenta de servicios cognitivos, para acceder a la API de Text Analytics.\n",
    "* Una cuenta de Twitter, para obtener un streaming de datos.\n",
    "* Power BI Desktop para visualizar y analizar los datos. \n",
    "* (Opcional) Una base de datos CosmosDB, para almacenar datos de forma persistente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Crear un **espacio de nombres**\n",
    "\n",
    "* Dá click en las tres líneas horizontales de la esquina superior izquierda de Azure, dá click en **Crear un recurso** y en el cuadro de búsqueda ingresa **Event Hub**.\n",
    "\n",
    "* Dá click en el ícono **Event Hubs** y luego en **Crear**. Ésto no creará un event hub inmediatamente, sino que primero se requerirá la creación de un **espacio de nombres**. Es por ello ue llegarán a la ventana **Crear un espacio de nombres**.\n",
    "\n",
    "* En **Crear un espacio de nombres** sólo ingresa datos en la pestaña de **Datos básicos** (lo que no se indica que modifiques déjalo tal cual, así en toda la guía):\n",
    "\n",
    "    * Suscripción: Suscripción de Azure 1\n",
    "        *Grupo de recursos: Crea uno nuevo: (nuevogrupo2)\n",
    "\n",
    "    * Nombre de espacio de nombres: tweetstopowerbi8\n",
    "    \n",
    "    * Ubicación: Busca una ubicación diferente (existen cuotas que entrega Azure por región que puede provocarte errores más adelante): (US) Centro-Sur de EE.UU.\n",
    "    \n",
    "    * Plan de tarifa: Estándar\n",
    "\n",
    "Espere hasta que salga el mensaje: Validación correcta.\n",
    "\n",
    "Dá click en **Crear**.\n",
    "\n",
    "Surgirá un mensaje: **La implementación está en curso**\n",
    "Espere hasta que aparezca el mensaje: **Se completó la implementación**\n",
    "\n",
    "Dá click a **Ir al recurso**\n",
    "\n",
    "Entramos al Espacio de nombres de Event Hubs llamado: tweetstopowerbi8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Crear un Event Hub\n",
    "\n",
    "En el espacio de nombres tweetstopowerbi8, da click a **+ Event Hub**. Dale un nombre: tweetstopowerbi8 y luego click a **Crear**.\n",
    "\n",
    "Ahora el **Event Hub** está listo para funcionar, y tenemos todas las cadenas de conexión necesarias para que Databricks envíe datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](figuras_twitter_powerbi/t_streaming_002.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el espacio de nombres tweetstopowerbi8, en la lista desplegable de la izquierda, en el listado de Configuración da click a **Directivas de acceso compartido** y en la ventana de la derecha a **RootManageSharedAccessKey**\n",
    "\n",
    "Se cargará automáticamente la **Directiva SAS**\n",
    "\n",
    "Copia en un txt las claves y ls cadenas de conexión:\n",
    "\n",
    "Clave principal\n",
    "fpKYGIGLA3QSCNihV5I8e3Sv/mCfmb3ZUgZQocKCXUc=\n",
    "\n",
    "Clave secundaria\n",
    "Aak8snk7d89yQULitOuoWtpct6OeYmhpaieRdpIe4mM=\n",
    "\n",
    "Cadena de conexión: clave principal\n",
    "<span style=\"color:red\">\n",
    "Endpoint=sb://tweetstopowerbi8.servicebus.windows.net/;SharedAccessKeyName=RootManageSharedAccessKey;SharedAccessKey=fpKYGIGLA3QSCNihV5I8e3Sv/mCfmb3ZUgZQocKCXUc=\n",
    "</span>\n",
    "    \n",
    "Cadena de conexión: clave secundaria   \n",
    "Endpoint=sb://tweetstopowerbi8.servicebus.windows.net/;SharedAccessKeyName=RootManageSharedAccessKey;SharedAccessKey=Aak8snk7d89yQULitOuoWtpct6OeYmhpaieRdpIe4mM="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora el **Event Hub** está listo para funcionar, y tenemos todas las cadenas de conexión necesarias para que Databricks envíe datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Construir un Workspace Databricks\n",
    "\n",
    "Busque **Azure Databricks** en **+ Crear un recurso** al hacer click a las tres lineas horizontales de la esquina superior izquierda. Da click en **Crear**. \n",
    "\n",
    "* Suscripción: Suscripción de Azure 1\n",
    "    *Grupo de recursos: Crea uno nuevo: nuevogrupo2\n",
    "\n",
    "* Nombre de espacio de nombres: tweetstopowerbi8\n",
    "    \n",
    "* Ubicación: Busca una ubicación diferente (existen cuotas que entrega Azure por región que puede provocarte errores más adelante): (US) Centro-Sur de EE.UU.\n",
    "    \n",
    "* Plan de tarifa: **Premium** Ésto es muy importante, pues si no elegimos premiun no vamos a obtener la cadena de conexión jdbc.\n",
    "\n",
    "Dá click en revisar y crear.\n",
    "\n",
    "Espera a que se te indique: **Validación correcta** y da click a **Crear**.\n",
    "\n",
    "Se inicializará la implementación.\n",
    "\n",
    "Cuando la implementación esté completa, damos click a **Ir al recurso**\n",
    "\n",
    "Le damos click  Iniciar área de trabajo, lo que nos redirecionará al portal de Azure Databrick. Necesitaremos ingresar las credenciales de Azure nuevamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.1 Construyendo Clusters.\n",
    "\n",
    "Ya estamos en Azure Databricks!\n",
    "\n",
    "Le damos clik a **New Cluster** debajo de la franja **Common Tasks**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acepte todos los demás valores predeterminados que no sean los siguientes:\n",
    "\n",
    "Ingrese un nombre para el clúster: tweetstopowerbi8\n",
    "\n",
    "Databricks Runtime Version: Runtime 6.4 (Scala 2.11, Spark 2.4.5)\n",
    "\n",
    "Asegúrate de que la casilla de verificación: **Terminar después de 120 minutos de inactividad** esté seleccionada. Proporcione una duración (en minutos) para terminar el clúster, si el clúster no se está utilizando.\n",
    "\n",
    "Seleccione **Crear clúster**.\n",
    "\n",
    "La creación del clúster lleva varios minutos. Una vez que el clúster se esté ejecutando, puedes adjuntarle notebooks y ejecutar trabajos de Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](figuras_twitter_powerbi/t_streaming_003.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2 Adjuntar bibliotecas al clúster de Spark\n",
    "\n",
    "Utilizarás las API de Twitter para enviar tweets a Event Hubs y el conector Apache Spark Event Hubs para leer y escribir datos en Azure Event Hubs. \n",
    "\n",
    "Para usar estas API como parte de tu clúster, agrégalas dándole click a **Install New** en **Libraries** en el cluster recién creado.\n",
    "\n",
    "Se abrirá la ventana **Install Library**, seleccionamos Maven en instalamos como coordenadas:\n",
    "\n",
    "com.microsoft.azure:azure-eventhubs-spark_2.11:2.3.10\n",
    "\n",
    "org.twitter4j:twitter4j-core:4.0.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](figuras_twitter_powerbi/t_streaming_004.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Kit de herramientas de servicios cognitivos\n",
    "\n",
    "Para calcular el sentimiento de los tweets, se requiere un acceso a los **Servicios Cognitivos** de Microsoft. Esto permitirá que Databricks llame a la API de Text Analytics en tiempo casi real directamente desde el notebook y calcule el sentimiento de un tweet dado. \n",
    "\n",
    "Crea un recurso como ya lo hemos hecho varias veces y busca:\n",
    "Text Analytics. Dá click en **Crear**.\n",
    "\n",
    "1 Nombre: tweetstopowerbi8\n",
    "\n",
    "2 Suscripción: Suscripción de Azure 1\n",
    "\n",
    "3 Ubicación: (US) Centro-Sur de EE.UU.\n",
    "\n",
    "4 Plan de tarifa: S\n",
    "\n",
    "5 Grupo de recursos: Crea uno nuevo: nuevogrupo2\n",
    "\n",
    "Da click en **Crear**\n",
    "\n",
    "Una vez implementado, haz click en ir al recurso, y bajo ADMINISTRACIÓN  DE RECURSOS, dá click en **Claves y punto de conexión** \n",
    "\n",
    "Toma nota de la URL del extremo y las claves. Estos valores serán necesarios para que Databricks llame con éxito a la API de Text Analytics.\n",
    "\n",
    "NOMBRE:\n",
    "\n",
    "tweetstopowerbi8\n",
    "\n",
    "Extremo:\n",
    "\n",
    "<span style=\"color:red\">\n",
    "https://tweetstopowerbi8.cognitiveservices.azure.com\n",
    "</span>\n",
    "\n",
    "Clave1:\n",
    "\n",
    "<span style=\"color:red\">\n",
    "3f6e681ce10c430fadc5fa12b5899774\n",
    "</span>\n",
    "\n",
    "Clave2:\n",
    "\n",
    "b9eb64027de54831ae0a754d2bff93a2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 Crear notebooks en Databricks\n",
    "\n",
    "Crearemos cuatro notebooks en el workspace de Databricks con los siguientes nombres:\n",
    "\n",
    "1 SendTweetsToEventHub (To send tweets to the event hub)\n",
    "2 TweetSentiment (To calculate sentiment from stream of tweets from event hub)\n",
    "3 ScheduledTableCreate (To create and continuously update the dataset)\n",
    "4 DatasetValidation (To validate the dataset directly within Databricks)\n",
    "\n",
    "En el cluster de Azure Databricks tweetstopowerbi8, en la franja negra de la izquierda anda seleccionando consecutivamente: Workspace, Shared, Shared, Create, Notebook.\n",
    "\n",
    "Crea los cuatro notebooks en el lenguaje **Scala**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Código del Notebook SendTweetsToEventHub\n",
    "\n",
    "Hay un detalle que hay que tomar en cuanta: Tweeter aplica una restricción a la cantidad que puede ser consumida de tweets cada 15 minutos.\n",
    "https://dev.twitter.com/docs/rate-limiting/1.1\n",
    "\n",
    "Haremos una búsqueda por **#covid**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "import scala.collection.JavaConverters._\n",
    "    import com.microsoft.azure.eventhubs._\n",
    "    import java.util.concurrent._\n",
    "    import scala.collection.immutable._\n",
    "    import scala.concurrent.Future\n",
    "    import scala.concurrent.ExecutionContext.Implicits.global\n",
    "\n",
    "    val namespaceName = \"tweetstopowerbi8.servicebus.windows.net/\"\n",
    "    val eventHubName = \"tweetstopowerbi8\"\n",
    "    val sasKeyName = \"RootManageSharedAccessKey\"\n",
    "    val sasKey = \"fpKYGIGLA3QSCNihV5I8e3Sv/mCfmb3ZUgZQocKCXUc=\"\n",
    "    val connStr = new ConnectionStringBuilder()\n",
    "                .setNamespaceName(namespaceName)\n",
    "                .setEventHubName(eventHubName)\n",
    "                .setSasKeyName(sasKeyName)\n",
    "                .setSasKey(sasKey)\n",
    "\n",
    "    val pool = Executors.newScheduledThreadPool(1)\n",
    "    val eventHubClient = EventHubClient.create(connStr.toString(), pool)\n",
    "\n",
    "    def sleep(time: Long): Unit = Thread.sleep(time)\n",
    "\n",
    "    def sendEvent(message: String, delay: Long) = {\n",
    "      sleep(delay)\n",
    "      val messageData = EventData.create(message.getBytes(\"UTF-8\"))\n",
    "      eventHubClient.get().send(messageData)\n",
    "      System.out.println(\"Sent event: \" + message + \"\\n\")\n",
    "    }\n",
    "\n",
    "    // Add your own values to the list\n",
    "    val testSource = List(\"Azure is the greatest!\", \"Azure isn't working :(\", \"Azure is okay.\")\n",
    "\n",
    "    // Specify 'test' if you prefer to not use Twitter API and loop through a list of values you define in `testSource`\n",
    "    // Otherwise specify 'twitter'\n",
    "\n",
    "    // val dataSource = \"test\"\n",
    "    val dataSource = \"twitter\"\n",
    "\n",
    "    if (dataSource == \"twitter\") {\n",
    "\n",
    "      import twitter4j._\n",
    "      import twitter4j.TwitterFactory\n",
    "      import twitter4j.Twitter\n",
    "      import twitter4j.conf.ConfigurationBuilder\n",
    "\n",
    "      // Twitter configuration!\n",
    "      // Replace values below with you\n",
    "\n",
    "      val twitterConsumerKey = \"koO4XqTuWFr5ADGcE8kjIkVoU\"\n",
    "      val twitterConsumerSecret = \"3F4sk9qU8zbKBROuLPUUj1uvE2YuhseXPe0ahMQoivg4icN5bL\"\n",
    "      val twitterOauthAccessToken = \"1230251564616515586-2KqPsCG2mIJp3irRjENgHpCfQUxTUg\"\n",
    "      val twitterOauthTokenSecret = \"6PJfMtYGY7w6csiIX9m1S5jFEKNZ3hE9PVkHKeN1S14iM\"\n",
    "\n",
    "      val cb = new ConfigurationBuilder()\n",
    "        cb.setDebugEnabled(true)\n",
    "        .setOAuthConsumerKey(twitterConsumerKey)\n",
    "        .setOAuthConsumerSecret(twitterConsumerSecret)\n",
    "        .setOAuthAccessToken(twitterOauthAccessToken)\n",
    "        .setOAuthAccessTokenSecret(twitterOauthTokenSecret)\n",
    "\n",
    "      val twitterFactory = new TwitterFactory(cb.build())\n",
    "      val twitter = twitterFactory.getInstance()\n",
    "\n",
    "      // Getting tweets with keyword \"Azure\" and sending them to the Event Hub in realtime!\n",
    "      // val query = new Query(\"source:twitter4j ministeriosalud\")\n",
    "      val query = new Query(\" #covid\")\n",
    "      query.setCount(100)\n",
    "      query.lang(\"en\")\n",
    "      var finished = false\n",
    "      while (!finished) {\n",
    "        val result = twitter.search(query)\n",
    "        val statuses = result.getTweets()\n",
    "        var lowestStatusId = Long.MaxValue\n",
    "        for (status <- statuses.asScala) {\n",
    "          if(!status.isRetweet()){\n",
    "            sendEvent(status.getText(), 5000)\n",
    "          }\n",
    "          lowestStatusId = Math.min(status.getId(), lowestStatusId)\n",
    "        }\n",
    "        query.setMaxId(lowestStatusId - 1)\n",
    "      }\n",
    "\n",
    "    } else if (dataSource == \"test\") {\n",
    "      // Loop through the list of test input data\n",
    "      while (true) {\n",
    "        testSource.foreach {\n",
    "          sendEvent(_,5000)\n",
    "        }\n",
    "      }\n",
    "\n",
    "    } else {\n",
    "      System.out.println(\"Unsupported Data Source. Set 'dataSource' to \\\"twitter\\\" or \\\"test\\\"\")\n",
    "    }\n",
    "\n",
    "    // Closing connection to the Event Hub\n",
    "    eventHubClient.get().close()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si el código está bien, al ejecutarse debe desplegarse lo siguiente:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](figuras_twitter_powerbi/t_streaming_005.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente paso es tomar esta secuencia de tweets y aplicarle un análisis de opinión. Las siguientes líneas de código leídas desde EventHub, llaman a la API de Text Analytics y pasan el cuerpo del tweet para que se calcule el análisis. Lo haremos con el notebook TweetSentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Creamos el notebook TweetSentiment, que analiza los tweets desde Event Hub.\n",
    "\n",
    "El siguiente programa está escrito en Scala:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "import org.apache.spark.eventhubs._\n",
    "\n",
    "// Build connection string with the above information\n",
    "val connectionString = ConnectionStringBuilder(\"Endpoint=sb://tweetstopowerbi8.servicebus.windows.net/;SharedAccessKeyName=RootManageSharedAccessKey;SharedAccessKey=fpKYGIGLA3QSCNihV5I8e3Sv/mCfmb3ZUgZQocKCXUc=\")\n",
    "  .setEventHubName(\"tweetstopowerbi8\")\n",
    "  .build\n",
    "\n",
    "val customEventhubParameters =\n",
    "  EventHubsConf(connectionString)\n",
    "  .setMaxEventsPerTrigger(5)\n",
    ".setStartingPosition(EventPosition.fromEndOfStream) //added this with martin from the databricks offical doc\n",
    "\n",
    "val incomingStream = spark.readStream.format(\"eventhubs\").options(customEventhubParameters.toMap).load()\n",
    "incomingStream.printSchema\n",
    "// Event Hub message format is JSON and contains \"body\" field\n",
    "// Body is binary, so we cast it to string to see the actual content of the message\n",
    "val messages =\n",
    "  incomingStream\n",
    "  .withColumn(\"Offset\", $\"offset\".cast(LongType))\n",
    "  .withColumn(\"Time (readable)\", $\"enqueuedTime\".cast(TimestampType))\n",
    "  .withColumn(\"Timestamp\", $\"enqueuedTime\".cast(LongType))\n",
    "  .withColumn(\"Body\", $\"body\".cast(StringType))\n",
    "  .select(\"Timestamp\", \"Body\")\n",
    ".as[(String, String)] //added this with martin from the databrick official doc\n",
    "\n",
    "\n",
    "// NEW CODE CELL ----------\n",
    "\n",
    "import java.io._\n",
    "import java.net._\n",
    "import java.util._\n",
    "\n",
    "class Document(var id: String, var text: String, var language: String = \"\", var sentiment: Double = 0.0) extends Serializable\n",
    "\n",
    "class Documents(var documents: List[Document] = new ArrayList[Document]()) extends Serializable {\n",
    "\n",
    "    def add(id: String, text: String, language: String = \"\") {\n",
    "        documents.add (new Document(id, text, language))\n",
    "    }\n",
    "    def add(doc: Document) {\n",
    "        documents.add (doc)\n",
    "    }\n",
    "}\n",
    "\n",
    "// NEW CODE CELL ----------\n",
    "\n",
    "class CC[T] extends Serializable { def unapply(a:Any):Option[T] = Some(a.asInstanceOf[T]) }\n",
    "object M extends CC[scala.collection.immutable.Map[String, Any]]\n",
    "object L extends CC[scala.collection.immutable.List[Any]]\n",
    "object S extends CC[String]\n",
    "object D extends CC[Double]\n",
    "\n",
    "// NEW CODE CELL ----------\n",
    "\n",
    "import javax.net.ssl.HttpsURLConnection\n",
    "import com.google.gson.Gson\n",
    "import com.google.gson.GsonBuilder\n",
    "import com.google.gson.JsonObject\n",
    "import com.google.gson.JsonParser\n",
    "import scala.util.parsing.json._\n",
    "\n",
    "object SentimentDetector extends Serializable {\n",
    "\n",
    "  // Cognitive Services API connection settings\n",
    "  val accessKey = \"3f6e681ce10c430fadc5fa12b5899774\"\n",
    "  val host = \"https://tweetstopowerbi8.cognitiveservices.azure.com/\"\n",
    "  val languagesPath = \"/text/analytics/v2.0/languages\"\n",
    "  val sentimentPath = \"/text/analytics/v2.0/sentiment\"\n",
    "  val languagesUrl = new URL(host+languagesPath)\n",
    "  val sentimenUrl = new URL(host+sentimentPath)\n",
    "\n",
    "  def getConnection(path: URL): HttpsURLConnection = {\n",
    "    val connection = path.openConnection().asInstanceOf[HttpsURLConnection]\n",
    "    connection.setRequestMethod(\"POST\")\n",
    "    connection.setRequestProperty(\"Content-Type\", \"text/json\")\n",
    "    connection.setRequestProperty(\"Ocp-Apim-Subscription-Key\", accessKey)\n",
    "    connection.setDoOutput(true)\n",
    "    return connection\n",
    "  }\n",
    "\n",
    "  def prettify (json_text: String): String = {\n",
    "    val parser = new JsonParser()\n",
    "    val json = parser.parse(json_text).getAsJsonObject()\n",
    "    val gson = new GsonBuilder().setPrettyPrinting().create()\n",
    "    return gson.toJson(json)\n",
    "  }\n",
    "\n",
    "  // Handles the call to Cognitive Services API.\n",
    "  // Expects Documents as parameters and the address of the API to call.\n",
    "  // Returns an instance of Documents in response.\n",
    "  def processUsingApi(inputDocs: Documents, path: URL): String = {\n",
    "    val docText = new Gson().toJson(inputDocs)\n",
    "    val encoded_text = docText.getBytes(\"UTF-8\")\n",
    "    val connection = getConnection(path)\n",
    "    val wr = new DataOutputStream(connection.getOutputStream())\n",
    "    wr.write(encoded_text, 0, encoded_text.length)\n",
    "    wr.flush()\n",
    "    wr.close()\n",
    "\n",
    "    val response = new StringBuilder()\n",
    "    val in = new BufferedReader(new InputStreamReader(connection.getInputStream()))\n",
    "    var line = in.readLine()\n",
    "    while (line != null) {\n",
    "        response.append(line)\n",
    "        line = in.readLine()\n",
    "    }\n",
    "    in.close()\n",
    "    return response.toString()\n",
    "  }\n",
    "\n",
    "  // Calls the language API for specified documents.\n",
    "  // Returns a documents with language field set.\n",
    "  def getLanguage (inputDocs: Documents): Documents = {\n",
    "    try {\n",
    "      val response = processUsingApi(inputDocs, languagesUrl)\n",
    "      // In case we need to log the json response somewhere\n",
    "      val niceResponse = prettify(response)\n",
    "      val docs = new Documents()\n",
    "      val result = for {\n",
    "            // Deserializing the JSON response from the API into Scala types\n",
    "            Some(M(map)) <- scala.collection.immutable.List(JSON.parseFull(niceResponse))\n",
    "            L(documents) = map(\"documents\")\n",
    "            M(document) <- documents\n",
    "            S(id) = document(\"id\")\n",
    "            L(detectedLanguages) = document(\"detectedLanguages\")\n",
    "            M(detectedLanguage) <- detectedLanguages\n",
    "            S(language) = detectedLanguage(\"iso6391Name\")\n",
    "      } yield {\n",
    "            docs.add(new Document(id = id, text = id, language = language))\n",
    "      }\n",
    "      return docs\n",
    "    } catch {\n",
    "          case e: Exception => return new Documents()\n",
    "    }\n",
    "  }\n",
    "\n",
    "  // Calls the sentiment API for specified documents. Needs a language field to be set for each of them.\n",
    "  // Returns documents with sentiment field set, taking a value in the range from 0 to 1.\n",
    "  def getSentiment (inputDocs: Documents): Documents = {\n",
    "    try {\n",
    "      val response = processUsingApi(inputDocs, sentimenUrl)\n",
    "      val niceResponse = prettify(response)\n",
    "      val docs = new Documents()\n",
    "      val result = for {\n",
    "            // Deserializing the JSON response from the API into Scala types\n",
    "            Some(M(map)) <- scala.collection.immutable.List(JSON.parseFull(niceResponse))\n",
    "            L(documents) = map(\"documents\")\n",
    "            M(document) <- documents\n",
    "            S(id) = document(\"id\")\n",
    "            D(sentiment) = document(\"score\")\n",
    "      } yield {\n",
    "            docs.add(new Document(id = id, text = id, sentiment = sentiment))\n",
    "      }\n",
    "      return docs\n",
    "    } catch {\n",
    "        case e: Exception => return new Documents()\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "// User Defined Function for processing content of messages to return their sentiment.\n",
    "val toSentiment = udf((textContent: String) => {\n",
    "  val inputDocs = new Documents()\n",
    "  inputDocs.add (textContent, textContent)\n",
    "  val docsWithLanguage = SentimentDetector.getLanguage(inputDocs)\n",
    "  val docsWithSentiment = SentimentDetector.getSentiment(docsWithLanguage)\n",
    "  if (docsWithLanguage.documents.isEmpty) {\n",
    "    // Placeholder value to display for no score returned by the sentiment API\n",
    "    (-1).toDouble\n",
    "  } else {\n",
    "    docsWithSentiment.documents.get(0).sentiment.toDouble\n",
    "  }\n",
    "})\n",
    "\n",
    "// NEW CODE CELL ----------\n",
    "\n",
    "// Prepare a dataframe with Content and Sentiment columns\n",
    "val streamingDataFrame = incomingStream.selectExpr(\"cast (body as string) AS Content\").withColumn(\"body\", toSentiment($\"Content\"))\n",
    "\n",
    "// Display the streaming data with the sentiment\n",
    "\n",
    "streamingDataFrame.writeStream.outputMode(\"append\").format(\"console\").option(\"truncate\", false).start()\n",
    "\n",
    "// NEW CODE CELL ----------\n",
    "\n",
    "//WRITE THE STREAM TO PARQUET FORMAT/////\n",
    "import org.apache.spark.sql.streaming.Trigger.ProcessingTime\n",
    "\n",
    "val result =\n",
    "  streamingDataFrame\n",
    "    .writeStream\n",
    "    .format(\"parquet\")\n",
    "    .option(\"path\", \"/mnt/DatabricksSentimentPowerBI\")\n",
    "    .option(\"checkpointLocation\", \"/mnt/sample/check2\")\n",
    "    .start() \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si el código está bien, al ejecutarse debe desplegarse lo siguiente:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](figuras_twitter_powerbi/t_streaming_006.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.1 Crear tabla de datos para que Power BI se conecte\n",
    "\n",
    "Necesitamos escribir datos como formato **parquet** en el almacenamiento de blobs que pasan en la ruta de nuestro almacenamiento de blobs montado. Es por ello que se añaden las siguientes líneas al final del programa TweetSentiment:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "//WRITE THE STREAM TO PARQUET FORMAT/////  \n",
    "import org.apache.spark.sql.streaming.Trigger.ProcessingTime \n",
    "val result = streamingDataFrame\n",
    ".writeStream\n",
    ".format(\"parquet\")\n",
    ".option(\"path\", \"/mnt/DatabricksSentimentPowerBI\")\n",
    ".option(\"checkpointLocation\", \"/mnt/sample/check2\")\n",
    ".start()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para verificar que los datos se escriben en el almacenamiento de blobs montado directamente desde el notebook Databricks, crea un nuevo notebook **DatasetValidation** y ejecuta los siguientes comandos para mostrar el contenido de los archivos de parquet directamente dentro de Databricks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notebook DatasetValidation\n",
    "```\n",
    "val sentimentdata = spark.read.parquet(\"/mnt/DatabricksSentimentPowerBI\")\n",
    "display(sentimentdata)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si los datos se escriben correctamente, la salida al consultar la tabla en Databricks debería ser similar a la siguiente:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](figuras_twitter_powerbi/t_streaming_007.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Crear el notebook ScheduledTableCreate\n",
    "\n",
    "Ahora tenemos transmisión de datos de Twitter con el sentimiento adjunto que fluye en un almacenamiento montado en blobs. El siguiente paso es conectar Databricks (y este conjunto de datos) directamente a PowerBI para el posterior análisis y disección de datos. \n",
    "\n",
    "Para hacer esto, necesitamos escribir los archivos de parquet en un conjunto de datos que PowerBI pueda leer con éxito a intervalos regulares (es decir, actualizar continuamente el conjunto de datos a intervalos específicos para el flujo de datos por lotes). \n",
    "\n",
    "Para hacer esto, cree el cuaderno final ScheduledTableCreate con la siguiente línea de código (siempre en Scala) y define una agenda para que ejecute cada 1 minuto (ésto actualizará la tabla creada cada 1 minuto con el streaming)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notebook ScheduledTableCreate\n",
    "```\n",
    "spark.read.parquet(\"/mnt/DatabricksSentimentPowerBI\").write.mode(SaveMode.Overwrite) saveAsTable(\"twitter_dataset\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](figuras_twitter_powerbi/t_streaming_008.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "no olvides tener los 4 programas en Databricks corriendo!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7 Conectar Power BI al clúster de Databricks\n",
    "\n",
    "Para permitir que PowerBI se conecte primero a Databricks, se requiere que la información de conexión JDBC de los clústeres se proporcione como una dirección de servidor para la conexión PowerBI. \n",
    "\n",
    "Para obtener esto, navega al clúster al que se va a conectar dentro de Databricks y selecciónalo. En **Opciones avanzadas**, selecciona la pestaña JDBC/ODBC (Nota: **si no creó un espacio de trabajo Premium Databricks, esta opción no estará disponible**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para construir la dirección del servidor, toma la URL JDBC que se muestra en el clúster y haz lo siguiente: \n",
    "\n",
    "• Reemplaza jdbc:spark con https. \n",
    "\n",
    "• Elimina todo en la ruta entre el número de puerto y sql y lo que sigue al antepenúltimo ; para que tenga una url similar a la siguiente:\n",
    "\n",
    "https://southcentralus.azuredatabricks.net:443/sql/protocolv1/o/2511609374679582/0427-213639-dicta184"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Necesitarás un token, para ello selecciona **User settings** en el ícono de la persona en el extremo superior derecho del panel de control del clúster.\n",
    "\n",
    "Da click a **Generate New Token**\n",
    "\n",
    "Agrega cualquier comentario y deja en blanco Lifetime.\n",
    "\n",
    "Dá click en **Generate**\n",
    "\n",
    "Se generará uno automáticamente:\n",
    "\n",
    "dapi1c02ebd896681ddc793f0fff4303c947\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8 Conexión a PowerBI\n",
    "\n",
    "El paso final es conectar Databricks a PowerBI para permitir el flujo de datos por lotes y realizar el análisis. Para hacer esto, abre  PowerBI desktop y haz clic en Obtener datos. Seleccione Spark para comenzar a configurar la conexión del clúster Databricks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como servidor ingresa:\n",
    "    \n",
    "https://southcentralus.azuredatabricks.net:443/sql/protocolv1/o/2511609374679582/0427-213639-dicta184\n",
    "\n",
    "Protocolo HTTP\n",
    "\n",
    "Modo de conectividad de datos:\n",
    "\n",
    "DirectQuery\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si es la primera vez que haces la conexión al servidor te va a pedir el token:\n",
    "    \n",
    "Como usuario ingresa: token y como contraseña:\n",
    "\n",
    "dapi1c02ebd896681ddc793f0fff4303c947\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-5-3c0517ef9e05>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-5-3c0517ef9e05>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    Se cargará automáticamente el dataset:\u001b[0m\n\u001b[1;37m             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Se cargará automáticamente el dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](figuras_twitter_powerbi/t_streaming_009.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dá click a cargar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](figuras_twitter_powerbi/t_streaming_010.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tenemos los tweets:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](figuras_twitter_powerbi/t_streaming_011.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tenemos los tweets clasificados a través de un análisis de opinión."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](figuras_twitter_powerbi/t_streaming_012.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
