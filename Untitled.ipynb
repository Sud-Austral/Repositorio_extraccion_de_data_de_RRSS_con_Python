{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'facebook'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-cc3e9fd3165b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    180\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 182\u001b[1;33m     \u001b[0mmake_dir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'facebook'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'properties_directory'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    183\u001b[0m     \u001b[0mmake_dir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'facebook'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'data_directory'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m     \u001b[1;32mimport\u001b[0m \u001b[0mtimeit\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\configparser.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    956\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    957\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdefault_section\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhas_section\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 958\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    959\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_proxies\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    960\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'facebook'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import configparser\n",
    "import json\n",
    "import string\n",
    "import pathlib\n",
    "import os\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read(os.path.join('properties', 'config.ini'))\n",
    "\n",
    "\n",
    "def load():\n",
    "    global config\n",
    "    import argparse\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--start_date',\n",
    "                        type=str,\n",
    "                        default=config['facebook']['start_date'],\n",
    "                        help='The start date from when the posts have to be extracted')\n",
    "    parser.add_argument('--end_date',\n",
    "                        type=str,\n",
    "                        default=config['facebook']['end_date'],\n",
    "                        help='The start date from when the posts have to be extracted')\n",
    "    parser.add_argument('--page_id',\n",
    "                        type=str,\n",
    "                        default=config['facebook']['page_id'],\n",
    "                        help='The start date from when the posts have to be extracted')\n",
    "    parser.add_argument('--ignore_search_tokens',\n",
    "                        type=str,\n",
    "                        default=config['facebook']['ignore_search_tokens'],\n",
    "                        help='Ignore search tokens')\n",
    "    parser.add_argument('--url',\n",
    "                        type=str,\n",
    "                        default=config['facebook']['url'],\n",
    "                        help='THe url that is present in the Facebook Graph API tool')\n",
    "    parser.add_argument('--domain',\n",
    "                        type=str,\n",
    "                        default=config['facebook']['domain'],\n",
    "                        help='The starting portion of the \"url\" argument. Do not change this.')\n",
    "    args = parser.parse_args()\n",
    "    # access_token = get_access_token(args, config)\n",
    "    return args\n",
    "\n",
    "\n",
    "def get_new_access_token(config):\n",
    "    refresh_token_url = config['facebook']['domain'] + \\\n",
    "                        'oauth/access_token?' + \\\n",
    "                        'grant_type=fb_exchange_token&' + \\\n",
    "                        'client_id=' + config['facebook']['client_id'] + '&' + \\\n",
    "                        'client_secret=' + config['facebook']['client_secret'] + '&' + \\\n",
    "                        'fb_exchange_token=' + config['facebook']['access_token']\n",
    "    new_access_token = requests.get(refresh_token_url)\n",
    "    return json.loads(new_access_token.text)['access_token']\n",
    "\n",
    "\n",
    "def file_exists(path):\n",
    "    path = pathlib.Path(path)\n",
    "    return path.is_file()\n",
    "\n",
    "\n",
    "def get_response(url):\n",
    "    global config\n",
    "    access_config = configparser.ConfigParser()\n",
    "    page_id = config['facebook']['page_id']\n",
    "    access_token_fn = 'properties/access_token'+page_id+'.ini'\n",
    "\n",
    "    if file_exists(access_token_fn):\n",
    "        access_config.read(access_token_fn)\n",
    "        config['facebook']['access_token'] = access_config['facebook']['access_token']\n",
    "    else:\n",
    "        access_config.add_section('facebook')\n",
    "        access_config.set('facebook', 'access_token', config['facebook']['access_token'])\n",
    "\n",
    "    config.set('facebook', 'access_token', get_new_access_token(config))\n",
    "    with open(access_token_fn, 'w') as f:\n",
    "        access_config.write(f)\n",
    "    response = requests.get(url + '&access_token=' + access_config['facebook']['access_token'])\n",
    "    return response\n",
    "\n",
    "\n",
    "def reconnect(url):\n",
    "    global config\n",
    "    counter = 0\n",
    "    response = get_response(url)\n",
    "    while response.status_code != 200:\n",
    "        counter += 1\n",
    "        response = get_response(url)\n",
    "        if counter == int(config['facebook']['reconnect']):\n",
    "            return None\n",
    "    return response\n",
    "\n",
    "\n",
    "def get_all_posts(args):\n",
    "    global config\n",
    "    url = construct_url(domain=args.domain,\n",
    "                        url=args.page_id+args.url)\n",
    "    date_range = '&since=%s&until=%s' % (args.start_date, args.end_date)\n",
    "    url += date_range\n",
    "    config['facebook']['page_id'] = args.page_id\n",
    "    config['facebook']['ignore_search_tokens'] = args.ignore_search_tokens\n",
    "    response = get_response(url)\n",
    "    start_pointer = json.loads(response.text)\n",
    "    return get_them_all(args, start_pointer)\n",
    "\n",
    "\n",
    "def is_in_topic(message):\n",
    "    global config\n",
    "    result = False\n",
    "    translator = str.maketrans(' ', ' ', string.punctuation)\n",
    "    considered_tokens = config['facebook']['searchTokens'].split(' ')\n",
    "    tokens_in_each_status = message.strip().translate(translator).lower().split(' ')\n",
    "    for each_considered_token in considered_tokens:\n",
    "        if each_considered_token in tokens_in_each_status:\n",
    "            result = True\n",
    "            break\n",
    "    return result\n",
    "\n",
    "\n",
    "def get_them_all(args, start_pointer, has_comment=True):\n",
    "\n",
    "    global config\n",
    "    all_posts = list()\n",
    "    while True:\n",
    "        try:\n",
    "            result = start_pointer['data']\n",
    "        except KeyError as e:\n",
    "            print('End of accessible data : + \\n' + e)\n",
    "        for each_status in result:\n",
    "            if has_comment:\n",
    "                try:\n",
    "                    if not bool(config['facebook']['ignore_search_tokens']) and \\\n",
    "                            not is_in_topic(each_status['message']):\n",
    "                        continue\n",
    "                    else:\n",
    "                        each_status['comments'] = get_them_all(args,\n",
    "                                                               each_status['comments'],\n",
    "                                                               has_comment=False)\n",
    "                except KeyError:\n",
    "                    if bool(config['facebook']['ignore_search_tokens']):\n",
    "                        all_posts.append(each_status)\n",
    "                    continue\n",
    "\n",
    "            all_posts.append(each_status)\n",
    "\n",
    "        try:\n",
    "            url = start_pointer['paging']['next']\n",
    "        except KeyError:\n",
    "            break\n",
    "        response = reconnect(url)\n",
    "        if response is None:\n",
    "            break\n",
    "        start_pointer = json.loads(response.text)\n",
    "\n",
    "    return all_posts\n",
    "\n",
    "\n",
    "def construct_url(domain, url):\n",
    "    return domain + url\n",
    "\n",
    "\n",
    "def construct_json_for_page(all_posts, args):\n",
    "    global config\n",
    "    result = dict()\n",
    "    result['id'] = args.page_id\n",
    "    result['data'] = all_posts\n",
    "    with open(os.path.join(config['facebook']['data_directory'], config['facebook']['data_append_fvalue']) + args.page_id, 'w') as f:\n",
    "        json.dump(result, f)\n",
    "\n",
    "\n",
    "def make_dir(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "\n",
    "def main():\n",
    "    args = load()\n",
    "    all_posts = get_all_posts(args)\n",
    "    construct_json_for_page(all_posts, args)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    make_dir(config['facebook']['properties_directory'])\n",
    "    make_dir(config['facebook']['data_directory'])\n",
    "    import timeit\n",
    "    start_time = timeit.default_timer()\n",
    "    main()\n",
    "    elapsed_time = timeit.default_timer() - start_time\n",
    "    print('elapsed time :', elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
